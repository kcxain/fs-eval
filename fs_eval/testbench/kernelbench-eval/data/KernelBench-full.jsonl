{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Hinge Loss for binary classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Hinge Loss for binary classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Hinge Loss for binary classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "100_HingeLoss", "problem_id": 100}, "task_id": "1-100"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.rand(N, M, K)\n    B = torch.rand(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.rand(N, M, K)\n    B = torch.rand(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.rand(N, M, K)\n    B = torch.rand(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "10_3D_tensor_matrix_multiplication", "problem_id": 10}, "task_id": "1-10"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.rand(b, i, j, l)\n    B = torch.rand(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.rand(b, i, j, l)\n    B = torch.rand(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.rand(b, i, j, l)\n    B = torch.rand(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "11_4D_tensor_matrix_multiplication", "problem_id": 11}, "task_id": "1-11"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).\n            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).\n\n        Returns:\n            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).\n        \"\"\"\n        return torch.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.rand(N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).\n            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).\n\n        Returns:\n            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).\n        \"\"\"\n        return torch.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.rand(N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).\n            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).\n\n        Returns:\n            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).\n        \"\"\"\n        return torch.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.rand(N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "12_Matmul_with_diagonal_matrices_", "problem_id": 12}, "task_id": "1-12"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of two symmetric matrices.\n\n        Args:\n            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.\n            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.\n\n        Returns:\n            torch.Tensor: Output matrix C, shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    A = torch.rand(N, N)\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = torch.rand(N, N)\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of two symmetric matrices.\n\n        Args:\n            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.\n            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.\n\n        Returns:\n            torch.Tensor: Output matrix C, shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    A = torch.rand(N, N)\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = torch.rand(N, N)\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of two symmetric matrices.\n\n        Args:\n            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.\n            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.\n\n        Returns:\n            torch.Tensor: Output matrix C, shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    A = torch.rand(N, N)\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = torch.rand(N, N)\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "13_Matmul_for_symmetric_matrices", "problem_id": 13}, "task_id": "1-13"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication for upper triangular matrices.\n\n        Args:\n            A (torch.Tensor): Upper triangular matrix of shape (N, N).\n            B (torch.Tensor): Upper triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).\n        \"\"\"\n        return torch.triu(torch.matmul(A, B))\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    A = torch.triu(torch.rand(N, N))\n    B = torch.triu(torch.rand(N, N))\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication for upper triangular matrices.\n\n        Args:\n            A (torch.Tensor): Upper triangular matrix of shape (N, N).\n            B (torch.Tensor): Upper triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).\n        \"\"\"\n        return torch.triu(torch.matmul(A, B))\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    A = torch.triu(torch.rand(N, N))\n    B = torch.triu(torch.rand(N, N))\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication for upper triangular matrices.\n\n        Args:\n            A (torch.Tensor): Upper triangular matrix of shape (N, N).\n            B (torch.Tensor): Upper triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).\n        \"\"\"\n        return torch.triu(torch.matmul(A, B))\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    A = torch.triu(torch.rand(N, N))\n    B = torch.triu(torch.rand(N, N))\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "14_Matmul_for_upper_triangular_matrices", "problem_id": 14}, "task_id": "1-14"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of lower triangular matrices A and B.\n\n        Args:\n            A (torch.Tensor): Lower triangular matrix of shape (N, N).\n            B (torch.Tensor): Lower triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The result of matrix multiplication C of shape (N, N).\n        \"\"\"\n        return torch.tril(torch.matmul(A, B))\n\nM = 4096\n\ndef get_inputs():\n    A = torch.rand(M, M)\n    B = torch.rand(M, M)\n    A = torch.tril(A)\n    B = torch.tril(B)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of lower triangular matrices A and B.\n\n        Args:\n            A (torch.Tensor): Lower triangular matrix of shape (N, N).\n            B (torch.Tensor): Lower triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The result of matrix multiplication C of shape (N, N).\n        \"\"\"\n        return torch.tril(torch.matmul(A, B))\n\nM = 4096\n\ndef get_inputs():\n    A = torch.rand(M, M)\n    B = torch.rand(M, M)\n    A = torch.tril(A)\n    B = torch.tril(B)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of lower triangular matrices A and B.\n\n        Args:\n            A (torch.Tensor): Lower triangular matrix of shape (N, N).\n            B (torch.Tensor): Lower triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The result of matrix multiplication C of shape (N, N).\n        \"\"\"\n        return torch.tril(torch.matmul(A, B))\n\nM = 4096\n\ndef get_inputs():\n    A = torch.rand(M, M)\n    B = torch.rand(M, M)\n    A = torch.tril(A)\n    B = torch.tril(B)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "15_Matmul_for_lower_triangular_matrices", "problem_id": 15}, "task_id": "1-15"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "16_Matmul_with_transposed_A", "problem_id": 16}, "task_id": "1-16"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "17_Matmul_with_transposed_B", "problem_id": 17}, "task_id": "1-17"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "18_Matmul_with_transposed_both", "problem_id": 18}, "task_id": "1-18"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n        \"\"\"\n        return torch.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n        \"\"\"\n        return torch.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n        \"\"\"\n        return torch.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "19_ReLU", "problem_id": 19}, "task_id": "1-19"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(N, N)\n    B = torch.rand(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(N, N)\n    B = torch.rand(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(N, N)\n    B = torch.rand(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "1_Square_matrix_multiplication_", "problem_id": 1}, "task_id": "1-1"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    def __init__(self, negative_slope: float = 0.01):\n        \"\"\"\n        Initializes the LeakyReLU module.\n\n        Args:\n            negative_slope (float, optional): The negative slope of the activation function. Defaults to 0.01.\n        \"\"\"\n        super(Model, self).__init__()\n        self.negative_slope = negative_slope\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LeakyReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with LeakyReLU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.leaky_relu(x, negative_slope=self.negative_slope)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    def __init__(self, negative_slope: float = 0.01):\n        \"\"\"\n        Initializes the LeakyReLU module.\n\n        Args:\n            negative_slope (float, optional): The negative slope of the activation function. Defaults to 0.01.\n        \"\"\"\n        super(Model, self).__init__()\n        self.negative_slope = negative_slope\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LeakyReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with LeakyReLU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.leaky_relu(x, negative_slope=self.negative_slope)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    def __init__(self, negative_slope: float = 0.01):\n        \"\"\"\n        Initializes the LeakyReLU module.\n\n        Args:\n            negative_slope (float, optional): The negative slope of the activation function. Defaults to 0.01.\n        \"\"\"\n        super(Model, self).__init__()\n        self.negative_slope = negative_slope\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LeakyReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with LeakyReLU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.leaky_relu(x, negative_slope=self.negative_slope)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "20_LeakyReLU", "problem_id": 20}, "task_id": "1-20"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Sigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.\n        \"\"\"\n        return torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Sigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.\n        \"\"\"\n        return torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Sigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.\n        \"\"\"\n        return torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "21_Sigmoid", "problem_id": 21}, "task_id": "1-21"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Tanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Tanh applied, same shape as input.\n        \"\"\"\n        return torch.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Tanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Tanh applied, same shape as input.\n        \"\"\"\n        return torch.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Tanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Tanh applied, same shape as input.\n        \"\"\"\n        return torch.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "22_Tanh", "problem_id": 22}, "task_id": "1-22"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Output tensor with Softmax applied, same shape as input.\n        \"\"\"\n        return torch.softmax(x, dim=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Output tensor with Softmax applied, same shape as input.\n        \"\"\"\n        return torch.softmax(x, dim=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Output tensor with Softmax applied, same shape as input.\n        \"\"\"\n        return torch.softmax(x, dim=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "23_Softmax", "problem_id": 23}, "task_id": "1-23"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(Model, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LogSoftmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, dim).\n\n        Returns:\n            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.\n        \"\"\"\n        return torch.log_softmax(x, dim=self.dim)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(Model, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LogSoftmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, dim).\n\n        Returns:\n            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.\n        \"\"\"\n        return torch.log_softmax(x, dim=self.dim)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(Model, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LogSoftmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, dim).\n\n        Returns:\n            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.\n        \"\"\"\n        return torch.log_softmax(x, dim=self.dim)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "24_LogSoftmax", "problem_id": 24}, "task_id": "1-24"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Swish activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Swish applied, same shape as input.\n        \"\"\"\n        return x * torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Swish activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Swish applied, same shape as input.\n        \"\"\"\n        return x * torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Swish activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Swish applied, same shape as input.\n        \"\"\"\n        return x * torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "25_Swish", "problem_id": 25}, "task_id": "1-25"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies GELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with GELU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies GELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with GELU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies GELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with GELU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "26_GELU_", "problem_id": 26}, "task_id": "1-26"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n    \nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n    \nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n    \nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "27_SELU_", "problem_id": 27}, "task_id": "1-27"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardSigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardSigmoid applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.hardsigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardSigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardSigmoid applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.hardsigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardSigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardSigmoid applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.hardsigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "28_HardSigmoid", "problem_id": 28}, "task_id": "1-28"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softplus activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softplus applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softplus activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softplus applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softplus activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softplus applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "29_Softplus", "problem_id": 29}, "task_id": "1-29"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "2_Standard_matrix_multiplication_", "problem_id": 2}, "task_id": "1-2"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softsign activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softsign applied, same shape as input.\n        \"\"\"\n        return x / (1 + torch.abs(x))\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softsign activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softsign applied, same shape as input.\n        \"\"\"\n        return x / (1 + torch.abs(x))\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softsign activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softsign applied, same shape as input.\n        \"\"\"\n        return x / (1 + torch.abs(x))\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "30_Softsign", "problem_id": 30}, "task_id": "1-30"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    def __init__(self, alpha: float = 1.0):\n        \"\"\"\n        Initializes the ELU model.\n\n        Args:\n            alpha (float, optional): The alpha parameter for the ELU function. Defaults to 1.0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ELU applied, same shape as input.\n        \"\"\"\n        return F.elu(x, alpha=self.alpha)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return [1.0]  # Provide alpha value for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    def __init__(self, alpha: float = 1.0):\n        \"\"\"\n        Initializes the ELU model.\n\n        Args:\n            alpha (float, optional): The alpha parameter for the ELU function. Defaults to 1.0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ELU applied, same shape as input.\n        \"\"\"\n        return F.elu(x, alpha=self.alpha)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return [1.0]  # Provide alpha value for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    def __init__(self, alpha: float = 1.0):\n        \"\"\"\n        Initializes the ELU model.\n\n        Args:\n            alpha (float, optional): The alpha parameter for the ELU function. Defaults to 1.0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ELU applied, same shape as input.\n        \"\"\"\n        return F.elu(x, alpha=self.alpha)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return [1.0]  # Provide alpha value for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "31_ELU", "problem_id": 31}, "task_id": "1-31"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardTanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardTanh applied, same shape as input.\n        \"\"\"\n        return F.hardtanh(x, min_val=-1., max_val=1.)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardTanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardTanh applied, same shape as input.\n        \"\"\"\n        return F.hardtanh(x, min_val=-1., max_val=1.)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardTanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardTanh applied, same shape as input.\n        \"\"\"\n        return F.hardtanh(x, min_val=-1., max_val=1.)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "32_HardTanh", "problem_id": 32}, "task_id": "1-32"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the BatchNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return self.bn(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the BatchNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return self.bn(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the BatchNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return self.bn(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "33_BatchNorm", "problem_id": 33}, "task_id": "1-33"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return self.inorm(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return self.inorm(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return self.inorm(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "34_InstanceNorm", "problem_id": 34}, "task_id": "1-34"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, num_groups: int):\n        \"\"\"\n        Initializes the GroupNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            num_groups (int): Number of groups to divide the channels into.\n        \"\"\"\n        super(Model, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return self.gn(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\nnum_groups = 8\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features, num_groups] # num_features```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, num_groups: int):\n        \"\"\"\n        Initializes the GroupNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            num_groups (int): Number of groups to divide the channels into.\n        \"\"\"\n        super(Model, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return self.gn(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\nnum_groups = 8\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features, num_groups] # num_features", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, num_groups: int):\n        \"\"\"\n        Initializes the GroupNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            num_groups (int): Number of groups to divide the channels into.\n        \"\"\"\n        super(Model, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return self.gn(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\nnum_groups = 8\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features, num_groups] # num_features", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "35_GroupNorm_", "problem_id": 35}, "task_id": "1-35"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, eps: float = 1e-5):\n        \"\"\"\n        Initializes the RMSNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        \"\"\"\n        super(Model, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # Calculate the RMS along the feature dimension\n        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n\n        # Normalize the input by dividing by the RMS\n        return x / rms\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, eps: float = 1e-5):\n        \"\"\"\n        Initializes the RMSNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        \"\"\"\n        super(Model, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # Calculate the RMS along the feature dimension\n        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n\n        # Normalize the input by dividing by the RMS\n        return x / rms\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, eps: float = 1e-5):\n        \"\"\"\n        Initializes the RMSNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        \"\"\"\n        super(Model, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # Calculate the RMS along the feature dimension\n        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n\n        # Normalize the input by dividing by the RMS\n        return x / rms\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "36_RMSNorm_", "problem_id": 36}, "task_id": "1-36"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Frobenius norm normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Frobenius norm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.\n        \"\"\"\n        norm = torch.norm(x, p='fro')\n        return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Frobenius norm normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Frobenius norm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.\n        \"\"\"\n        norm = torch.norm(x, p='fro')\n        return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Frobenius norm normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Frobenius norm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.\n        \"\"\"\n        norm = torch.norm(x, p='fro')\n        return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "37_FrobeniusNorm_", "problem_id": 37}, "task_id": "1-37"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "38_L1Norm_", "problem_id": 38}, "task_id": "1-38"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "39_L2Norm_", "problem_id": 39}, "task_id": "1-39"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k)\n    B = torch.rand(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k)\n    B = torch.rand(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k)\n    B = torch.rand(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "3_Batched_matrix_multiplication", "problem_id": 3}, "task_id": "1-3"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    def __init__(self, normalized_shape: tuple):\n        \"\"\"\n        Initializes the LayerNorm layer.\n\n        Args:\n            normalized_shape (tuple): Shape of the input tensor to be normalized.\n        \"\"\"\n        super(Model, self).__init__()\n        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, normalized_shape).\n\n        Returns:\n            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        return self.ln(x)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [(features, dim1, dim2)]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    def __init__(self, normalized_shape: tuple):\n        \"\"\"\n        Initializes the LayerNorm layer.\n\n        Args:\n            normalized_shape (tuple): Shape of the input tensor to be normalized.\n        \"\"\"\n        super(Model, self).__init__()\n        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, normalized_shape).\n\n        Returns:\n            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        return self.ln(x)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [(features, dim1, dim2)]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    def __init__(self, normalized_shape: tuple):\n        \"\"\"\n        Initializes the LayerNorm layer.\n\n        Args:\n            normalized_shape (tuple): Shape of the input tensor to be normalized.\n        \"\"\"\n        super(Model, self).__init__()\n        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, normalized_shape).\n\n        Returns:\n            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        return self.ln(x)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [(features, dim1, dim2)]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "40_LayerNorm", "problem_id": 40}, "task_id": "1-40"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 1D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 1D layer.\n\n        Args:\n            kernel_size (int): Size of the window to take a max over.\n            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).\n            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 1D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3            \n\nreturn_indices = False\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation, return_indices]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 1D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 1D layer.\n\n        Args:\n            kernel_size (int): Size of the window to take a max over.\n            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).\n            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 1D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3            \n\nreturn_indices = False\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation, return_indices]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 1D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 1D layer.\n\n        Args:\n            kernel_size (int): Size of the window to take a max over.\n            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).\n            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 1D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3            \n\nreturn_indices = False\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation, return_indices]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "41_Max_Pooling_1D", "problem_id": 41}, "task_id": "1-41"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 2D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):\n        \"\"\"\n        Initializes the Max Pooling 2D layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int): Stride of the pooling window.\n            padding (int): Padding to be applied before pooling.\n            dilation (int): Spacing between kernel elements.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 2D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 2D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):\n        \"\"\"\n        Initializes the Max Pooling 2D layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int): Stride of the pooling window.\n            padding (int): Padding to be applied before pooling.\n            dilation (int): Spacing between kernel elements.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 2D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 2D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):\n        \"\"\"\n        Initializes the Max Pooling 2D layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int): Stride of the pooling window.\n            padding (int): Padding to be applied before pooling.\n            dilation (int): Spacing between kernel elements.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 2D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "42_Max_Pooling_2D", "problem_id": 42}, "task_id": "1-42"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "43_Max_Pooling_3D", "problem_id": 43}, "task_id": "1-43"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 1D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n        \"\"\"\n        Initializes the 1D Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to 1.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 1D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).\n\n        Returns:\n            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\nkernel_size = 8\nstride = 1\npadding = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, input_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 1D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n        \"\"\"\n        Initializes the 1D Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to 1.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 1D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).\n\n        Returns:\n            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\nkernel_size = 8\nstride = 1\npadding = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, input_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 1D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n        \"\"\"\n        Initializes the 1D Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to 1.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 1D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).\n\n        Returns:\n            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\nkernel_size = 8\nstride = 1\npadding = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, input_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "44_Average_Pooling_1D", "problem_id": 44}, "task_id": "1-44"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\nkernel_size = 11\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\nkernel_size = 11\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\nkernel_size = 11\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "45_Average_Pooling_2D", "problem_id": 45}, "task_id": "1-45"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 3D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the kernel to apply pooling.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which uses the kernel size.\n            padding (int, optional): Padding to apply before pooling. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied, shape depends on kernel_size, stride and padding.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 3D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the kernel to apply pooling.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which uses the kernel size.\n            padding (int, optional): Padding to apply before pooling. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied, shape depends on kernel_size, stride and padding.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 3D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the kernel to apply pooling.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which uses the kernel size.\n            padding (int, optional): Padding to apply before pooling. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied, shape depends on kernel_size, stride and padding.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "46_Average_Pooling_3D", "problem_id": 46}, "task_id": "1-46"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): Dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies sum reduction over the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).\n        \"\"\"\n        return torch.sum(x, dim=self.dim, keepdim=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): Dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies sum reduction over the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).\n        \"\"\"\n        return torch.sum(x, dim=self.dim, keepdim=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): Dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies sum reduction over the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).\n        \"\"\"\n        return torch.sum(x, dim=self.dim, keepdim=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "47_Sum_reduction_over_a_dimension", "problem_id": 47}, "task_id": "1-47"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reduces the input tensor along the specified dimension by taking the mean.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.\n        \"\"\"\n        return torch.mean(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reduces the input tensor along the specified dimension by taking the mean.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.\n        \"\"\"\n        return torch.mean(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reduces the input tensor along the specified dimension by taking the mean.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.\n        \"\"\"\n        return torch.mean(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "48_Mean_reduction_over_a_dimension", "problem_id": 48}, "task_id": "1-48"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after Max reduction over the specified dimension.\n        \"\"\"\n        return torch.max(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after Max reduction over the specified dimension.\n        \"\"\"\n        return torch.max(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after Max reduction over the specified dimension.\n        \"\"\"\n        return torch.max(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "49_Max_reduction_over_a_dimension", "problem_id": 49}, "task_id": "1-49"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "4_Matrix_vector_multiplication_", "problem_id": 4}, "task_id": "1-4"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n# Test code\nbatch_size = 256\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n# Test code\nbatch_size = 256\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n# Test code\nbatch_size = 256\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "50_conv_standard_2D__square_input__square_kernel", "problem_id": 50}, "task_id": "1-50"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmax.\n\n        Args:\n            dim (int): The dimension to perform argmax over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies argmax over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.\n        \"\"\"\n        return torch.argmax(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmax.\n\n        Args:\n            dim (int): The dimension to perform argmax over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies argmax over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.\n        \"\"\"\n        return torch.argmax(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmax.\n\n        Args:\n            dim (int): The dimension to perform argmax over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies argmax over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.\n        \"\"\"\n        return torch.argmax(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "51_Argmax_over_a_dimension", "problem_id": 51}, "task_id": "1-51"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmin on.\n\n        Args:\n            dim (int): Dimension along which to find the minimum value.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Finds the index of the minimum value along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.\n        \"\"\"\n        return torch.argmin(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmin on.\n\n        Args:\n            dim (int): Dimension along which to find the minimum value.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Finds the index of the minimum value along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.\n        \"\"\"\n        return torch.argmin(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [dim]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmin on.\n\n        Args:\n            dim (int): Dimension along which to find the minimum value.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Finds the index of the minimum value along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.\n        \"\"\"\n        return torch.argmin(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "52_Argmin_over_a_dimension", "problem_id": 52}, "task_id": "1-52"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies min reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after min reduction over the specified dimension.\n        \"\"\"\n        return torch.min(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies min reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after min reduction over the specified dimension.\n        \"\"\"\n        return torch.min(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies min reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after min reduction over the specified dimension.\n        \"\"\"\n        return torch.min(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "53_Min_reduction_over_a_dimension", "problem_id": 53}, "task_id": "1-53"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "54_conv_standard_3D__square_input__square_kernel", "problem_id": 54}, "task_id": "1-54"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\nin_channels = 64  # increased channels\nout_channels = 128\nkernel_size = 3\n# asymmetric input: make width considerably larger than height\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\nin_channels = 64  # increased channels\nout_channels = 128\nkernel_size = 3\n# asymmetric input: make width considerably larger than height\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\nin_channels = 64  # increased channels\nout_channels = 128\nkernel_size = 3\n# asymmetric input: make width considerably larger than height\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "55_conv_standard_2D__asymmetric_input__square_kernel", "problem_id": 55}, "task_id": "1-55"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of two integers representing the height and width of the convolution kernel.\n        stride (tuple, optional): Tuple of two integers representing the stride in the height and width dimensions. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of two integers representing the padding in the height and width dimensions. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of two integers representing the dilation in the height and width dimensions. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nheight = 512\nwidth = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of two integers representing the height and width of the convolution kernel.\n        stride (tuple, optional): Tuple of two integers representing the stride in the height and width dimensions. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of two integers representing the padding in the height and width dimensions. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of two integers representing the dilation in the height and width dimensions. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nheight = 512\nwidth = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of two integers representing the height and width of the convolution kernel.\n        stride (tuple, optional): Tuple of two integers representing the stride in the height and width dimensions. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of two integers representing the padding in the height and width dimensions. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of two integers representing the dilation in the height and width dimensions. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nheight = 512\nwidth = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "56_conv_standard_2D__asymmetric_input__asymmetric_kernel", "problem_id": 56}, "task_id": "1-56"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64  # double channels for heavier compute\nout_channels = 64\nkernel_size = 3\n# larger square input\nheight = 1024\nwidth = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64  # double channels for heavier compute\nout_channels = 64\nkernel_size = 3\n# larger square input\nheight = 1024\nwidth = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64  # double channels for heavier compute\nout_channels = 64\nkernel_size = 3\n# larger square input\nheight = 1024\nwidth = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "57_conv_transposed_2D__square_input__square_kernel", "problem_id": 57}, "task_id": "1-57"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of 3 integers representing the kernel size in the form (depth, height, width).\n        stride (tuple, optional): Tuple of 3 integers representing the stride in the form (depth, height, width). Defaults to (1, 1, 1).\n        padding (tuple, optional): Tuple of 3 integers representing the padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Tuple of 3 integers representing the output padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth_in, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth_in, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of 3 integers representing the kernel size in the form (depth, height, width).\n        stride (tuple, optional): Tuple of 3 integers representing the stride in the form (depth, height, width). Defaults to (1, 1, 1).\n        padding (tuple, optional): Tuple of 3 integers representing the padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Tuple of 3 integers representing the output padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth_in, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth_in, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of 3 integers representing the kernel size in the form (depth, height, width).\n        stride (tuple, optional): Tuple of 3 integers representing the stride in the form (depth, height, width). Defaults to (1, 1, 1).\n        padding (tuple, optional): Tuple of 3 integers representing the padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Tuple of 3 integers representing the output padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth_in, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth_in, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "58_conv_transposed_3D__asymmetric_input__asymmetric_kernel", "problem_id": 58}, "task_id": "1-58"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel (kernel_size x kernel_size).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel (kernel_size x kernel_size).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel (kernel_size x kernel_size).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "59_conv_standard_3D__asymmetric_input__square_kernel", "problem_id": 59}, "task_id": "1-59"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "5_Matrix_scalar_multiplication", "problem_id": 5}, "task_id": "1-5"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_width, kernel_height, kernel_depth).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, width, height, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, width_out, height_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, width, height, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_width, kernel_height, kernel_depth).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, width, height, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, width_out, height_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, width, height, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_width, kernel_height, kernel_depth).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, width, height, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, width_out, height_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, width, height, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "60_conv_standard_3D__square_input__asymmetric_kernel", "problem_id": 60}, "task_id": "1-60"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 48\nkernel_size = 3\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 48\nkernel_size = 3\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 48\nkernel_size = 3\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "61_conv_transposed_3D__square_input__square_kernel", "problem_id": 61}, "task_id": "1-61"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "62_conv_standard_2D__square_input__asymmetric_kernel", "problem_id": 62}, "task_id": "1-62"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "63_conv_standard_2D__square_input__square_kernel", "problem_id": 63}, "task_id": "1-63"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n# much larger signal length for heavier workload\nlength = 65536\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n# much larger signal length for heavier workload\nlength = 65536\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n# much larger signal length for heavier workload\nlength = 65536\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "64_conv_transposed_1D", "problem_id": 64}, "task_id": "1-64"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)  # larger asymmetric kernel\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)  # larger asymmetric kernel\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)  # larger asymmetric kernel\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "65_conv_transposed_2D__square_input__asymmetric_kernel", "problem_id": 65}, "task_id": "1-65"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).\n        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).\n        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).\n        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).\n        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).\n        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).\n        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "66_conv_standard_3D__asymmetric_input__asymmetric_kernel", "problem_id": 66}, "task_id": "1-66"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nlength = 131072\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nlength = 131072\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nlength = 131072\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "67_conv_standard_1D", "problem_id": 67}, "task_id": "1-67"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), \n                             where kernel_width == kernel_height.\n        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), \n                             where kernel_width == kernel_height.\n        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), \n                             where kernel_width == kernel_height.\n        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "68_conv_transposed_3D__square_input__asymmetric_kernel", "problem_id": 68}, "task_id": "1-68"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution operation with asymmetric input and kernel size.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of integers representing the kernel size (height, width).\n        stride (tuple, optional): Tuple of integers representing the stride of the convolution. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of integers representing the padding applied to the input. Defaults to (0, 0).\n        output_padding (tuple, optional): Tuple of integers representing the additional size added to one side of the output shape. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of integers representing the spacing between kernel elements. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), output_padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution operation with asymmetric input and kernel size.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of integers representing the kernel size (height, width).\n        stride (tuple, optional): Tuple of integers representing the stride of the convolution. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of integers representing the padding applied to the input. Defaults to (0, 0).\n        output_padding (tuple, optional): Tuple of integers representing the additional size added to one side of the output shape. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of integers representing the spacing between kernel elements. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), output_padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution operation with asymmetric input and kernel size.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of integers representing the kernel size (height, width).\n        stride (tuple, optional): Tuple of integers representing the stride of the convolution. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of integers representing the padding applied to the input. Defaults to (0, 0).\n        output_padding (tuple, optional): Tuple of integers representing the additional size added to one side of the output shape. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of integers representing the spacing between kernel elements. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), output_padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "69_conv_transposed_2D__asymmetric_input__asymmetric_kernel", "problem_id": 69}, "task_id": "1-69"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "6_Matmul_with_large_K_dimension_", "problem_id": 6}, "task_id": "1-6"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. \n                                                  Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, \n                 dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), \n                                                stride=stride, padding=padding, output_padding=output_padding, \n                                                dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 24\nkernel_size = 3\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. \n                                                  Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, \n                 dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), \n                                                stride=stride, padding=padding, output_padding=output_padding, \n                                                dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 24\nkernel_size = 3\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. \n                                                  Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, \n                 dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), \n                                                stride=stride, padding=padding, output_padding=output_padding, \n                                                dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 24\nkernel_size = 3\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "70_conv_transposed_3D__asymmetric_input__square_kernel", "problem_id": 70}, "task_id": "1-70"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n# large asymmetric input\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n# large asymmetric input\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n# large asymmetric input\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "71_conv_transposed_2D__asymmetric_input__square_kernel", "problem_id": 71}, "task_id": "1-71"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).\n        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).\n        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).\n        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\ndepth = 12\nheight = 24\nwidth = 48\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).\n        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).\n        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).\n        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\ndepth = 12\nheight = 24\nwidth = 48\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).\n        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).\n        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).\n        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\ndepth = 12\nheight = 24\nwidth = 48\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_", "problem_id": 72}, "task_id": "1-72"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and square kernel.\n    The input is padded before the convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 4\nin_channels = 32\nout_channels = 32\nkernel_size = 3\ndepth = 32\nheight = 64\nwidth = 128\nstride = 2\npadding = 1\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and square kernel.\n    The input is padded before the convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 4\nin_channels = 32\nout_channels = 32\nkernel_size = 3\ndepth = 32\nheight = 64\nwidth = 128\nstride = 2\npadding = 1\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and square kernel.\n    The input is padded before the convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 4\nin_channels = 32\nout_channels = 32\nkernel_size = 3\ndepth = 32\nheight = 64\nwidth = 128\nstride = 2\npadding = 1\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "73_conv_transposed_3D_asymmetric_input_square_kernel__strided_padded__grouped", "problem_id": 73}, "task_id": "1-73"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nlength = 131072\nstride = 1\npadding = 0\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nlength = 131072\nstride = 1\npadding = 0\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nlength = 131072\nstride = 1\npadding = 0\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "74_conv_transposed_1D_dilated", "problem_id": 74}, "task_id": "1-74"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input, asymmetric kernel, \n    grouped, padded, and dilated.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nheight = 128\nwidth = 256\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input, asymmetric kernel, \n    grouped, padded, and dilated.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nheight = 128\nwidth = 256\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input, asymmetric kernel, \n    grouped, padded, and dilated.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nheight = 128\nwidth = 256\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "75_conv_transposed_2D_asymmetric_input_asymmetric_kernel_strided__grouped____padded____dilated__", "problem_id": 75}, "task_id": "1-75"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation with asymmetric input and a square kernel, potentially dilated and strided.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n# longer signal\nlength = 524280\nstride = 3\ndilation = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation with asymmetric input and a square kernel, potentially dilated and strided.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n# longer signal\nlength = 524280\nstride = 3\ndilation = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation with asymmetric input and a square kernel, potentially dilated and strided.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n# longer signal\nlength = 524280\nstride = 3\ndilation = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "76_conv_standard_1D_dilated_strided__", "problem_id": 76}, "task_id": "1-76"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with square input and square kernel,\n    and supports padding, dilation, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square kernel, so only one value needed).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\ndepth = 16\nheight = 32\nwidth = 32\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with square input and square kernel,\n    and supports padding, dilation, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square kernel, so only one value needed).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\ndepth = 16\nheight = 32\nwidth = 32\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with square input and square kernel,\n    and supports padding, dilation, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square kernel, so only one value needed).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\ndepth = 16\nheight = 32\nwidth = 32\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "77_conv_transposed_3D_square_input_square_kernel___padded____dilated____strided__", "problem_id": 77}, "task_id": "1-77"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and kernel, with optional padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nheight = 512\nwidth = 1024\nstride = (1, 1)\npadding = (1, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and kernel, with optional padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nheight = 512\nwidth = 1024\nstride = (1, 1)\npadding = (1, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and kernel, with optional padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nheight = 512\nwidth = 1024\nstride = (1, 1)\npadding = (1, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "78_conv_transposed_2D_asymmetric_input_asymmetric_kernel___padded__", "problem_id": 78}, "task_id": "1-78"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with asymmetric input and square kernel.\n    Supports padding, striding, and dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\n# long sequence\nlength = 131072\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with asymmetric input and square kernel.\n    Supports padding, striding, and dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\n# long sequence\nlength = 131072\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with asymmetric input and square kernel.\n    Supports padding, striding, and dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\n# long sequence\nlength = 131072\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "79_conv_transposed_1D_asymmetric_input_square_kernel___padded____strided____dilated__", "problem_id": 79}, "task_id": "1-79"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "7_Matmul_with_small_K_dimension_", "problem_id": 7}, "task_id": "1-7"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with square input and asymmetric kernel, with dilation and padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width). \n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (tuple, optional): Padding applied to the input (top/bottom, left/right). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: tuple = (0, 0), dilation: tuple = (1, 1), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\nstride = 1\npadding = (2, 4)\ndilation = (2, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with square input and asymmetric kernel, with dilation and padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width). \n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (tuple, optional): Padding applied to the input (top/bottom, left/right). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: tuple = (0, 0), dilation: tuple = (1, 1), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\nstride = 1\npadding = (2, 4)\ndilation = (2, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with square input and asymmetric kernel, with dilation and padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width). \n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (tuple, optional): Padding applied to the input (top/bottom, left/right). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: tuple = (0, 0), dilation: tuple = (1, 1), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\nstride = 1\npadding = (2, 4)\ndilation = (2, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "80_conv_standard_2D_square_input_asymmetric_kernel___dilated____padded__", "problem_id": 80}, "task_id": "1-80"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and square kernel, supporting dilation, padding, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square, e.g., 3 for a 3x3 kernel).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in). \n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nheight_in = 64\nwidth_in = 128\nstride = 5\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and square kernel, supporting dilation, padding, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square, e.g., 3 for a 3x3 kernel).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in). \n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nheight_in = 64\nwidth_in = 128\nstride = 5\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and square kernel, supporting dilation, padding, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square, e.g., 3 for a 3x3 kernel).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in). \n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nheight_in = 64\nwidth_in = 128\nstride = 5\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "81_conv_transposed_2D_asymmetric_input_square_kernel___dilated____padded____strided__", "problem_id": 81}, "task_id": "1-81"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "82_conv_depthwise_2D_square_input_square_kernel", "problem_id": 82}, "task_id": "1-82"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 8\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 8\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 8\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "83_conv_depthwise_2D_square_input_asymmetric_kernel", "problem_id": 83}, "task_id": "1-83"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nwidth_in = 512\nheight_in = 256\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nwidth_in = 512\nheight_in = 256\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nwidth_in = 512\nheight_in = 256\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "84_conv_depthwise_2D_asymmetric_input_square_kernel", "problem_id": 84}, "task_id": "1-84"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size_h (int): Height of the convolution kernel.\n        kernel_size_w (int): Width of the convolution kernel.\n        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.\n        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.\n        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.\n        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.\n        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.\n        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 128\nout_channels = 128\nkernel_size_h = 3\nkernel_size_w = 7\nwidth = 256\nheight = 128\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\ndilation_h = 1\ndilation_w = 1\ngroups = in_channels\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size_h (int): Height of the convolution kernel.\n        kernel_size_w (int): Width of the convolution kernel.\n        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.\n        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.\n        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.\n        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.\n        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.\n        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 128\nout_channels = 128\nkernel_size_h = 3\nkernel_size_w = 7\nwidth = 256\nheight = 128\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\ndilation_h = 1\ndilation_w = 1\ngroups = in_channels\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size_h (int): Height of the convolution kernel.\n        kernel_size_w (int): Width of the convolution kernel.\n        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.\n        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.\n        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.\n        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.\n        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.\n        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 128\nout_channels = 128\nkernel_size_h = 3\nkernel_size_w = 7\nwidth = 256\nheight = 128\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\ndilation_h = 1\ndilation_w = 1\ngroups = in_channels\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "85_conv_depthwise_2D_asymmetric_input_asymmetric_kernel", "problem_id": 85}, "task_id": "1-85"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise-separable 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise-separable 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise-separable 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "86_conv_depthwise_separable_2D", "problem_id": 86}, "task_id": "1-86"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a pointwise 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the pointwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a pointwise 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the pointwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a pointwise 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the pointwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "87_conv_pointwise_2D", "problem_id": 87}, "task_id": "1-87"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nbatch_size = 8192\ndim = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, dim)]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nbatch_size = 8192\ndim = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, dim)]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nbatch_size = 8192\ndim = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, dim)]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "88_MinGPTNewGelu", "problem_id": 88}, "task_id": "1-88"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the scan operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the Scan model.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative sum.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` \n                              can vary depending on the use case.\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.\n        \"\"\"\n        return torch.cumsum(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    \"\"\"\n    Generates random inputs for testing the Scan model.\n\n    Returns:\n        list: A list containing a single randomly generated tensor with shape \n              (batch_size, *input_shape).\n    \"\"\"\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    \"\"\"\n    Returns the initialization parameters for the Scan model.\n\n    Returns:\n        list: A list containing the `dim` parameter for model initialization.\n    \"\"\"\n    return [dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the scan operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the Scan model.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative sum.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` \n                              can vary depending on the use case.\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.\n        \"\"\"\n        return torch.cumsum(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    \"\"\"\n    Generates random inputs for testing the Scan model.\n\n    Returns:\n        list: A list containing a single randomly generated tensor with shape \n              (batch_size, *input_shape).\n    \"\"\"\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    \"\"\"\n    Returns the initialization parameters for the Scan model.\n\n    Returns:\n        list: A list containing the `dim` parameter for model initialization.\n    \"\"\"\n    return [dim]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the scan operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the Scan model.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative sum.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` \n                              can vary depending on the use case.\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.\n        \"\"\"\n        return torch.cumsum(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    \"\"\"\n    Generates random inputs for testing the Scan model.\n\n    Returns:\n        list: A list containing a single randomly generated tensor with shape \n              (batch_size, *input_shape).\n    \"\"\"\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    \"\"\"\n    Returns the initialization parameters for the Scan model.\n\n    Returns:\n        list: A list containing the `dim` parameter for model initialization.\n    \"\"\"\n    return [dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "89_cumsum", "problem_id": 89}, "task_id": "1-89"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "8_Matmul_with_irregular_shapes_", "problem_id": 8}, "task_id": "1-8"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a cumulative product operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the cumulative product operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the CumulativeProductModel.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative product.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass, computing the cumulative product along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.\n        \"\"\"\n        return torch.cumprod(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a cumulative product operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the cumulative product operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the CumulativeProductModel.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative product.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass, computing the cumulative product along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.\n        \"\"\"\n        return torch.cumprod(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a cumulative product operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the cumulative product operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the CumulativeProductModel.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative product.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass, computing the cumulative product along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.\n        \"\"\"\n        return torch.cumprod(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "90_cumprod", "problem_id": 90}, "task_id": "1-90"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a reverse cumulative sum operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the reverse cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a reverse cumulative sum operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the reverse cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a reverse cumulative sum operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the reverse cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "91_cumsum_reverse", "problem_id": 91}, "task_id": "1-91"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs an exclusive cumulative sum (does not include the current element).\n\n    Parameters:\n        dim (int): The dimension along which to perform the exclusive cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]\n        return torch.cumsum(exclusive_cumsum, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs an exclusive cumulative sum (does not include the current element).\n\n    Parameters:\n        dim (int): The dimension along which to perform the exclusive cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]\n        return torch.cumsum(exclusive_cumsum, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs an exclusive cumulative sum (does not include the current element).\n\n    Parameters:\n        dim (int): The dimension along which to perform the exclusive cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]\n        return torch.cumsum(exclusive_cumsum, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "92_cumsum_exclusive", "problem_id": 92}, "task_id": "1-92"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "93_masked_cumsum", "problem_id": 93}, "task_id": "1-93"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes the Mean Squared Error loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean((predictions - targets) ** 2)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes the Mean Squared Error loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean((predictions - targets) ** 2)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes the Mean Squared Error loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean((predictions - targets) ** 2)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "94_MSELoss", "problem_id": 94}, "task_id": "1-94"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 32768\nnum_classes = 4096\ninput_shape = (num_classes,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 32768\nnum_classes = 4096\ninput_shape = (num_classes,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 32768\nnum_classes = 4096\ninput_shape = (num_classes,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "95_CrossEntropyLoss", "problem_id": 95}, "task_id": "1-95"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Smooth L1 (Huber) Loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.smooth_l1_loss(predictions, targets)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Smooth L1 (Huber) Loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.smooth_l1_loss(predictions, targets)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Smooth L1 (Huber) Loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.smooth_l1_loss(predictions, targets)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "96_HuberLoss", "problem_id": 96}, "task_id": "1-96"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        return out\n\nbatch_size = 32\nnum_heads = 32\nsequence_length = 512\nembedding_dimension = 1024\n\ndef get_inputs():\n    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    return [Q, K, V]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        return out\n\nbatch_size = 32\nnum_heads = 32\nsequence_length = 512\nembedding_dimension = 1024\n\ndef get_inputs():\n    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    return [Q, K, V]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        return out\n\nbatch_size = 32\nnum_heads = 32\nsequence_length = 512\nembedding_dimension = 1024\n\ndef get_inputs():\n    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    return [Q, K, V]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "97_ScaledDotProductAttention", "problem_id": 97}, "task_id": "1-97"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')\n\nbatch_size = 8192 * 2\ninput_shape = (8192 * 2,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')\n\nbatch_size = 8192 * 2\ninput_shape = (8192 * 2,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')\n\nbatch_size = 8192 * 2\ninput_shape = (8192 * 2,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "98_KLDivLoss", "problem_id": 98}, "task_id": "1-98"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Triplet Margin Loss for metric learning tasks.\n\n    Parameters:\n        margin (float): The margin between the positive and negative samples.\n    \"\"\"\n    def __init__(self, margin=1.0):\n        super(Model, self).__init__()\n        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)\n\nbatch_size = 32768\ninput_shape = (8192,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]\n    \ndef get_init_inputs():\n    return [1.0]  # Default margin\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Triplet Margin Loss for metric learning tasks.\n\n    Parameters:\n        margin (float): The margin between the positive and negative samples.\n    \"\"\"\n    def __init__(self, margin=1.0):\n        super(Model, self).__init__()\n        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)\n\nbatch_size = 32768\ninput_shape = (8192,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]\n    \ndef get_init_inputs():\n    return [1.0]  # Default margin\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Triplet Margin Loss for metric learning tasks.\n\n    Parameters:\n        margin (float): The margin between the positive and negative samples.\n    \"\"\"\n    def __init__(self, margin=1.0):\n        super(Model, self).__init__()\n        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)\n\nbatch_size = 32768\ninput_shape = (8192,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]\n    \ndef get_init_inputs():\n    return [1.0]  # Default margin\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "99_TripletMarginLoss", "problem_id": 99}, "task_id": "1-99"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "9_Tall_skinny_matrix_multiplication_", "problem_id": 9}, "task_id": "1-9"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a transposed 3D convolution, clamps the output to a minimum value, \n    and then divides the result by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, min_value, divisor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.min_value = min_value\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.clamp(x, min=self.min_value)\n        x = x / self.divisor\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\ndepth, height, width = 24, 48, 48\nkernel_size = 3\nstride = 2\npadding = 1\nmin_value = -1.0\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, min_value, divisor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a transposed 3D convolution, clamps the output to a minimum value, \n    and then divides the result by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, min_value, divisor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.min_value = min_value\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.clamp(x, min=self.min_value)\n        x = x / self.divisor\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\ndepth, height, width = 24, 48, 48\nkernel_size = 3\nstride = 2\npadding = 1\nmin_value = -1.0\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, min_value, divisor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a transposed 3D convolution, clamps the output to a minimum value, \n    and then divides the result by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, min_value, divisor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.min_value = min_value\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.clamp(x, min=self.min_value)\n        x = x / self.divisor\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\ndepth, height, width = 24, 48, 48\nkernel_size = 3\nstride = 2\npadding = 1\nmin_value = -1.0\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, min_value, divisor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "100_ConvTranspose3d_Clamp_Min_Divide", "problem_id": 100}, "task_id": "2-100"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, followed by max pooling, hardtanh activation, mean operation, and tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=maxpool_stride)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.maxpool(x)\n        x = self.hardtanh(x)\n        x = torch.mean(x, dim=(2, 3), keepdim=True)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 256  \nkernel_size  = 3\nstride = 1\npadding = 1\nmaxpool_kernel_size = 2\nmaxpool_stride = 2\nhardtanh_min = -1\nhardtanh_max = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, followed by max pooling, hardtanh activation, mean operation, and tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=maxpool_stride)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.maxpool(x)\n        x = self.hardtanh(x)\n        x = torch.mean(x, dim=(2, 3), keepdim=True)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 256  \nkernel_size  = 3\nstride = 1\npadding = 1\nmaxpool_kernel_size = 2\nmaxpool_stride = 2\nhardtanh_min = -1\nhardtanh_max = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, followed by max pooling, hardtanh activation, mean operation, and tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=maxpool_stride)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.maxpool(x)\n        x = self.hardtanh(x)\n        x = torch.mean(x, dim=(2, 3), keepdim=True)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 256  \nkernel_size  = 3\nstride = 1\npadding = 1\nmaxpool_kernel_size = 2\nmaxpool_stride = 2\nhardtanh_min = -1\nhardtanh_max = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "10_ConvTranspose2d_MaxPool_Hardtanh_Mean_Tanh", "problem_id": 10}, "task_id": "2-10"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, batch normalization, tanh activation, max pooling, and group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, num_groups):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.tanh = nn.Tanh()\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = self.tanh(x)\n        x = self.max_pool(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 512\nin_channels  = 64  \nout_channels = 128  \nheight = width = 2048  \nkernel_size  = 5\nstride       = 1  \npadding      = 1\ngroups       = 8\nnum_groups   = 8\nheight, width = 32, 32\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups, num_groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, batch normalization, tanh activation, max pooling, and group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, num_groups):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.tanh = nn.Tanh()\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = self.tanh(x)\n        x = self.max_pool(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 512\nin_channels  = 64  \nout_channels = 128  \nheight = width = 2048  \nkernel_size  = 5\nstride       = 1  \npadding      = 1\ngroups       = 8\nnum_groups   = 8\nheight, width = 32, 32\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups, num_groups]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, batch normalization, tanh activation, max pooling, and group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, num_groups):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.tanh = nn.Tanh()\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = self.tanh(x)\n        x = self.max_pool(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 512\nin_channels  = 64  \nout_channels = 128  \nheight = width = 2048  \nkernel_size  = 5\nstride       = 1  \npadding      = 1\ngroups       = 8\nnum_groups   = 8\nheight, width = 32, 32\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups, num_groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "11_ConvTranspose2d_BatchNorm_Tanh_MaxPool_GroupNorm", "problem_id": 11}, "task_id": "2-11"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Gemm, multiplies the result, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.multiplier = multiplier\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        return x\n\nbatch_size = 1024\nin_features  = 8192  \nout_features = 8192\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Gemm, multiplies the result, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.multiplier = multiplier\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        return x\n\nbatch_size = 1024\nin_features  = 8192  \nout_features = 8192\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Gemm, multiplies the result, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.multiplier = multiplier\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        return x\n\nbatch_size = 1024\nin_features  = 8192  \nout_features = 8192\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "12_Gemm_Multiply_LeakyReLU", "problem_id": 12}, "task_id": "2-12"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a series of operations:\n    1. Transposed 3D convolution\n    2. Mean pooling (across depth)\n    3. Addition\n    4. Softmax (across channels)\n    5. Tanh activation\n    6. Scaling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))  # Broadcastable bias over channels\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)                            # (B, C, D, H, W)\n        x = x.mean(dim=2, keepdim=True)                       # Mean pool over depth dim (D)\n        x = x + self.bias                                     # Bias add per channel\n        x = torch.softmax(x, dim=1)                           # Softmax over channels\n        x = torch.tanh(x)                                     # Nonlinearity\n        x = x * self.scaling_factor                           # Scaling\n        return x\n\n# === Test config ===\nbatch_size = 16\nin_channels  = 16  \nout_channels = 64  \ndepth = 32; height = width = 128  \nkernel_size  = 3\nstride       = 1  \npadding = 1\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scaling_factor]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a series of operations:\n    1. Transposed 3D convolution\n    2. Mean pooling (across depth)\n    3. Addition\n    4. Softmax (across channels)\n    5. Tanh activation\n    6. Scaling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))  # Broadcastable bias over channels\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)                            # (B, C, D, H, W)\n        x = x.mean(dim=2, keepdim=True)                       # Mean pool over depth dim (D)\n        x = x + self.bias                                     # Bias add per channel\n        x = torch.softmax(x, dim=1)                           # Softmax over channels\n        x = torch.tanh(x)                                     # Nonlinearity\n        x = x * self.scaling_factor                           # Scaling\n        return x\n\n# === Test config ===\nbatch_size = 16\nin_channels  = 16  \nout_channels = 64  \ndepth = 32; height = width = 128  \nkernel_size  = 3\nstride       = 1  \npadding = 1\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scaling_factor]\n", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a series of operations:\n    1. Transposed 3D convolution\n    2. Mean pooling (across depth)\n    3. Addition\n    4. Softmax (across channels)\n    5. Tanh activation\n    6. Scaling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))  # Broadcastable bias over channels\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)                            # (B, C, D, H, W)\n        x = x.mean(dim=2, keepdim=True)                       # Mean pool over depth dim (D)\n        x = x + self.bias                                     # Bias add per channel\n        x = torch.softmax(x, dim=1)                           # Softmax over channels\n        x = torch.tanh(x)                                     # Nonlinearity\n        x = x * self.scaling_factor                           # Scaling\n        return x\n\n# === Test config ===\nbatch_size = 16\nin_channels  = 16  \nout_channels = 64  \ndepth = 32; height = width = 128  \nkernel_size  = 3\nstride       = 1  \npadding = 1\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scaling_factor]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "13_ConvTranspose3d_Mean_Add_Softmax_Tanh_Scaling", "problem_id": 13}, "task_id": "2-13"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, division, summation, and scaling.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = torch.matmul(x, self.weight.T)  # Gemm\n        x = x / 2  # Divide\n        x = torch.sum(x, dim=1, keepdim=True) # Sum\n        x = x * self.scaling_factor  # Scaling\n        return x\n\n\nbatch_size   = 1024  \ninput_size   = 8192  \nhidden_size  = 8192 \nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, division, summation, and scaling.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = torch.matmul(x, self.weight.T)  # Gemm\n        x = x / 2  # Divide\n        x = torch.sum(x, dim=1, keepdim=True) # Sum\n        x = x * self.scaling_factor  # Scaling\n        return x\n\n\nbatch_size   = 1024  \ninput_size   = 8192  \nhidden_size  = 8192 \nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, division, summation, and scaling.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = torch.matmul(x, self.weight.T)  # Gemm\n        x = x / 2  # Divide\n        x = torch.sum(x, dim=1, keepdim=True) # Sum\n        x = x * self.scaling_factor  # Scaling\n        return x\n\n\nbatch_size   = 1024  \ninput_size   = 8192  \nhidden_size  = 8192 \nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "14_Gemm_Divide_Sum_Scaling", "problem_id": 14}, "task_id": "2-14"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A 3D convolutional transpose layer followed by Batch Normalization and subtraction.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.batch_norm = nn.BatchNorm3d(out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = x - torch.mean(x, dim=(2, 3, 4), keepdim=True)  # Subtract mean along spatial dimensions\n        return x\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A 3D convolutional transpose layer followed by Batch Normalization and subtraction.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.batch_norm = nn.BatchNorm3d(out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = x - torch.mean(x, dim=(2, 3, 4), keepdim=True)  # Subtract mean along spatial dimensions\n        return x\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A 3D convolutional transpose layer followed by Batch Normalization and subtraction.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.batch_norm = nn.BatchNorm3d(out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = x - torch.mean(x, dim=(2, 3, 4), keepdim=True)  # Subtract mean along spatial dimensions\n        return x\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "15_ConvTranspose3d_BatchNorm_Subtract", "problem_id": 15}, "task_id": "2-15"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies Mish activation, adds a value, \n    applies Hardtanh activation, and scales the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.add_value = add_value\n        self.scale = scale\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.nn.functional.mish(x) # Mish activation\n        x = x + self.add_value\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh activation\n        x = x * self.scale # Scaling\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 128  \nkernel_size  = 3\nstride       = 2  \npadding      = 1\noutput_padding = 1\nadd_value = 0.5\nscale = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies Mish activation, adds a value, \n    applies Hardtanh activation, and scales the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.add_value = add_value\n        self.scale = scale\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.nn.functional.mish(x) # Mish activation\n        x = x + self.add_value\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh activation\n        x = x * self.scale # Scaling\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 128  \nkernel_size  = 3\nstride       = 2  \npadding      = 1\noutput_padding = 1\nadd_value = 0.5\nscale = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies Mish activation, adds a value, \n    applies Hardtanh activation, and scales the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.add_value = add_value\n        self.scale = scale\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.nn.functional.mish(x) # Mish activation\n        x = x + self.add_value\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh activation\n        x = x * self.scale # Scaling\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 128  \nkernel_size  = 3\nstride       = 2  \npadding      = 1\noutput_padding = 1\nadd_value = 0.5\nscale = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "16_ConvTranspose2d_Mish_Add_Hardtanh_Scaling", "problem_id": 16}, "task_id": "2-16"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.instance_norm = nn.InstanceNorm2d(out_channels)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.instance_norm(x)\n        x = x / self.divide_by\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 128  \nheight = width = 128  \nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.instance_norm = nn.InstanceNorm2d(out_channels)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.instance_norm(x)\n        x = x / self.divide_by\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 128  \nheight = width = 128  \nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.instance_norm = nn.InstanceNorm2d(out_channels)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.instance_norm(x)\n        x = x / self.divide_by\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 128  \nheight = width = 128  \nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "17_Conv2d_InstanceNorm_Divide", "problem_id": 17}, "task_id": "2-17"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a sequence of operations:\n        - Matrix multiplication\n        - Summation\n        - Max\n        - Average pooling\n        - LogSumExp\n        - LogSumExp\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)  # (batch_size, out_features)\n        x = torch.sum(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.max(x, dim=1, keepdim=True)[0] # (batch_size, 1)\n        x = torch.mean(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        return x\n\nbatch_size = 1024\nin_features  = 8192  \nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a sequence of operations:\n        - Matrix multiplication\n        - Summation\n        - Max\n        - Average pooling\n        - LogSumExp\n        - LogSumExp\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)  # (batch_size, out_features)\n        x = torch.sum(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.max(x, dim=1, keepdim=True)[0] # (batch_size, 1)\n        x = torch.mean(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        return x\n\nbatch_size = 1024\nin_features  = 8192  \nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a sequence of operations:\n        - Matrix multiplication\n        - Summation\n        - Max\n        - Average pooling\n        - LogSumExp\n        - LogSumExp\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)  # (batch_size, out_features)\n        x = torch.sum(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.max(x, dim=1, keepdim=True)[0] # (batch_size, 1)\n        x = torch.mean(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        return x\n\nbatch_size = 1024\nin_features  = 8192  \nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "18_Matmul_Sum_Max_AvgPool_LogSumExp_LogSumExp", "problem_id": 18}, "task_id": "2-18"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies GELU, and normalizes with GroupNorm.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)\n        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size   = 128  \nin_channels  = 64  \nout_channels = 64  \nheight = width = 256  \nkernel_size  = 3\nstride       = 1\ngroups = 8\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, groups, num_groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies GELU, and normalizes with GroupNorm.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)\n        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size   = 128  \nin_channels  = 64  \nout_channels = 64  \nheight = width = 256  \nkernel_size  = 3\nstride       = 1\ngroups = 8\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, groups, num_groups]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies GELU, and normalizes with GroupNorm.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)\n        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size   = 128  \nin_channels  = 64  \nout_channels = 64  \nheight = width = 256  \nkernel_size  = 3\nstride       = 1\ngroups = 8\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, groups, num_groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "19_ConvTranspose2d_GELU_GroupNorm", "problem_id": 19}, "task_id": "2-19"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and adds a bias term.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 128  \nheight = width = 128\nkernel_size = 3\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and adds a bias term.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 128  \nheight = width = 128\nkernel_size = 3\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and adds a bias term.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 128  \nheight = width = 128\nkernel_size = 3\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "1_Conv2D_ReLU_BiasAdd", "problem_id": 1}, "task_id": "2-1"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, \n    a residual add, a multiplication, and another residual add.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        original_x = x.clone().detach()\n        x = x + self.bias\n        x = x + original_x\n        x = x * original_x\n        x = x + original_x\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, \n    a residual add, a multiplication, and another residual add.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        original_x = x.clone().detach()\n        x = x + self.bias\n        x = x + original_x\n        x = x * original_x\n        x = x + original_x\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, \n    a residual add, a multiplication, and another residual add.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        original_x = x.clone().detach()\n        x = x + self.bias\n        x = x + original_x\n        x = x * original_x\n        x = x + original_x\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "20_ConvTranspose3d_Sum_ResidualAdd_Multiply_ResidualAdd", "problem_id": 20}, "task_id": "2-20"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, adds a bias term, scales, applies sigmoid, and performs group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x + self.bias\n        x = x * self.scale\n        x = torch.sigmoid(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 32\nheight = width = 256\nkernel_size = 3\nnum_groups = 8\nbias_shape = (out_channels, 1, 1)\nscale_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, adds a bias term, scales, applies sigmoid, and performs group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x + self.bias\n        x = x * self.scale\n        x = torch.sigmoid(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 32\nheight = width = 256\nkernel_size = 3\nnum_groups = 8\nbias_shape = (out_channels, 1, 1)\nscale_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, adds a bias term, scales, applies sigmoid, and performs group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x + self.bias\n        x = x * self.scale\n        x = torch.sigmoid(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 32\nheight = width = 256\nkernel_size = 3\nnum_groups = 8\nbias_shape = (out_channels, 1, 1)\nscale_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "21_Conv2d_Add_Scale_Sigmoid_GroupNorm", "problem_id": 21}, "task_id": "2-21"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, scales the result, adds a residual connection, clamps the output,\n    applies LogSumExp, and finally applies the Mish activation function.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(input_size, hidden_size)\n        self.scale_factor = scale_factor\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.matmul(x)\n        x = x * self.scale_factor\n        x = x + x\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = x * torch.nn.functional.mish(x)  # Mish activation\n        return x\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nscale_factor = 2.0\nclamp_min = -10.0\nclamp_max = 10.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scale_factor, clamp_min, clamp_max]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, scales the result, adds a residual connection, clamps the output,\n    applies LogSumExp, and finally applies the Mish activation function.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(input_size, hidden_size)\n        self.scale_factor = scale_factor\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.matmul(x)\n        x = x * self.scale_factor\n        x = x + x\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = x * torch.nn.functional.mish(x)  # Mish activation\n        return x\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nscale_factor = 2.0\nclamp_min = -10.0\nclamp_max = 10.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scale_factor, clamp_min, clamp_max]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, scales the result, adds a residual connection, clamps the output,\n    applies LogSumExp, and finally applies the Mish activation function.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(input_size, hidden_size)\n        self.scale_factor = scale_factor\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.matmul(x)\n        x = x * self.scale_factor\n        x = x + x\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = x * torch.nn.functional.mish(x)  # Mish activation\n        return x\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nscale_factor = 2.0\nclamp_min = -10.0\nclamp_max = 10.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scale_factor, clamp_min, clamp_max]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "22_Matmul_Scale_ResidualAdd_Clamp_LogSumExp_Mish", "problem_id": 22}, "task_id": "2-22"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Group Normalization, computes the mean\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x.mean(dim=[1, 2, 3, 4]) # Compute mean across all dimensions except batch\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 24\nD, H, W = 24, 32, 32\nkernel_size = 3\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Group Normalization, computes the mean\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x.mean(dim=[1, 2, 3, 4]) # Compute mean across all dimensions except batch\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 24\nD, H, W = 24, 32, 32\nkernel_size = 3\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Group Normalization, computes the mean\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x.mean(dim=[1, 2, 3, 4]) # Compute mean across all dimensions except batch\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 24\nD, H, W = 24, 32, 32\nkernel_size = 3\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "23_Conv3d_GroupNorm_Mean", "problem_id": 23}, "task_id": "2-23"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a 3D convolution, applies minimum operation along a specific dimension, \n    and then applies softmax.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, dim):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W)\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, H, W)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.min(x, dim=self.dim)[0]  # Apply minimum along the specified dimension\n        x = torch.softmax(x, dim=1)  # Apply softmax along the channel dimension\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 24  # Increased output channels\nD, H, W = 24, 32, 32  # Increased depth\nkernel_size = 3\ndim = 2  # Dimension along which to apply minimum operation (e.g., depth)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a 3D convolution, applies minimum operation along a specific dimension, \n    and then applies softmax.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, dim):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W)\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, H, W)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.min(x, dim=self.dim)[0]  # Apply minimum along the specified dimension\n        x = torch.softmax(x, dim=1)  # Apply softmax along the channel dimension\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 24  # Increased output channels\nD, H, W = 24, 32, 32  # Increased depth\nkernel_size = 3\ndim = 2  # Dimension along which to apply minimum operation (e.g., depth)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, dim]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a 3D convolution, applies minimum operation along a specific dimension, \n    and then applies softmax.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, dim):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W)\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, H, W)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.min(x, dim=self.dim)[0]  # Apply minimum along the specified dimension\n        x = torch.softmax(x, dim=1)  # Apply softmax along the channel dimension\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 24  # Increased output channels\nD, H, W = 24, 32, 32  # Increased depth\nkernel_size = 3\ndim = 2  # Dimension along which to apply minimum operation (e.g., depth)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "24_Conv3d_Min_Softmax", "problem_id": 24}, "task_id": "2-24"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies minimum operation, Tanh, and another Tanh.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] # Apply minimum operation along the channel dimension\n        x = torch.tanh(x)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 128\nin_channels = 16\nout_channels = 64\nheight = width = 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies minimum operation, Tanh, and another Tanh.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] # Apply minimum operation along the channel dimension\n        x = torch.tanh(x)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 128\nin_channels = 16\nout_channels = 64\nheight = width = 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies minimum operation, Tanh, and another Tanh.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] # Apply minimum operation along the channel dimension\n        x = torch.tanh(x)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 128\nin_channels = 16\nout_channels = 64\nheight = width = 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "25_Conv2d_Min_Tanh_Tanh", "problem_id": 25}, "task_id": "2-25"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, adds an input tensor, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x, add_input):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n            add_input (torch.Tensor): Input tensor to be added after transposed convolution, of shape (batch_size, out_channels, D, H, W).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W) after HardSwish activation.\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = x + add_input\n        x = x * torch.nn.functional.hardswish(x)\n        return x\n\n\nbatch_size = 128\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 16, 16\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W), torch.rand(batch_size, out_channels, D*stride, H*stride, W*stride)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, adds an input tensor, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x, add_input):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n            add_input (torch.Tensor): Input tensor to be added after transposed convolution, of shape (batch_size, out_channels, D, H, W).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W) after HardSwish activation.\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = x + add_input\n        x = x * torch.nn.functional.hardswish(x)\n        return x\n\n\nbatch_size = 128\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 16, 16\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W), torch.rand(batch_size, out_channels, D*stride, H*stride, W*stride)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, adds an input tensor, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x, add_input):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n            add_input (torch.Tensor): Input tensor to be added after transposed convolution, of shape (batch_size, out_channels, D, H, W).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W) after HardSwish activation.\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = x + add_input\n        x = x * torch.nn.functional.hardswish(x)\n        return x\n\n\nbatch_size = 128\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 16, 16\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W), torch.rand(batch_size, out_channels, D*stride, H*stride, W*stride)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "26_ConvTranspose3d_Add_HardSwish", "problem_id": 26}, "task_id": "2-26"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs:\n    1. Conv3D\n    2. HardSwish activation\n    3. GroupNorm  \n    4. Mean pooling across spatial dimensions\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)                             # (B, C, D, H, W)\n        x = F.hardswish(x)                           # Nonlinear activation\n        x = self.group_norm(x)                       # Normalization over channels\n        x = torch.mean(x, dim=[2, 3, 4])             # Mean over spatial dims \u2192 (B, C)\n        return x\n\n# === Test config ===\nbatch_size = 1024\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs:\n    1. Conv3D\n    2. HardSwish activation\n    3. GroupNorm  \n    4. Mean pooling across spatial dimensions\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)                             # (B, C, D, H, W)\n        x = F.hardswish(x)                           # Nonlinear activation\n        x = self.group_norm(x)                       # Normalization over channels\n        x = torch.mean(x, dim=[2, 3, 4])             # Mean over spatial dims \u2192 (B, C)\n        return x\n\n# === Test config ===\nbatch_size = 1024\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs:\n    1. Conv3D\n    2. HardSwish activation\n    3. GroupNorm  \n    4. Mean pooling across spatial dimensions\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)                             # (B, C, D, H, W)\n        x = F.hardswish(x)                           # Nonlinear activation\n        x = self.group_norm(x)                       # Normalization over channels\n        x = torch.mean(x, dim=[2, 3, 4])             # Mean over spatial dims \u2192 (B, C)\n        return x\n\n# === Test config ===\nbatch_size = 1024\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "27_Conv3d_HardSwish_GroupNorm_Mean", "problem_id": 27}, "task_id": "2-27"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 1024  # Increased batch size\nin_features = 8192  # Increased input features\nout_features = 8192  # Increased output features\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features), torch.rand(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 1024  # Increased batch size\nin_features = 8192  # Increased input features\nout_features = 8192  # Increased output features\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features), torch.rand(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 1024  # Increased batch size\nin_features = 8192  # Increased input features\nout_features = 8192  # Increased output features\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features), torch.rand(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply", "problem_id": 28}, "task_id": "2-28"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "29_Matmul_Mish_Mish", "problem_id": 29}, "task_id": "2-29"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, adds a bias term, clamps, scales, clamps, and divides.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.bias\n        x = torch.clamp(x, min=0.0, max=1.0)\n        x = x * self.scaling_factor\n        x = torch.clamp(x, min=0.0, max=1.0)\n        x = x / self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 128 \nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, adds a bias term, clamps, scales, clamps, and divides.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.bias\n        x = torch.clamp(x, min=0.0, max=1.0)\n        x = x * self.scaling_factor\n        x = torch.clamp(x, min=0.0, max=1.0)\n        x = x / self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 128 \nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, adds a bias term, clamps, scales, clamps, and divides.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.bias\n        x = torch.clamp(x, min=0.0, max=1.0)\n        x = x * self.scaling_factor\n        x = torch.clamp(x, min=0.0, max=1.0)\n        x = x / self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels  = 64  \nout_channels = 64  \nheight = width = 128 \nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "2_ConvTranspose2d_BiasAdd_Clamp_Scaling_Clamp_Divide", "problem_id": 2}, "task_id": "2-2"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM, applies Group Normalization, and then HardTanh.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 16\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM, applies Group Normalization, and then HardTanh.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 16\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM, applies Group Normalization, and then HardTanh.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 16\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "30_Gemm_GroupNorm_Hardtanh", "problem_id": 30}, "task_id": "2-30"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, takes the minimum with a constant, adds a bias term, and multiplies by a scaling factor.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.constant_value = constant_value\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.min(x, torch.tensor(self.constant_value))\n        x = x + self.bias\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight = width = 128\nkernel_size = 3\nconstant_value = 0.5\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, takes the minimum with a constant, adds a bias term, and multiplies by a scaling factor.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.constant_value = constant_value\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.min(x, torch.tensor(self.constant_value))\n        x = x + self.bias\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight = width = 128\nkernel_size = 3\nconstant_value = 0.5\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, takes the minimum with a constant, adds a bias term, and multiplies by a scaling factor.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.constant_value = constant_value\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.min(x, torch.tensor(self.constant_value))\n        x = x + self.bias\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight = width = 128\nkernel_size = 3\nconstant_value = 0.5\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "31_Conv2d_Min_Add_Multiply", "problem_id": 31}, "task_id": "2-31"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, scales the output, and then applies a minimum operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.conv(x)\n        x = x * self.scale_factor\n        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum along channel dimension\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nheight = width = 256\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, scales the output, and then applies a minimum operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.conv(x)\n        x = x * self.scale_factor\n        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum along channel dimension\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nheight = width = 256\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, scales the output, and then applies a minimum operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.conv(x)\n        x = x * self.scale_factor\n        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum along channel dimension\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nheight = width = 256\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "32_Conv2d_Scaling_Min", "problem_id": 32}, "task_id": "2-32"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM (general matrix multiplication), applies scaling, \n    and then batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM (general matrix multiplication), applies scaling, \n    and then batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM (general matrix multiplication), applies scaling, \n    and then batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "33_Gemm_Scale_BatchNorm", "problem_id": 33}, "task_id": "2-33"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, layer normalization, GELU activation, and scaling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True, eps=1e-5, scaling_factor=1.0):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.layer_norm = nn.LayerNorm(out_channels, eps=eps)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.layer_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 32, 32\nkernel_size = 4\nstride = 2\npadding = 1\nbias = True\neps = 1e-5\nscaling_factor = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias, eps, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, layer normalization, GELU activation, and scaling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True, eps=1e-5, scaling_factor=1.0):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.layer_norm = nn.LayerNorm(out_channels, eps=eps)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.layer_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 32, 32\nkernel_size = 4\nstride = 2\npadding = 1\nbias = True\neps = 1e-5\nscaling_factor = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias, eps, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, layer normalization, GELU activation, and scaling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True, eps=1e-5, scaling_factor=1.0):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.layer_norm = nn.LayerNorm(out_channels, eps=eps)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.layer_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 32, 32\nkernel_size = 4\nstride = 2\npadding = 1\nbias = True\neps = 1e-5\nscaling_factor = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias, eps, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "34_ConvTranspose3d_LayerNorm_GELU_Scaling", "problem_id": 34}, "task_id": "2-34"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtracts a value, applies HardSwish, MaxPool, and Mish activation functions.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract_value = subtract_value\n        self.pool = nn.MaxPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract_value\n        x = torch.nn.functional.hardswish(x)\n        x = self.pool(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight = width = 128\nkernel_size = 3\nsubtract_value = 0.5\npool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtracts a value, applies HardSwish, MaxPool, and Mish activation functions.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract_value = subtract_value\n        self.pool = nn.MaxPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract_value\n        x = torch.nn.functional.hardswish(x)\n        x = self.pool(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight = width = 128\nkernel_size = 3\nsubtract_value = 0.5\npool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtracts a value, applies HardSwish, MaxPool, and Mish activation functions.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract_value = subtract_value\n        self.pool = nn.MaxPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract_value\n        x = torch.nn.functional.hardswish(x)\n        x = self.pool(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight = width = 128\nkernel_size = 3\nsubtract_value = 0.5\npool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "35_Conv2d_Subtract_HardSwish_MaxPool_Mish", "problem_id": 35}, "task_id": "2-35"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a convolution transpose, minimum operation, sum operation, GELU activation and addition.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum operation along channel dimension\n        x = torch.sum(x, dim=2, keepdim=True)  # Sum operation along height dimension\n        x = torch.nn.functional.gelu(x)  # GELU activation\n        x = x + self.bias\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a convolution transpose, minimum operation, sum operation, GELU activation and addition.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum operation along channel dimension\n        x = torch.sum(x, dim=2, keepdim=True)  # Sum operation along height dimension\n        x = torch.nn.functional.gelu(x)  # GELU activation\n        x = x + self.bias\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a convolution transpose, minimum operation, sum operation, GELU activation and addition.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum operation along channel dimension\n        x = torch.sum(x, dim=2, keepdim=True)  # Sum operation along height dimension\n        x = torch.nn.functional.gelu(x)  # GELU activation\n        x = x + self.bias\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "36_ConvTranspose2d_Min_Sum_GELU_Add", "problem_id": 36}, "task_id": "2-36"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, applies Swish activation, sums with a bias term, and normalizes with GroupNorm.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = torch.sigmoid(x) * x  # Swish activation\n        x = x + self.bias\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 32768\nin_features = 1024\nout_features = 4096\nnum_groups = 64\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, applies Swish activation, sums with a bias term, and normalizes with GroupNorm.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = torch.sigmoid(x) * x  # Swish activation\n        x = x + self.bias\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 32768\nin_features = 1024\nout_features = 4096\nnum_groups = 64\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, applies Swish activation, sums with a bias term, and normalizes with GroupNorm.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = torch.sigmoid(x) * x  # Swish activation\n        x = x + self.bias\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 32768\nin_features = 1024\nout_features = 4096\nnum_groups = 64\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "37_Matmul_Swish_Sum_GroupNorm", "problem_id": 37}, "task_id": "2-37"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs average pooling, 3D transposed convolution, clamping,\n    spatial softmax, and multiplication by a learnable scale.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(pool_kernel_size)\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth, height, width).\n        \"\"\"\n        x = self.avg_pool(x)\n        x = self.conv_transpose(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        b, c, d, h, w = x.shape\n        x = x.view(b, c, -1)                     # flatten spatial dims\n        x = torch.softmax(x, dim=2)\n        x = x.view(b, c, d, h, w)\n        x = x * self.scale\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 64, 64\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs average pooling, 3D transposed convolution, clamping,\n    spatial softmax, and multiplication by a learnable scale.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(pool_kernel_size)\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth, height, width).\n        \"\"\"\n        x = self.avg_pool(x)\n        x = self.conv_transpose(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        b, c, d, h, w = x.shape\n        x = x.view(b, c, -1)                     # flatten spatial dims\n        x = torch.softmax(x, dim=2)\n        x = x.view(b, c, d, h, w)\n        x = x * self.scale\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 64, 64\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max]\n", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs average pooling, 3D transposed convolution, clamping,\n    spatial softmax, and multiplication by a learnable scale.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(pool_kernel_size)\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth, height, width).\n        \"\"\"\n        x = self.avg_pool(x)\n        x = self.conv_transpose(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        b, c, d, h, w = x.shape\n        x = x.view(b, c, -1)                     # flatten spatial dims\n        x = torch.softmax(x, dim=2)\n        x = x.view(b, c, d, h, w)\n        x = x * self.scale\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 64, 64\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "38_ConvTranspose3d_AvgPool_Clamp_Softmax_Multiply", "problem_id": 38}, "task_id": "2-38"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "39_Gemm_Scale_BatchNorm", "problem_id": 39}, "task_id": "2-39"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, layer normalization, average pooling, and GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.sum_weight = nn.Parameter(torch.tensor(sum_weight))\n        self.norm = nn.LayerNorm(norm_shape)\n        self.avg_pool = nn.AvgPool3d(kernel_size=pool_kernel_size)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.sum_weight\n        x = self.norm(x)\n        x = self.avg_pool(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = (3, 3, 3)\nstride = (2, 2, 2)\npadding = (1, 1, 1)\noutput_padding = (1, 1, 1)\nsum_weight = 1.0\nnorm_shape = (out_channels,)\npool_kernel_size = (2, 2, 2)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, layer normalization, average pooling, and GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.sum_weight = nn.Parameter(torch.tensor(sum_weight))\n        self.norm = nn.LayerNorm(norm_shape)\n        self.avg_pool = nn.AvgPool3d(kernel_size=pool_kernel_size)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.sum_weight\n        x = self.norm(x)\n        x = self.avg_pool(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = (3, 3, 3)\nstride = (2, 2, 2)\npadding = (1, 1, 1)\noutput_padding = (1, 1, 1)\nsum_weight = 1.0\nnorm_shape = (out_channels,)\npool_kernel_size = (2, 2, 2)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, layer normalization, average pooling, and GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.sum_weight = nn.Parameter(torch.tensor(sum_weight))\n        self.norm = nn.LayerNorm(norm_shape)\n        self.avg_pool = nn.AvgPool3d(kernel_size=pool_kernel_size)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.sum_weight\n        x = self.norm(x)\n        x = self.avg_pool(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 32\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = (3, 3, 3)\nstride = (2, 2, 2)\npadding = (1, 1, 1)\noutput_padding = (1, 1, 1)\nsum_weight = 1.0\nnorm_shape = (out_channels,)\npool_kernel_size = (2, 2, 2)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "3_ConvTranspose3d_Sum_LayerNorm_AvgPool_GELU", "problem_id": 3}, "task_id": "2-3"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        original_x = x.clone().detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        original_x = x.clone().detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        original_x = x.clone().detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "40_Matmul_Scaling_ResidualAdd", "problem_id": 40}, "task_id": "2-40"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, BatchNorm, GELU, and ReLU in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, BatchNorm, GELU, and ReLU in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, BatchNorm, GELU, and ReLU in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 16384\nin_features = 4096\nout_features = 4096\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "41_Gemm_BatchNorm_GELU_ReLU", "problem_id": 41}, "task_id": "2-41"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, global average pooling, adds a bias, applies log-sum-exp, sum, and multiplication.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.mean(x, dim=(2, 3), keepdim=True)  # Global average pooling\n        x = x + self.bias\n        x = torch.logsumexp(x, dim=1, keepdim=True)  # Log-sum-exp\n        x = torch.sum(x, dim=(2, 3))  # Sum\n        x = x * 10.0  # Multiplication\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight = width = 512\nkernel_size = 3\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, global average pooling, adds a bias, applies log-sum-exp, sum, and multiplication.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.mean(x, dim=(2, 3), keepdim=True)  # Global average pooling\n        x = x + self.bias\n        x = torch.logsumexp(x, dim=1, keepdim=True)  # Log-sum-exp\n        x = torch.sum(x, dim=(2, 3))  # Sum\n        x = x * 10.0  # Multiplication\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight = width = 512\nkernel_size = 3\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, global average pooling, adds a bias, applies log-sum-exp, sum, and multiplication.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.mean(x, dim=(2, 3), keepdim=True)  # Global average pooling\n        x = x + self.bias\n        x = torch.logsumexp(x, dim=1, keepdim=True)  # Log-sum-exp\n        x = torch.sum(x, dim=(2, 3))  # Sum\n        x = x * 10.0  # Multiplication\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight = width = 512\nkernel_size = 3\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "42_ConvTranspose2d_GlobalAvgPool_BiasAdd_LogSumExp_Sum_Multiply", "problem_id": 42}, "task_id": "2-42"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, max pooling, log sum exp, and ReLU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, depth, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels, depth', height', width')\n        \"\"\"\n        x = self.conv(x)\n        x = self.max_pool(x)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 4\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 128, 128\nkernel_size = 3\nstride = 1\npadding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, max pooling, log sum exp, and ReLU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, depth, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels, depth', height', width')\n        \"\"\"\n        x = self.conv(x)\n        x = self.max_pool(x)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 4\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 128, 128\nkernel_size = 3\nstride = 1\npadding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, max pooling, log sum exp, and ReLU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, depth, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels, depth', height', width')\n        \"\"\"\n        x = self.conv(x)\n        x = self.max_pool(x)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 4\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 128, 128\nkernel_size = 3\nstride = 1\npadding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "43_Conv3d_Max_LogSumExp_ReLU", "problem_id": 43}, "task_id": "2-43"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, multiplies by a scalar, applies global average pooling, \n    another global average pooling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = multiplier\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.multiplier\n        x = torch.mean(x, dim=[2, 3], keepdim=True)  # First global average pooling\n        x = torch.mean(x, dim=[2, 3], keepdim=True)  # Second global average pooling\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, multiplies by a scalar, applies global average pooling, \n    another global average pooling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = multiplier\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.multiplier\n        x = torch.mean(x, dim=[2, 3], keepdim=True)  # First global average pooling\n        x = torch.mean(x, dim=[2, 3], keepdim=True)  # Second global average pooling\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, multiplies by a scalar, applies global average pooling, \n    another global average pooling\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = multiplier\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.multiplier\n        x = torch.mean(x, dim=[2, 3], keepdim=True)  # First global average pooling\n        x = torch.mean(x, dim=[2, 3], keepdim=True)  # Second global average pooling\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "44_ConvTranspose2d_Multiply_GlobalAvgPool_GlobalAvgPool_Mean", "problem_id": 44}, "task_id": "2-44"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid,\n    another Gemm, and computes LogSumExp over features.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = self.linear2(x)\n        x = torch.logsumexp(x, dim=1)  # compute LogSumExp over features per sample\n        return x\n\nbatch_size = 16384\ninput_size = 2048\nhidden_size = 4096\noutput_size = 1024\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid,\n    another Gemm, and computes LogSumExp over features.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = self.linear2(x)\n        x = torch.logsumexp(x, dim=1)  # compute LogSumExp over features per sample\n        return x\n\nbatch_size = 16384\ninput_size = 2048\nhidden_size = 4096\noutput_size = 1024\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid,\n    another Gemm, and computes LogSumExp over features.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = self.linear2(x)\n        x = torch.logsumexp(x, dim=1)  # compute LogSumExp over features per sample\n        return x\n\nbatch_size = 16384\ninput_size = 2048\nhidden_size = 4096\noutput_size = 1024\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "45_Gemm_Sigmoid_LogSumExp", "problem_id": 45}, "task_id": "2-45"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtraction, tanh activation, subtraction and average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract1_value = subtract1_value\n        self.subtract2_value = subtract2_value\n        self.avgpool = nn.AvgPool2d(kernel_size_pool)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract1_value\n        x = torch.tanh(x)\n        x = x - self.subtract2_value\n        x = self.avgpool(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nsubtract1_value = 0.5\nsubtract2_value = 0.2\nkernel_size_pool = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtraction, tanh activation, subtraction and average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract1_value = subtract1_value\n        self.subtract2_value = subtract2_value\n        self.avgpool = nn.AvgPool2d(kernel_size_pool)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract1_value\n        x = torch.tanh(x)\n        x = x - self.subtract2_value\n        x = self.avgpool(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nsubtract1_value = 0.5\nsubtract2_value = 0.2\nkernel_size_pool = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtraction, tanh activation, subtraction and average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract1_value = subtract1_value\n        self.subtract2_value = subtract2_value\n        self.avgpool = nn.AvgPool2d(kernel_size_pool)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract1_value\n        x = torch.tanh(x)\n        x = x - self.subtract2_value\n        x = self.avgpool(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\nsubtract1_value = 0.5\nsubtract2_value = 0.2\nkernel_size_pool = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "46_Conv2d_Subtract_Tanh_Subtract_AvgPool", "problem_id": 46}, "task_id": "2-46"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Mish activation, and then applies Tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nD, H, W = 32, 64, 64\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Mish activation, and then applies Tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nD, H, W = 32, 64, 64\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Mish activation, and then applies Tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nD, H, W = 32, 64, 64\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "47_Conv3d_Mish_Tanh", "problem_id": 47}, "task_id": "2-47"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, scales the output, applies tanh, multiplies by a scaling factor, and applies sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scaling_factor \n        x = torch.tanh(x)\n        x = x * self.bias\n        x = torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 64, 64\nkernel_size = 3\nscaling_factor = 2\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, scales the output, applies tanh, multiplies by a scaling factor, and applies sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scaling_factor \n        x = torch.tanh(x)\n        x = x * self.bias\n        x = torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 64, 64\nkernel_size = 3\nscaling_factor = 2\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, scales the output, applies tanh, multiplies by a scaling factor, and applies sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scaling_factor \n        x = torch.tanh(x)\n        x = x * self.bias\n        x = torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 64, 64\nkernel_size = 3\nscaling_factor = 2\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "48_Conv3d_Scaling_Tanh_Multiply_Sigmoid", "problem_id": 48}, "task_id": "2-48"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies Softmax and Sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, bias=bias)\n        self.softmax = nn.Softmax(dim=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.softmax(x)\n        x = self.sigmoid(x)\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies Softmax and Sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, bias=bias)\n        self.softmax = nn.Softmax(dim=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.softmax(x)\n        x = self.sigmoid(x)\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies Softmax and Sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, bias=bias)\n        self.softmax = nn.Softmax(dim=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.softmax(x)\n        x = self.sigmoid(x)\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nD, H, W = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "49_ConvTranspose3d_Softmax_Sigmoid", "problem_id": 49}, "task_id": "2-49"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Mish, and another Mish.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size   = 64  \nin_channels  = 64  \nout_channels = 128  \nheight = width = 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Mish, and another Mish.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size   = 64  \nin_channels  = 64  \nout_channels = 128  \nheight = width = 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Mish, and another Mish.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size   = 64  \nin_channels  = 64  \nout_channels = 128  \nheight = width = 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "4_Conv2d_Mish_Mish", "problem_id": 4}, "task_id": "2-4"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, scaling, average pooling, bias addition, and scaling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.scale1 = nn.Parameter(torch.tensor(scale1))\n        self.avg_pool = nn.AvgPool3d(kernel_size=2)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scale2 = nn.Parameter(torch.tensor(scale2))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale1\n        x = self.avg_pool(x)\n        x = x + self.bias\n        x = x * self.scale2\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale1 = 0.5\nscale2 = 1.0\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, scaling, average pooling, bias addition, and scaling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.scale1 = nn.Parameter(torch.tensor(scale1))\n        self.avg_pool = nn.AvgPool3d(kernel_size=2)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scale2 = nn.Parameter(torch.tensor(scale2))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale1\n        x = self.avg_pool(x)\n        x = x + self.bias\n        x = x * self.scale2\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale1 = 0.5\nscale2 = 1.0\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, scaling, average pooling, bias addition, and scaling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.scale1 = nn.Parameter(torch.tensor(scale1))\n        self.avg_pool = nn.AvgPool3d(kernel_size=2)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scale2 = nn.Parameter(torch.tensor(scale2))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale1\n        x = self.avg_pool(x)\n        x = x + self.bias\n        x = x * self.scale2\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale1 = 0.5\nscale2 = 1.0\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "50_ConvTranspose3d_Scaling_AvgPool_BiasAdd_Scaling", "problem_id": 50}, "task_id": "2-50"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x.clone().detach()\n        # Gemm\n        x = self.gemm(x)\n\n        # Subtract\n        x = x - self.subtract\n\n        # GlobalAvgPool\n        x = torch.mean(x, dim=1, keepdim=True)\n\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n\n        # GELU\n        x = torch.nn.functional.gelu(x)\n\n        # ResidualAdd\n        x = x + original_x\n\n        return x\n\nbatch_size = 2048\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x.clone().detach()\n        # Gemm\n        x = self.gemm(x)\n\n        # Subtract\n        x = x - self.subtract\n\n        # GlobalAvgPool\n        x = torch.mean(x, dim=1, keepdim=True)\n\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n\n        # GELU\n        x = torch.nn.functional.gelu(x)\n\n        # ResidualAdd\n        x = x + original_x\n\n        return x\n\nbatch_size = 2048\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x.clone().detach()\n        # Gemm\n        x = self.gemm(x)\n\n        # Subtract\n        x = x - self.subtract\n\n        # GlobalAvgPool\n        x = torch.mean(x, dim=1, keepdim=True)\n\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n\n        # GELU\n        x = torch.nn.functional.gelu(x)\n\n        # ResidualAdd\n        x = x + original_x\n\n        return x\n\nbatch_size = 2048\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "51_Gemm_Subtract_GlobalAvgPool_LogSumExp_GELU_ResidualAdd", "problem_id": 51}, "task_id": "2-51"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies activation, and then applies Batch Normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.multiply(torch.tanh(torch.nn.functional.softplus(x)), x)\n        x = self.bn(x)\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies activation, and then applies Batch Normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.multiply(torch.tanh(torch.nn.functional.softplus(x)), x)\n        x = self.bn(x)\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies activation, and then applies Batch Normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.multiply(torch.tanh(torch.nn.functional.softplus(x)), x)\n        x = self.bn(x)\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "52_Conv2d_Activation_BatchNorm", "problem_id": 52}, "task_id": "2-52"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, scaling, hardtanh, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scaling_factor\n        x = self.hardtanh(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 2048\nin_features = 8192\nout_features = 8192\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, scaling, hardtanh, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scaling_factor\n        x = self.hardtanh(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 2048\nin_features = 8192\nout_features = 8192\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, scaling, hardtanh, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scaling_factor\n        x = self.hardtanh(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 2048\nin_features = 8192\nout_features = 8192\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "53_Gemm_Scaling_Hardtanh_GELU", "problem_id": 53}, "task_id": "2-53"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, multiplies by a learnable scalar, applies LeakyReLU, and then GELU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape)) \n        self.leaky_relu = nn.LeakyReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, multiplies by a learnable scalar, applies LeakyReLU, and then GELU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape)) \n        self.leaky_relu = nn.LeakyReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, multiplies by a learnable scalar, applies LeakyReLU, and then GELU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape)) \n        self.leaky_relu = nn.LeakyReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 64\nin_channels = 64\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "54_Conv2d_Multiply_LeakyReLU_GELU", "problem_id": 54}, "task_id": "2-54"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs matrix multiplication, max pooling, sum, and scaling.\n    \"\"\"\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n\nbatch_size = 128\nin_features = 32768\nout_features = 32768\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs matrix multiplication, max pooling, sum, and scaling.\n    \"\"\"\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n\nbatch_size = 128\nin_features = 32768\nout_features = 32768\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs matrix multiplication, max pooling, sum, and scaling.\n    \"\"\"\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n\nbatch_size = 128\nin_features = 32768\nout_features = 32768\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "55_Matmul_MaxPool_Sum_Scale", "problem_id": 55}, "task_id": "2-55"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies sigmoid, and sums the result.\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1, keepdim=True)\n        return x\n\nbatch_size = 128\ninput_size = 32768\nhidden_size = 32768\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies sigmoid, and sums the result.\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1, keepdim=True)\n        return x\n\nbatch_size = 128\ninput_size = 32768\nhidden_size = 32768\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies sigmoid, and sums the result.\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1, keepdim=True)\n        return x\n\nbatch_size = 128\ninput_size = 32768\nhidden_size = 32768\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "56_Matmul_Sigmoid_Sum", "problem_id": 56}, "task_id": "2-56"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x * torch.clamp((x + 3) / 6, 0, 1)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x * torch.clamp((x + 3) / 6, 0, 1)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x * torch.clamp((x + 3) / 6, 0, 1)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "57_Conv2d_ReLU_HardSwish", "problem_id": 57}, "task_id": "2-57"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, LogSumExp, HardSwish, subtraction, clamp operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.bias = nn.Parameter(torch.randn(1, 1, 1, 1)) \n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = x * torch.sigmoid(x + 3) / 6\n        x = x - self.bias\n        x = torch.clamp(x, min=-1, max=1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nbias_shape = (1, 1, 1, 1)  \n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, LogSumExp, HardSwish, subtraction, clamp operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.bias = nn.Parameter(torch.randn(1, 1, 1, 1)) \n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = x * torch.sigmoid(x + 3) / 6\n        x = x - self.bias\n        x = torch.clamp(x, min=-1, max=1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nbias_shape = (1, 1, 1, 1)  \n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]\n", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, LogSumExp, HardSwish, subtraction, clamp operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.bias = nn.Parameter(torch.randn(1, 1, 1, 1)) \n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        x = x * torch.sigmoid(x + 3) / 6\n        x = x - self.bias\n        x = torch.clamp(x, min=-1, max=1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nbias_shape = (1, 1, 1, 1)  \n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "58_ConvTranspose3d_LogSumExp_HardSwish_Subtract_Clamp", "problem_id": 58}, "task_id": "2-58"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_features = 32768\nout_features = 32768\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_features = 32768\nout_features = 32768\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_features = 32768\nout_features = 32768\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "59_Matmul_Swish_Scaling", "problem_id": 59}, "task_id": "2-59"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, subtracts a bias term, and applies tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x - self.bias\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 32\nin_channels  = 64  \nout_channels = 64  \nheight = width = 256 \nkernel_size = 4\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, subtracts a bias term, and applies tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x - self.bias\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 32\nin_channels  = 64  \nout_channels = 64  \nheight = width = 256 \nkernel_size = 4\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, subtracts a bias term, and applies tanh activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x - self.bias\n        x = torch.tanh(x)\n        return x\n\nbatch_size = 32\nin_channels  = 64  \nout_channels = 64  \nheight = width = 256 \nkernel_size = 4\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "5_ConvTranspose2d_Subtract_Tanh", "problem_id": 5}, "task_id": "2-5"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies Swish activation, \n    group normalization, and then HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.sigmoid(x) * x  # Swish activation\n        x = self.group_norm(x)\n        x = torch.nn.functional.hardswish(x)  # HardSwish activation\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\neps = 1e-5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies Swish activation, \n    group normalization, and then HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.sigmoid(x) * x  # Swish activation\n        x = self.group_norm(x)\n        x = torch.nn.functional.hardswish(x)  # HardSwish activation\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\neps = 1e-5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies Swish activation, \n    group normalization, and then HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.sigmoid(x) * x  # Swish activation\n        x = self.group_norm(x)\n        x = torch.nn.functional.hardswish(x)  # HardSwish activation\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\neps = 1e-5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "60_ConvTranspose3d_Swish_GroupNorm_HardSwish", "problem_id": 60}, "task_id": "2-60"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed 3D convolution, applies ReLU, and then applies group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=bias)\n        self.relu = nn.ReLU()\n        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.relu(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nD, H, W = 32, 32, 32\nkernel_size = 3\ngroups = 8\nbias = False\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups, bias]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed 3D convolution, applies ReLU, and then applies group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=bias)\n        self.relu = nn.ReLU()\n        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.relu(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nD, H, W = 32, 32, 32\nkernel_size = 3\ngroups = 8\nbias = False\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups, bias]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed 3D convolution, applies ReLU, and then applies group normalization.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=bias)\n        self.relu = nn.ReLU()\n        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).\n        \"\"\"\n        x = self.conv_transpose(x)\n        x = self.relu(x)\n        x = self.group_norm(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nD, H, W = 32, 32, 32\nkernel_size = 3\ngroups = 8\nbias = False\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups, bias]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "61_ConvTranspose3d_ReLU_GroupNorm", "problem_id": 61}, "task_id": "2-61"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.fc(x)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nnum_groups = 512\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.fc(x)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nnum_groups = 512\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.fc(x)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nnum_groups = 512\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "62_Matmul_GroupNorm_LeakyReLU_Sum", "problem_id": 62}, "task_id": "2-62"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies ReLU, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_features, out_features, divisor):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.relu(x)\n        x = x / self.divisor\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies ReLU, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_features, out_features, divisor):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.relu(x)\n        x = x / self.divisor\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies ReLU, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_features, out_features, divisor):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.relu(x)\n        x = x / self.divisor\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "63_Gemm_ReLU_Divide", "problem_id": 63}, "task_id": "2-63"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), followed by LogSumExp, LeakyReLU, \n    LeakyReLU, GELU, and GELU activations.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Gemm\n        x = self.linear(x)\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        # LeakyReLU\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        # LeakyReLU\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        # GELU\n        x = torch.nn.functional.gelu(x)\n        # GELU\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), followed by LogSumExp, LeakyReLU, \n    LeakyReLU, GELU, and GELU activations.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Gemm\n        x = self.linear(x)\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        # LeakyReLU\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        # LeakyReLU\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        # GELU\n        x = torch.nn.functional.gelu(x)\n        # GELU\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), followed by LogSumExp, LeakyReLU, \n    LeakyReLU, GELU, and GELU activations.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Gemm\n        x = self.linear(x)\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n        # LeakyReLU\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        # LeakyReLU\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        # GELU\n        x = torch.nn.functional.gelu(x)\n        # GELU\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "64_Gemm_LogSumExp_LeakyReLU_LeakyReLU_GELU_GELU", "problem_id": 64}, "task_id": "2-64"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    This model performs a convolution, average pooling, applies sigmoid, and sums the result.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.avg_pool = nn.AvgPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.avg_pool(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=[1,2,3]) # Sum over all spatial dimensions\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 384, 384\nkernel_size = 3\npool_kernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, pool_kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    This model performs a convolution, average pooling, applies sigmoid, and sums the result.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.avg_pool = nn.AvgPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.avg_pool(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=[1,2,3]) # Sum over all spatial dimensions\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 384, 384\nkernel_size = 3\npool_kernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, pool_kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    This model performs a convolution, average pooling, applies sigmoid, and sums the result.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.avg_pool = nn.AvgPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.avg_pool(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=[1,2,3]) # Sum over all spatial dimensions\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 384, 384\nkernel_size = 3\npool_kernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, pool_kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "65_Conv2d_AvgPool_Sigmoid_Sum", "problem_id": 65}, "task_id": "2-65"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs matrix multiplication, applies dropout, and then applies softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout_p):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.dropout(x)\n        x = torch.softmax(x, dim=1)  # Softmax over features\n        return x\n\nbatch_size = 128\nin_features = 16384\nout_features = 16384\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs matrix multiplication, applies dropout, and then applies softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout_p):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.dropout(x)\n        x = torch.softmax(x, dim=1)  # Softmax over features\n        return x\n\nbatch_size = 128\nin_features = 16384\nout_features = 16384\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs matrix multiplication, applies dropout, and then applies softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout_p):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.dropout(x)\n        x = torch.softmax(x, dim=1)  # Softmax over features\n        return x\n\nbatch_size = 128\nin_features = 16384\nout_features = 16384\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "66_Matmul_Dropout_Softmax", "problem_id": 66}, "task_id": "2-66"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies GELU, and then performs global average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        x = x.squeeze(-1).squeeze(-1)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies GELU, and then performs global average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        x = x.squeeze(-1).squeeze(-1)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies GELU, and then performs global average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        x = x.squeeze(-1).squeeze(-1)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "67_Conv2d_GELU_GlobalAvgPool", "problem_id": 67}, "task_id": "2-67"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies minimum, and subtracts a constant.\n    \"\"\"\n    def __init__(self, in_features, out_features, constant):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.constant = nn.Parameter(torch.tensor(constant))\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.min(x, self.constant)\n        x = x - self.constant\n        return x\n\nbatch_size = 128\nin_features = 16384\nout_features = 16384\nconstant = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, constant]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies minimum, and subtracts a constant.\n    \"\"\"\n    def __init__(self, in_features, out_features, constant):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.constant = nn.Parameter(torch.tensor(constant))\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.min(x, self.constant)\n        x = x - self.constant\n        return x\n\nbatch_size = 128\nin_features = 16384\nout_features = 16384\nconstant = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, constant]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies minimum, and subtracts a constant.\n    \"\"\"\n    def __init__(self, in_features, out_features, constant):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.constant = nn.Parameter(torch.tensor(constant))\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.min(x, self.constant)\n        x = x - self.constant\n        return x\n\nbatch_size = 128\nin_features = 16384\nout_features = 16384\nconstant = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, constant]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "68_Matmul_Min_Subtract", "problem_id": 68}, "task_id": "2-68"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies HardSwish, and then ReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.hardswish(x)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies HardSwish, and then ReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.hardswish(x)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies HardSwish, and then ReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.hardswish(x)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "69_Conv2d_HardSwish_ReLU", "problem_id": 69}, "task_id": "2-69"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Softmax, and performs two max pooling operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.pool1 = nn.MaxPool3d(pool_kernel_size)\n        self.pool2 = nn.MaxPool3d(pool_kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, depth, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels, depth', height', width') where depth', height', width' are the dimensions after pooling.\n        \"\"\"\n        x = self.conv(x)\n        x = torch.softmax(x, dim=1)\n        x = self.pool1(x)\n        x = self.pool2(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\npool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, pool_kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Softmax, and performs two max pooling operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.pool1 = nn.MaxPool3d(pool_kernel_size)\n        self.pool2 = nn.MaxPool3d(pool_kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, depth, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels, depth', height', width') where depth', height', width' are the dimensions after pooling.\n        \"\"\"\n        x = self.conv(x)\n        x = torch.softmax(x, dim=1)\n        x = self.pool1(x)\n        x = self.pool2(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\npool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, pool_kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Softmax, and performs two max pooling operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.pool1 = nn.MaxPool3d(pool_kernel_size)\n        self.pool2 = nn.MaxPool3d(pool_kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, depth, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels, depth', height', width') where depth', height', width' are the dimensions after pooling.\n        \"\"\"\n        x = self.conv(x)\n        x = torch.softmax(x, dim=1)\n        x = self.pool1(x)\n        x = self.pool2(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\npool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, pool_kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "6_Conv3d_Softmax_MaxPool_MaxPool", "problem_id": 6}, "task_id": "2-6"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model implementing the pattern \"Gemm_Sigmoid_Scaling_ResidualAdd\".\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.gemm(x)\n        original_x = x\n        x = torch.sigmoid(x)\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model implementing the pattern \"Gemm_Sigmoid_Scaling_ResidualAdd\".\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.gemm(x)\n        original_x = x\n        x = torch.sigmoid(x)\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model implementing the pattern \"Gemm_Sigmoid_Scaling_ResidualAdd\".\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.gemm(x)\n        original_x = x\n        x = torch.sigmoid(x)\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 1024\ninput_size = 8192\nhidden_size = 8192\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "70_Gemm_Sigmoid_Scaling_ResidualAdd", "problem_id": 70}, "task_id": "2-70"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "71_Conv2d_Divide_LeakyReLU", "problem_id": 71}, "task_id": "2-71"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a 3D transposed convolution, followed by batch normalization, \n    two average pooling layers.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm3d(out_channels)\n        self.avg_pool1 = nn.AvgPool3d(kernel_size=2)\n        self.avg_pool2 = nn.AvgPool3d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = self.avg_pool1(x)\n        x = self.avg_pool2(x)\n        return x\n\n\nbatch_size = 64\nin_channels = 3\nout_channels = 16\ndepth, height, width = 32, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a 3D transposed convolution, followed by batch normalization, \n    two average pooling layers.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm3d(out_channels)\n        self.avg_pool1 = nn.AvgPool3d(kernel_size=2)\n        self.avg_pool2 = nn.AvgPool3d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = self.avg_pool1(x)\n        x = self.avg_pool2(x)\n        return x\n\n\nbatch_size = 64\nin_channels = 3\nout_channels = 16\ndepth, height, width = 32, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a 3D transposed convolution, followed by batch normalization, \n    two average pooling layers.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm3d(out_channels)\n        self.avg_pool1 = nn.AvgPool3d(kernel_size=2)\n        self.avg_pool2 = nn.AvgPool3d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.batch_norm(x)\n        x = self.avg_pool1(x)\n        x = self.avg_pool2(x)\n        return x\n\n\nbatch_size = 64\nin_channels = 3\nout_channels = 16\ndepth, height, width = 32, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "72_ConvTranspose3d_BatchNorm_AvgPool_AvgPool", "problem_id": 72}, "task_id": "2-72"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Batch Normalization, and scales the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Batch Normalization, and scales the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Batch Normalization, and scales the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "73_Conv2d_BatchNorm_Scaling", "problem_id": 73}, "task_id": "2-73"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies LeakyReLU, multiplies by a learnable parameter, \n    applies LeakyReLU again, and performs a max pooling operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.max_pool = nn.MaxPool3d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.leaky_relu(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        x = self.max_pool(x)\n        return x\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies LeakyReLU, multiplies by a learnable parameter, \n    applies LeakyReLU again, and performs a max pooling operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.max_pool = nn.MaxPool3d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.leaky_relu(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        x = self.max_pool(x)\n        return x\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, applies LeakyReLU, multiplies by a learnable parameter, \n    applies LeakyReLU again, and performs a max pooling operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.max_pool = nn.MaxPool3d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.leaky_relu(x)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        x = self.max_pool(x)\n        return x\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "74_ConvTranspose3d_LeakyReLU_Multiply_LeakyReLU_Max", "problem_id": 74}, "task_id": "2-74"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] \n        x = x + self.bias\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 512\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] \n        x = x + self.bias\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 512\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] \n        x = x + self.bias\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 512\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "75_Gemm_GroupNorm_Min_BiasAdd", "problem_id": 75}, "task_id": "2-75"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a bias term, and applies ReLU.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor with shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor with shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x + self.bias\n        x = torch.relu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a bias term, and applies ReLU.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor with shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor with shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x + self.bias\n        x = torch.relu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a bias term, and applies ReLU.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor with shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor with shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x + self.bias\n        x = torch.relu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "76_Gemm_Add_ReLU", "problem_id": 76}, "task_id": "2-76"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, scales the output, applies batch normalization, \n    and then performs global average pooling. \n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = self.global_avg_pool(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\ndepth, height, width = 16, 32, 32\nkernel_size = 5\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, scales the output, applies batch normalization, \n    and then performs global average pooling. \n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = self.global_avg_pool(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\ndepth, height, width = 16, 32, 32\nkernel_size = 5\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, scales the output, applies batch normalization, \n    and then performs global average pooling. \n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = self.global_avg_pool(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 128\ndepth, height, width = 16, 32, 32\nkernel_size = 5\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool", "problem_id": 77}, "task_id": "2-77"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by two max pooling layers and a sum operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.max_pool1 = nn.MaxPool3d(kernel_size=2)\n        self.max_pool2 = nn.MaxPool3d(kernel_size=3)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool1(x)\n        x = self.max_pool2(x)\n        x = torch.sum(x, dim=1, keepdim=True) \n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 32, 32\nkernel_size = 5\nstride = 2\npadding = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by two max pooling layers and a sum operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.max_pool1 = nn.MaxPool3d(kernel_size=2)\n        self.max_pool2 = nn.MaxPool3d(kernel_size=3)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool1(x)\n        x = self.max_pool2(x)\n        x = torch.sum(x, dim=1, keepdim=True) \n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 32, 32\nkernel_size = 5\nstride = 2\npadding = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by two max pooling layers and a sum operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.max_pool1 = nn.MaxPool3d(kernel_size=2)\n        self.max_pool2 = nn.MaxPool3d(kernel_size=3)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool1(x)\n        x = self.max_pool2(x)\n        x = torch.sum(x, dim=1, keepdim=True) \n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 32, 32, 32\nkernel_size = 5\nstride = 2\npadding = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "78_ConvTranspose3d_Max_Max_Sum", "problem_id": 78}, "task_id": "2-78"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A 3D convolutional layer followed by multiplication, instance normalization, clamping, multiplication, and a max operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.instance_norm = nn.InstanceNorm3d(out_channels)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.multiplier\n        x = self.instance_norm(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        x = x * self.multiplier\n        x = torch.max(x, dim=1)[0]\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1, 1)\nclamp_min = -1.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A 3D convolutional layer followed by multiplication, instance normalization, clamping, multiplication, and a max operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.instance_norm = nn.InstanceNorm3d(out_channels)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.multiplier\n        x = self.instance_norm(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        x = x * self.multiplier\n        x = torch.max(x, dim=1)[0]\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1, 1)\nclamp_min = -1.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A 3D convolutional layer followed by multiplication, instance normalization, clamping, multiplication, and a max operation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.instance_norm = nn.InstanceNorm3d(out_channels)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.multiplier\n        x = self.instance_norm(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        x = x * self.multiplier\n        x = torch.max(x, dim=1)[0]\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1, 1)\nclamp_min = -1.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "79_Conv3d_Multiply_InstanceNorm_Clamp_Multiply_Max", "problem_id": 79}, "task_id": "2-79"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies ReLU, LeakyReLU, GELU, Sigmoid activations, and bias in sequence.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        x = torch.nn.functional.gelu(x)\n        x = torch.sigmoid(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 64\nin_channels = 8\nout_channels = 32\ndepth, height, width = 32, 64, 64\nkernel_size = 3\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies ReLU, LeakyReLU, GELU, Sigmoid activations, and bias in sequence.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        x = torch.nn.functional.gelu(x)\n        x = torch.sigmoid(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 64\nin_channels = 8\nout_channels = 32\ndepth, height, width = 32, 64, 64\nkernel_size = 3\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies ReLU, LeakyReLU, GELU, Sigmoid activations, and bias in sequence.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        x = torch.nn.functional.gelu(x)\n        x = torch.sigmoid(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 64\nin_channels = 8\nout_channels = 32\ndepth, height, width = 32, 64, 64\nkernel_size = 3\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "7_Conv3d_ReLU_LeakyReLU_GELU_Sigmoid_BiasAdd", "problem_id": 7}, "task_id": "2-7"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, followed by a max operation, subtraction, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, max_dim):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.max_dim = max_dim\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_features)\n\n        Returns:\n            Output tensor of shape (batch_size, out_features)\n        \"\"\"\n        x = self.gemm(x)\n        x = torch.max(x, dim=self.max_dim, keepdim=True).values\n        x = x - x.mean(dim=1, keepdim=True)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nmax_dim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, max_dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, followed by a max operation, subtraction, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, max_dim):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.max_dim = max_dim\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_features)\n\n        Returns:\n            Output tensor of shape (batch_size, out_features)\n        \"\"\"\n        x = self.gemm(x)\n        x = torch.max(x, dim=self.max_dim, keepdim=True).values\n        x = x - x.mean(dim=1, keepdim=True)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nmax_dim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, max_dim]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, followed by a max operation, subtraction, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, max_dim):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.max_dim = max_dim\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_features)\n\n        Returns:\n            Output tensor of shape (batch_size, out_features)\n        \"\"\"\n        x = self.gemm(x)\n        x = torch.max(x, dim=self.max_dim, keepdim=True).values\n        x = x - x.mean(dim=1, keepdim=True)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nmax_dim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, max_dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "80_Gemm_Max_Subtract_GELU", "problem_id": 80}, "task_id": "2-80"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a gemm, swish, divide, clamp, tanh, and clamp operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x / 2.0\n        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1\n        x = torch.tanh(x)  # Tanh activation\n        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a gemm, swish, divide, clamp, tanh, and clamp operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x / 2.0\n        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1\n        x = torch.tanh(x)  # Tanh activation\n        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a gemm, swish, divide, clamp, tanh, and clamp operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x / 2.0\n        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1\n        x = torch.tanh(x)  # Tanh activation\n        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "81_Gemm_Swish_Divide_Clamp_Tanh_Clamp", "problem_id": 81}, "task_id": "2-81"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scaling_factor = scaling_factor\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.max_pool = nn.MaxPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        # Convolution\n        x = self.conv(x)\n        # Tanh activation\n        x = torch.tanh(x)\n        # Scaling\n        x = x * self.scaling_factor\n        # Bias addition\n        x = x + self.bias\n        # Max-pooling\n        x = self.max_pool(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nscaling_factor = 2.0\nbias_shape = (out_channels, 1, 1)\npool_kernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scaling_factor = scaling_factor\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.max_pool = nn.MaxPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        # Convolution\n        x = self.conv(x)\n        # Tanh activation\n        x = torch.tanh(x)\n        # Scaling\n        x = x * self.scaling_factor\n        # Bias addition\n        x = x + self.bias\n        # Max-pooling\n        x = self.max_pool(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nscaling_factor = 2.0\nbias_shape = (out_channels, 1, 1)\npool_kernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scaling_factor = scaling_factor\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.max_pool = nn.MaxPool2d(pool_kernel_size)\n\n    def forward(self, x):\n        # Convolution\n        x = self.conv(x)\n        # Tanh activation\n        x = torch.tanh(x)\n        # Scaling\n        x = x * self.scaling_factor\n        # Bias addition\n        x = x + self.bias\n        # Max-pooling\n        x = self.max_pool(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nscaling_factor = 2.0\nbias_shape = (out_channels, 1, 1)\npool_kernel_size = 4\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "82_Conv2d_Tanh_Scaling_BiasAdd_Max", "problem_id": 82}, "task_id": "2-82"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.norm = nn.GroupNorm(groups, out_channels)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = torch.min(x, torch.tensor(min_value, device=x.device))\n        x = torch.clamp(x, min=min_value, max=max_value)\n        x = self.dropout(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 64, 64\nkernel_size = 3\ngroups = 8\nmin_value = 0.0\nmax_value = 1.0\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.norm = nn.GroupNorm(groups, out_channels)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = torch.min(x, torch.tensor(min_value, device=x.device))\n        x = torch.clamp(x, min=min_value, max=max_value)\n        x = self.dropout(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 64, 64\nkernel_size = 3\ngroups = 8\nmin_value = 0.0\nmax_value = 1.0\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.norm = nn.GroupNorm(groups, out_channels)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = torch.min(x, torch.tensor(min_value, device=x.device))\n        x = torch.clamp(x, min=min_value, max=max_value)\n        x = self.dropout(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 64, 64\nkernel_size = 3\ngroups = 8\nmin_value = 0.0\nmax_value = 1.0\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "83_Conv3d_GroupNorm_Min_Clamp_Dropout", "problem_id": 83}, "task_id": "2-83"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "84_Gemm_BatchNorm_Scaling_Softmax", "problem_id": 84}, "task_id": "2-84"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128 \nkernel_size = 3\nnum_groups = 16\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 4\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128 \nkernel_size = 3\nnum_groups = 16\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 4\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128 \nkernel_size = 3\nnum_groups = 16\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 4\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "85_Conv2d_GroupNorm_Scale_MaxPool_Clamp", "problem_id": 85}, "task_id": "2-85"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, divides by a scalar, and applies GELU activation.\n    \"\"\"\n    def __init__(self, input_size, output_size, divisor):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, output_size).\n        \"\"\"\n        x = self.linear(x)\n        x = x / self.divisor\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\ninput_size = 8192\noutput_size = 8192\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, divides by a scalar, and applies GELU activation.\n    \"\"\"\n    def __init__(self, input_size, output_size, divisor):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, output_size).\n        \"\"\"\n        x = self.linear(x)\n        x = x / self.divisor\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\ninput_size = 8192\noutput_size = 8192\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, divides by a scalar, and applies GELU activation.\n    \"\"\"\n    def __init__(self, input_size, output_size, divisor):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, output_size).\n        \"\"\"\n        x = self.linear(x)\n        x = x / self.divisor\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 1024\ninput_size = 8192\noutput_size = 8192\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "86_Matmul_Divide_GELU", "problem_id": 86}, "task_id": "2-86"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtracts two values, applies Mish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract_value_1 = subtract_value_1\n        self.subtract_value_2 = subtract_value_2\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract_value_1\n        x = x - self.subtract_value_2\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nsubtract_value_1 = 0.5\nsubtract_value_2 = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtracts two values, applies Mish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract_value_1 = subtract_value_1\n        self.subtract_value_2 = subtract_value_2\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract_value_1\n        x = x - self.subtract_value_2\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nsubtract_value_1 = 0.5\nsubtract_value_2 = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, subtracts two values, applies Mish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.subtract_value_1 = subtract_value_1\n        self.subtract_value_2 = subtract_value_2\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x - self.subtract_value_1\n        x = x - self.subtract_value_2\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 256, 256\nkernel_size = 3\nsubtract_value_1 = 0.5\nsubtract_value_2 = 0.2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "87_Conv2d_Subtract_Subtract_Mish", "problem_id": 87}, "task_id": "2-87"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 256\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 256\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nnum_groups = 256\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "88_Gemm_GroupNorm_Swish_Multiply_Swish", "problem_id": 88}, "task_id": "2-88"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a sequence of operations:\n        - ConvTranspose3d\n        - MaxPool3d\n        - Softmax\n        - Subtract\n        - Swish\n        - Max\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding)\n        self.subtract = nn.Parameter(torch.randn(out_channels)) # Assuming subtraction is element-wise across channels\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool(x)\n        x = torch.softmax(x, dim=1) # Apply softmax across channels (dim=1)\n        x = x - self.subtract.view(1, -1, 1, 1, 1) # Subtract across channels\n        x = torch.sigmoid(x) * x # Swish activation\n        x = torch.max(x, dim=1)[0] # Max pooling across channels\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\npool_stride = 2\npool_padding = 0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a sequence of operations:\n        - ConvTranspose3d\n        - MaxPool3d\n        - Softmax\n        - Subtract\n        - Swish\n        - Max\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding)\n        self.subtract = nn.Parameter(torch.randn(out_channels)) # Assuming subtraction is element-wise across channels\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool(x)\n        x = torch.softmax(x, dim=1) # Apply softmax across channels (dim=1)\n        x = x - self.subtract.view(1, -1, 1, 1, 1) # Subtract across channels\n        x = torch.sigmoid(x) * x # Swish activation\n        x = torch.max(x, dim=1)[0] # Max pooling across channels\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\npool_stride = 2\npool_padding = 0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a sequence of operations:\n        - ConvTranspose3d\n        - MaxPool3d\n        - Softmax\n        - Subtract\n        - Swish\n        - Max\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding)\n        self.subtract = nn.Parameter(torch.randn(out_channels)) # Assuming subtraction is element-wise across channels\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool(x)\n        x = torch.softmax(x, dim=1) # Apply softmax across channels (dim=1)\n        x = x - self.subtract.view(1, -1, 1, 1, 1) # Subtract across channels\n        x = torch.sigmoid(x) * x # Swish activation\n        x = torch.max(x, dim=1)[0] # Max pooling across channels\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\npool_stride = 2\npool_padding = 0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "89_ConvTranspose3d_MaxPool_Softmax_Subtract_Swish_Max", "problem_id": 89}, "task_id": "2-89"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, divides by a constant, applies max pooling,\n    global average pooling, adds a bias term, and sums along a specific dimension.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n        self.max_pool = nn.MaxPool3d(pool_size)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.sum_dim = sum_dim\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = self.max_pool(x)\n        x = self.global_avg_pool(x)\n        x = x + self.bias\n        x = torch.sum(x, dim=self.sum_dim)\n        return x\n\nbatch_size   = 128  \nin_channels  = 8            \nout_channels = 16  \ndepth = 16; height = width = 64 \nkernel_size = (3, 3, 3)\ndivisor = 2.0\npool_size = (2, 2, 2)\nbias_shape = (out_channels, 1, 1, 1)\nsum_dim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, divides by a constant, applies max pooling,\n    global average pooling, adds a bias term, and sums along a specific dimension.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n        self.max_pool = nn.MaxPool3d(pool_size)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.sum_dim = sum_dim\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = self.max_pool(x)\n        x = self.global_avg_pool(x)\n        x = x + self.bias\n        x = torch.sum(x, dim=self.sum_dim)\n        return x\n\nbatch_size   = 128  \nin_channels  = 8            \nout_channels = 16  \ndepth = 16; height = width = 64 \nkernel_size = (3, 3, 3)\ndivisor = 2.0\npool_size = (2, 2, 2)\nbias_shape = (out_channels, 1, 1, 1)\nsum_dim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, divides by a constant, applies max pooling,\n    global average pooling, adds a bias term, and sums along a specific dimension.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n        self.max_pool = nn.MaxPool3d(pool_size)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.sum_dim = sum_dim\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = self.max_pool(x)\n        x = self.global_avg_pool(x)\n        x = x + self.bias\n        x = torch.sum(x, dim=self.sum_dim)\n        return x\n\nbatch_size   = 128  \nin_channels  = 8            \nout_channels = 16  \ndepth = 16; height = width = 64 \nkernel_size = (3, 3, 3)\ndivisor = 2.0\npool_size = (2, 2, 2)\nbias_shape = (out_channels, 1, 1, 1)\nsum_dim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "8_Conv3d_Divide_Max_GlobalAvgPool_BiasAdd_Sum", "problem_id": 8}, "task_id": "2-8"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies LeakyReLU, sums with a tensor, clamps, and applies GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = x + self.sum_tensor\n        x = torch.clamp(x, min=-1.0, max=1.0)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\ndepth, height, width = 16, 64, 64\nkernel_size = 3\nsum_tensor_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, sum_tensor_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies LeakyReLU, sums with a tensor, clamps, and applies GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = x + self.sum_tensor\n        x = torch.clamp(x, min=-1.0, max=1.0)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\ndepth, height, width = 16, 64, 64\nkernel_size = 3\nsum_tensor_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, sum_tensor_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies LeakyReLU, sums with a tensor, clamps, and applies GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = x + self.sum_tensor\n        x = torch.clamp(x, min=-1.0, max=1.0)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\ndepth, height, width = 16, 64, 64\nkernel_size = 3\nsum_tensor_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, sum_tensor_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "90_Conv3d_LeakyReLU_Sum_Clamp_GELU", "problem_id": 90}, "task_id": "2-90"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies softmax, adds a bias term, scales the result, and applies sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.softmax(x, dim=1)\n        x = x + self.bias\n        x = x * self.scaling_factor\n        x = torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 64, 64\nkernel_size = 4\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies softmax, adds a bias term, scales the result, and applies sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.softmax(x, dim=1)\n        x = x + self.bias\n        x = x * self.scaling_factor\n        x = torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 64, 64\nkernel_size = 4\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, applies softmax, adds a bias term, scales the result, and applies sigmoid.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = torch.softmax(x, dim=1)\n        x = x + self.bias\n        x = x * self.scaling_factor\n        x = torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 64, 64\nkernel_size = 4\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "91_ConvTranspose2d_Softmax_BiasAdd_Scaling_Sigmoid", "problem_id": 91}, "task_id": "2-91"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies Group Normalization, Tanh, HardSwish, \n    Residual Addition, and LogSumExp.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, eps=1e-5):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(groups, out_channels, eps=eps)\n        self.tanh = nn.Tanh()\n        self.hard_swish = nn.Hardswish()\n\n    def forward(self, x):\n        # Convolution\n        x_conv = self.conv(x)\n        # Group Normalization\n        x_norm = self.group_norm(x_conv)\n        # Tanh\n        x_tanh = self.tanh(x_norm)\n        # HardSwish\n        x_hard_swish = self.hard_swish(x_tanh)\n        # Residual Addition\n        x_res = x_conv + x_hard_swish\n        # LogSumExp\n        x_logsumexp = torch.logsumexp(x_res, dim=1, keepdim=True)\n        return x_logsumexp\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\ngroups = 16\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies Group Normalization, Tanh, HardSwish, \n    Residual Addition, and LogSumExp.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, eps=1e-5):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(groups, out_channels, eps=eps)\n        self.tanh = nn.Tanh()\n        self.hard_swish = nn.Hardswish()\n\n    def forward(self, x):\n        # Convolution\n        x_conv = self.conv(x)\n        # Group Normalization\n        x_norm = self.group_norm(x_conv)\n        # Tanh\n        x_tanh = self.tanh(x_norm)\n        # HardSwish\n        x_hard_swish = self.hard_swish(x_tanh)\n        # Residual Addition\n        x_res = x_conv + x_hard_swish\n        # LogSumExp\n        x_logsumexp = torch.logsumexp(x_res, dim=1, keepdim=True)\n        return x_logsumexp\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\ngroups = 16\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a convolution, applies Group Normalization, Tanh, HardSwish, \n    Residual Addition, and LogSumExp.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, groups, eps=1e-5):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(groups, out_channels, eps=eps)\n        self.tanh = nn.Tanh()\n        self.hard_swish = nn.Hardswish()\n\n    def forward(self, x):\n        # Convolution\n        x_conv = self.conv(x)\n        # Group Normalization\n        x_norm = self.group_norm(x_conv)\n        # Tanh\n        x_tanh = self.tanh(x_norm)\n        # HardSwish\n        x_hard_swish = self.hard_swish(x_tanh)\n        # Residual Addition\n        x_res = x_conv + x_hard_swish\n        # LogSumExp\n        x_logsumexp = torch.logsumexp(x_res, dim=1, keepdim=True)\n        return x_logsumexp\n\nbatch_size = 128\nin_channels = 8\nout_channels = 64\nheight, width = 128, 128\nkernel_size = 3\ngroups = 16\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "92_Conv2d_GroupNorm_Tanh_HardSwish_ResidualAdd_LogSumExp", "problem_id": 92}, "task_id": "2-92"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, adds a value, takes the minimum, applies GELU, and multiplies by a value.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, add_value, multiply_value):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)\n        self.add_value = add_value\n        self.multiply_value = multiply_value\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.add_value\n        x = torch.min(x, torch.tensor(0.0, device=x.device))\n        x = torch.nn.functional.gelu(x)\n        x = x * self.multiply_value\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 64, 64\nkernel_size = 4\nstride = 2\nadd_value = 0.5\nmultiply_value = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, add_value, multiply_value]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, adds a value, takes the minimum, applies GELU, and multiplies by a value.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, add_value, multiply_value):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)\n        self.add_value = add_value\n        self.multiply_value = multiply_value\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.add_value\n        x = torch.min(x, torch.tensor(0.0, device=x.device))\n        x = torch.nn.functional.gelu(x)\n        x = x * self.multiply_value\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 64, 64\nkernel_size = 4\nstride = 2\nadd_value = 0.5\nmultiply_value = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, add_value, multiply_value]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed convolution, adds a value, takes the minimum, applies GELU, and multiplies by a value.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, add_value, multiply_value):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)\n        self.add_value = add_value\n        self.multiply_value = multiply_value\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x + self.add_value\n        x = torch.min(x, torch.tensor(0.0, device=x.device))\n        x = torch.nn.functional.gelu(x)\n        x = x * self.multiply_value\n        return x\n\nbatch_size = 128\nin_channels = 64\nout_channels = 128\nheight, width = 64, 64\nkernel_size = 4\nstride = 2\nadd_value = 0.5\nmultiply_value = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, add_value, multiply_value]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "93_ConvTranspose2d_Add_Min_GELU_Multiply", "problem_id": 93}, "task_id": "2-93"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a GEMM, BiasAdd, Hardtanh, Mish, and GroupNorm operations in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias_shape, num_groups):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.hardtanh = nn.Hardtanh()\n        self.mish = nn.Mish()\n        self.groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x + self.bias\n        x = self.hardtanh(x)\n        x = self.mish(x)\n        x = self.groupnorm(x)\n        return x\n\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbias_shape = (out_features,)\nnum_groups = 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape, num_groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a GEMM, BiasAdd, Hardtanh, Mish, and GroupNorm operations in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias_shape, num_groups):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.hardtanh = nn.Hardtanh()\n        self.mish = nn.Mish()\n        self.groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x + self.bias\n        x = self.hardtanh(x)\n        x = self.mish(x)\n        x = self.groupnorm(x)\n        return x\n\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbias_shape = (out_features,)\nnum_groups = 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape, num_groups]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a GEMM, BiasAdd, Hardtanh, Mish, and GroupNorm operations in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias_shape, num_groups):\n        super(Model, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.hardtanh = nn.Hardtanh()\n        self.mish = nn.Mish()\n        self.groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = x + self.bias\n        x = self.hardtanh(x)\n        x = self.mish(x)\n        x = self.groupnorm(x)\n        return x\n\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbias_shape = (out_features,)\nnum_groups = 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape, num_groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "94_Gemm_BiasAdd_Hardtanh_Mish_GroupNorm", "problem_id": 94}, "task_id": "2-94"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "95_Matmul_Add_Swish_Tanh_GELU_Hardtanh", "problem_id": 95}, "task_id": "2-95"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed 3D convolution, multiplies by a scalar, applies max pooling, \n    global average pooling, and clamps the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.scale = scale\n        self.maxpool = nn.MaxPool3d(kernel_size=maxpool_kernel_size)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.clamp_min = 0\n        self.clamp_max = 1\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = self.global_avg_pool(x)\n        x = torch.clamp(x, min=self.clamp_min, max=self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale = 0.5\nmaxpool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed 3D convolution, multiplies by a scalar, applies max pooling, \n    global average pooling, and clamps the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.scale = scale\n        self.maxpool = nn.MaxPool3d(kernel_size=maxpool_kernel_size)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.clamp_min = 0\n        self.clamp_max = 1\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = self.global_avg_pool(x)\n        x = torch.clamp(x, min=self.clamp_min, max=self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale = 0.5\nmaxpool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a transposed 3D convolution, multiplies by a scalar, applies max pooling, \n    global average pooling, and clamps the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n        self.scale = scale\n        self.maxpool = nn.MaxPool3d(kernel_size=maxpool_kernel_size)\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.clamp_min = 0\n        self.clamp_max = 1\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = self.global_avg_pool(x)\n        x = torch.clamp(x, min=self.clamp_min, max=self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale = 0.5\nmaxpool_kernel_size = 2\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "96_ConvTranspose3d_Multiply_Max_GlobalAvgPool_Clamp", "problem_id": 96}, "task_id": "2-96"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "97_Matmul_BatchNorm_BiasAdd_Divide_Swish", "problem_id": 97}, "task_id": "2-97"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\npool_kernel_size = 16\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\npool_kernel_size = 16\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(Model, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\npool_kernel_size = 16\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "98_Matmul_AvgPool_GELU_Scale_Max", "problem_id": 98}, "task_id": "2-98"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies GELU, and then applies Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.softmax(x, dim=1)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies GELU, and then applies Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.softmax(x, dim=1)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies GELU, and then applies Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.softmax(x, dim=1)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "99_Matmul_GELU_Softmax", "problem_id": 99}, "task_id": "2-99"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, subtraction, multiplication, and ReLU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, subtract_value, multiply_value):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.subtract_value = subtract_value\n        self.multiply_value = multiply_value\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = x - self.subtract_value\n        x = x * self.multiply_value\n        x = torch.relu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nsubtract_value = 2.0\nmultiply_value = 1.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, subtract_value, multiply_value]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_2", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, subtraction, multiplication, and ReLU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, subtract_value, multiply_value):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.subtract_value = subtract_value\n        self.multiply_value = multiply_value\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = x - self.subtract_value\n        x = x * self.multiply_value\n        x = torch.relu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nsubtract_value = 2.0\nmultiply_value = 1.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, subtract_value, multiply_value]", "level": "2", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, subtraction, multiplication, and ReLU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, subtract_value, multiply_value):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.subtract_value = subtract_value\n        self.multiply_value = multiply_value\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = x - self.subtract_value\n        x = x * self.multiply_value\n        x = torch.relu(x)\n        return x\n\nbatch_size = 1024\nin_features = 8192\nout_features = 8192\nsubtract_value = 2.0\nmultiply_value = 1.5\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, subtract_value, multiply_value]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "9_Matmul_Subtract_Multiply_ReLU", "problem_id": 9}, "task_id": "2-9"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels * expansion, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, layers, num_classes=1000):\n        \"\"\"\n        :param block: Type of block to use (BasicBlock or Bottleneck)\n        :param layers: List of integers specifying the number of blocks in each layer\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        block = Bottleneck\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 10\nheight = 224\nwidth = 224\nlayers = [3, 4, 23, 3]\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [layers, num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels * expansion, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, layers, num_classes=1000):\n        \"\"\"\n        :param block: Type of block to use (BasicBlock or Bottleneck)\n        :param layers: List of integers specifying the number of blocks in each layer\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        block = Bottleneck\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 10\nheight = 224\nwidth = 224\nlayers = [3, 4, 23, 3]\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [layers, num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels * expansion, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, layers, num_classes=1000):\n        \"\"\"\n        :param block: Type of block to use (BasicBlock or Bottleneck)\n        :param layers: List of integers specifying the number of blocks in each layer\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        block = Bottleneck\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 10\nheight = 224\nwidth = 224\nlayers = [3, 4, 23, 3]\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [layers, num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "10_ResNet101", "problem_id": 10}, "task_id": "3-10"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        Initialize the VGG16 model.\n        \n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # VGG16 architecture: 5 blocks of convolutional layers followed by max pooling\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Fully connected layers\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the VGG16 model.\n        \n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        Initialize the VGG16 model.\n        \n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # VGG16 architecture: 5 blocks of convolutional layers followed by max pooling\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Fully connected layers\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the VGG16 model.\n        \n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        Initialize the VGG16 model.\n        \n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # VGG16 architecture: 5 blocks of convolutional layers followed by max pooling\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Fully connected layers\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the VGG16 model.\n        \n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "11_VGG16", "problem_id": 11}, "task_id": "3-11"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        Initialize the VGG19 model.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # VGG19 architecture: 16 Conv layers + 5 MaxPool layers + 3 Fully Connected layers\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the VGG19 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        Initialize the VGG19 model.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # VGG19 architecture: 16 Conv layers + 5 MaxPool layers + 3 Fully Connected layers\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the VGG19 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        Initialize the VGG19 model.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # VGG19 architecture: 16 Conv layers + 5 MaxPool layers + 3 Fully Connected layers\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the VGG19 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "12_VGG19", "problem_id": 12}, "task_id": "3-12"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(Model, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nbatch_size = 128\nnum_input_features = 32\nnum_output_features = 64\nheight, width = 256, 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, num_output_features]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(Model, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nbatch_size = 128\nnum_input_features = 32\nnum_output_features = 64\nheight, width = 256, 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, num_output_features]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(Model, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nbatch_size = 128\nnum_input_features = 32\nnum_output_features = 64\nheight, width = 256, 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, num_output_features]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "13_DenseNet121TransitionLayer", "problem_id": 13}, "task_id": "3-13"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(Model, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n    \nbatch_size = 10\nnum_layers = 6\nnum_input_features = 32\ngrowth_rate = 32\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_layers, num_input_features , growth_rate]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(Model, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n    \nbatch_size = 10\nnum_layers = 6\nnum_input_features = 32\ngrowth_rate = 32\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_layers, num_input_features , growth_rate]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(Model, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n    \nbatch_size = 10\nnum_layers = 6\nnum_input_features = 32\ngrowth_rate = 32\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_layers, num_input_features , growth_rate]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "14_DenseNet121DenseBlock", "problem_id": 14}, "task_id": "3-14"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass Model(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 24, 16]  # Corresponding layers in DenseNet121\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet121 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass Model(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 24, 16]  # Corresponding layers in DenseNet121\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet121 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass Model(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 24, 16]  # Corresponding layers in DenseNet121\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet121 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "15_DenseNet121", "problem_id": 15}, "task_id": "3-15"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass Model(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass Model(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass Model(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "16_DenseNet201", "problem_id": 16}, "task_id": "3-16"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n# Test code\nbatch_size = 128\nnum_input_features = 3\nnum_output_features = 64\nheight, width = 256, 256\nsqueeze_channels = 6\nexpand1x1_channels = 64\nexpand3x3_channels = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, squeeze_channels, expand1x1_channels, expand3x3_channels]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n# Test code\nbatch_size = 128\nnum_input_features = 3\nnum_output_features = 64\nheight, width = 256, 256\nsqueeze_channels = 6\nexpand1x1_channels = 64\nexpand3x3_channels = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, squeeze_channels, expand1x1_channels, expand3x3_channels]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n# Test code\nbatch_size = 128\nnum_input_features = 3\nnum_output_features = 64\nheight, width = 256, 256\nsqueeze_channels = 6\nexpand1x1_channels = 64\nexpand3x3_channels = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, squeeze_channels, expand1x1_channels, expand3x3_channels]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "17_SqueezeNetFireModule", "problem_id": 17}, "task_id": "3-17"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 64\ninput_channels = 3\nheight = 512\nwidth = 512\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 64\ninput_channels = 3\nheight = 512\nwidth = 512\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 64\ninput_channels = 3\nheight = 512\nwidth = 512\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "18_SqueezeNet", "problem_id": 18}, "task_id": "3-18"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "19_MobileNetV1", "problem_id": 19}, "task_id": "3-19"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for layer_size in layer_sizes:\n            layers.append(nn.Linear(current_input_size, layer_size))\n            layers.append(nn.ReLU())\n            current_input_size = layer_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 128\ninput_size = 16384\nlayer_sizes = [16384, 16384]\noutput_size = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, layer_sizes, output_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for layer_size in layer_sizes:\n            layers.append(nn.Linear(current_input_size, layer_size))\n            layers.append(nn.ReLU())\n            current_input_size = layer_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 128\ninput_size = 16384\nlayer_sizes = [16384, 16384]\noutput_size = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, layer_sizes, output_size]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for layer_size in layer_sizes:\n            layers.append(nn.Linear(current_input_size, layer_size))\n            layers.append(nn.ReLU())\n            current_input_size = layer_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 128\ninput_size = 16384\nlayer_sizes = [16384, 16384]\noutput_size = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, layer_sizes, output_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "1_MLP", "problem_id": 1}, "task_id": "3-1"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Final layer\n        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Final layer\n        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Final layer\n        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "20_MobileNetV2", "problem_id": 20}, "task_id": "3-20"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        \"\"\"\n        MBConv block implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param kernel_size: Kernel size for the depthwise convolution.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the intermediate channels.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MBConv block.\n\n        :param x: The input tensor, shape (batch_size, in_channels, H, W)\n        :return: The output tensor, shape (batch_size, out_channels, H', W')\n        \"\"\"\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\n# Test code\nbatch_size = 10\nin_channels = 112\nout_channels = 192\nkernel_size = 5\nstride = 2\nexpand_ratio = 6\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, expand_ratio]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        \"\"\"\n        MBConv block implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param kernel_size: Kernel size for the depthwise convolution.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the intermediate channels.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MBConv block.\n\n        :param x: The input tensor, shape (batch_size, in_channels, H, W)\n        :return: The output tensor, shape (batch_size, out_channels, H', W')\n        \"\"\"\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\n# Test code\nbatch_size = 10\nin_channels = 112\nout_channels = 192\nkernel_size = 5\nstride = 2\nexpand_ratio = 6\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, expand_ratio]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        \"\"\"\n        MBConv block implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param kernel_size: Kernel size for the depthwise convolution.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the intermediate channels.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MBConv block.\n\n        :param x: The input tensor, shape (batch_size, in_channels, H, W)\n        :return: The output tensor, shape (batch_size, out_channels, H', W')\n        \"\"\"\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\n# Test code\nbatch_size = 10\nin_channels = 112\nout_channels = 192\nkernel_size = 5\nstride = 2\nexpand_ratio = 6\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, expand_ratio]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "21_EfficientNetMBConv", "problem_id": 21}, "task_id": "3-21"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB0 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.blocks = nn.Sequential(\n            # MBConv1 (32, 16, 1, 1)\n            MBConv(32, 16, kernel_size=3, stride=1, expand_ratio=1),\n            # MBConv6 (16, 24, 2, 6)\n            MBConv(16, 24, kernel_size=3, stride=2, expand_ratio=6),\n            # MBConv6 (24, 24, 1, 6)\n            MBConv(24, 24, kernel_size=3, stride=1, expand_ratio=6),\n            # MBConv6 (24, 40, 2, 6)\n            MBConv(24, 40, kernel_size=5, stride=2, expand_ratio=6),\n            # MBConv6 (40, 40, 1, 6)\n            MBConv(40, 40, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (40, 80, 2, 6)\n            MBConv(40, 80, kernel_size=3, stride=2, expand_ratio=6),\n            # MBConv6 (80, 80, 1, 6)\n            MBConv(80, 80, kernel_size=3, stride=1, expand_ratio=6),\n            # MBConv6 (80, 112, 1, 6)\n            MBConv(80, 112, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (112, 112, 1, 6)\n            MBConv(112, 112, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (112, 192, 2, 6)\n            MBConv(112, 192, kernel_size=5, stride=2, expand_ratio=6),\n            # MBConv6 (192, 192, 1, 6)\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (192, 192, 1, 6)\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (192, 320, 1, 6)\n            MBConv(192, 320, kernel_size=3, stride=1, expand_ratio=6)\n        )\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB0 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.blocks(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nclass MBConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        \"\"\"\n        MBConv block implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param kernel_size: Kernel size for the depthwise convolution.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the intermediate channels.\n        \"\"\"\n        super(MBConv, self).__init__()\n        \n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MBConv block.\n\n        :param x: The input tensor, shape (batch_size, in_channels, H, W)\n        :return: The output tensor, shape (batch_size, out_channels, H', W')\n        \"\"\"\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB0 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.blocks = nn.Sequential(\n            # MBConv1 (32, 16, 1, 1)\n            MBConv(32, 16, kernel_size=3, stride=1, expand_ratio=1),\n            # MBConv6 (16, 24, 2, 6)\n            MBConv(16, 24, kernel_size=3, stride=2, expand_ratio=6),\n            # MBConv6 (24, 24, 1, 6)\n            MBConv(24, 24, kernel_size=3, stride=1, expand_ratio=6),\n            # MBConv6 (24, 40, 2, 6)\n            MBConv(24, 40, kernel_size=5, stride=2, expand_ratio=6),\n            # MBConv6 (40, 40, 1, 6)\n            MBConv(40, 40, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (40, 80, 2, 6)\n            MBConv(40, 80, kernel_size=3, stride=2, expand_ratio=6),\n            # MBConv6 (80, 80, 1, 6)\n            MBConv(80, 80, kernel_size=3, stride=1, expand_ratio=6),\n            # MBConv6 (80, 112, 1, 6)\n            MBConv(80, 112, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (112, 112, 1, 6)\n            MBConv(112, 112, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (112, 192, 2, 6)\n            MBConv(112, 192, kernel_size=5, stride=2, expand_ratio=6),\n            # MBConv6 (192, 192, 1, 6)\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (192, 192, 1, 6)\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (192, 320, 1, 6)\n            MBConv(192, 320, kernel_size=3, stride=1, expand_ratio=6)\n        )\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB0 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.blocks(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nclass MBConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        \"\"\"\n        MBConv block implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param kernel_size: Kernel size for the depthwise convolution.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the intermediate channels.\n        \"\"\"\n        super(MBConv, self).__init__()\n        \n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MBConv block.\n\n        :param x: The input tensor, shape (batch_size, in_channels, H, W)\n        :return: The output tensor, shape (batch_size, out_channels, H', W')\n        \"\"\"\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB0 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.blocks = nn.Sequential(\n            # MBConv1 (32, 16, 1, 1)\n            MBConv(32, 16, kernel_size=3, stride=1, expand_ratio=1),\n            # MBConv6 (16, 24, 2, 6)\n            MBConv(16, 24, kernel_size=3, stride=2, expand_ratio=6),\n            # MBConv6 (24, 24, 1, 6)\n            MBConv(24, 24, kernel_size=3, stride=1, expand_ratio=6),\n            # MBConv6 (24, 40, 2, 6)\n            MBConv(24, 40, kernel_size=5, stride=2, expand_ratio=6),\n            # MBConv6 (40, 40, 1, 6)\n            MBConv(40, 40, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (40, 80, 2, 6)\n            MBConv(40, 80, kernel_size=3, stride=2, expand_ratio=6),\n            # MBConv6 (80, 80, 1, 6)\n            MBConv(80, 80, kernel_size=3, stride=1, expand_ratio=6),\n            # MBConv6 (80, 112, 1, 6)\n            MBConv(80, 112, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (112, 112, 1, 6)\n            MBConv(112, 112, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (112, 192, 2, 6)\n            MBConv(112, 192, kernel_size=5, stride=2, expand_ratio=6),\n            # MBConv6 (192, 192, 1, 6)\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (192, 192, 1, 6)\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            # MBConv6 (192, 320, 1, 6)\n            MBConv(192, 320, kernel_size=3, stride=1, expand_ratio=6)\n        )\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB0 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.blocks(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nclass MBConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        \"\"\"\n        MBConv block implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param kernel_size: Kernel size for the depthwise convolution.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the intermediate channels.\n        \"\"\"\n        super(MBConv, self).__init__()\n        \n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MBConv block.\n\n        :param x: The input tensor, shape (batch_size, in_channels, H, W)\n        :return: The output tensor, shape (batch_size, out_channels, H', W')\n        \"\"\"\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "22_EfficientNetB0", "problem_id": 22}, "task_id": "3-22"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB1 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Creates a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride of the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the hidden layer.\n        :return: A sequential MBConv block.\n        \"\"\"\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB1 model.\n\n        :param x: Input tensor, shape (batch_size, 3, 240, 240)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB1 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Creates a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride of the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the hidden layer.\n        :return: A sequential MBConv block.\n        \"\"\"\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB1 model.\n\n        :param x: Input tensor, shape (batch_size, 3, 240, 240)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB1 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Creates a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride of the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the hidden layer.\n        :return: A sequential MBConv block.\n        \"\"\"\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB1 model.\n\n        :param x: Input tensor, shape (batch_size, 3, 240, 240)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "23_EfficientNetB1", "problem_id": 23}, "task_id": "3-23"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB2 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Define the EfficientNetB2 architecture components\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Define the MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 96, 1, 3)\n        self.mbconv2 = self._make_mbconv_block(96, 144, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(144, 192, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(192, 288, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(288, 384, 1, 6)\n        \n        # Final layers\n        self.conv_final = nn.Conv2d(384, 1408, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_final = nn.BatchNorm2d(1408)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(1408, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Helper function to create a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the MBConv block.\n        :return: A sequential container of layers forming the MBConv block.\n        \"\"\"\n        layers = []\n        expanded_channels = in_channels * expand_ratio\n        \n        # Expansion phase\n        if expand_ratio != 1:\n            layers.append(nn.Conv2d(in_channels, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))\n            layers.append(nn.BatchNorm2d(expanded_channels))\n            layers.append(nn.ReLU(inplace=True))\n        \n        # Depthwise convolution\n        layers.append(nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3, stride=stride, padding=1, groups=expanded_channels, bias=False))\n        layers.append(nn.BatchNorm2d(expanded_channels))\n        layers.append(nn.ReLU(inplace=True))\n        \n        # Squeeze and Excitation\n        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n        layers.append(nn.Conv2d(expanded_channels, expanded_channels // 4, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(expanded_channels // 4, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.Sigmoid())\n        \n        # Output phase\n        layers.append(nn.Conv2d(expanded_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.relu(self.bn_final(self.conv_final(x)))\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB2 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Define the EfficientNetB2 architecture components\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Define the MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 96, 1, 3)\n        self.mbconv2 = self._make_mbconv_block(96, 144, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(144, 192, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(192, 288, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(288, 384, 1, 6)\n        \n        # Final layers\n        self.conv_final = nn.Conv2d(384, 1408, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_final = nn.BatchNorm2d(1408)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(1408, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Helper function to create a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the MBConv block.\n        :return: A sequential container of layers forming the MBConv block.\n        \"\"\"\n        layers = []\n        expanded_channels = in_channels * expand_ratio\n        \n        # Expansion phase\n        if expand_ratio != 1:\n            layers.append(nn.Conv2d(in_channels, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))\n            layers.append(nn.BatchNorm2d(expanded_channels))\n            layers.append(nn.ReLU(inplace=True))\n        \n        # Depthwise convolution\n        layers.append(nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3, stride=stride, padding=1, groups=expanded_channels, bias=False))\n        layers.append(nn.BatchNorm2d(expanded_channels))\n        layers.append(nn.ReLU(inplace=True))\n        \n        # Squeeze and Excitation\n        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n        layers.append(nn.Conv2d(expanded_channels, expanded_channels // 4, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(expanded_channels // 4, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.Sigmoid())\n        \n        # Output phase\n        layers.append(nn.Conv2d(expanded_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.relu(self.bn_final(self.conv_final(x)))\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB2 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Define the EfficientNetB2 architecture components\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Define the MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 96, 1, 3)\n        self.mbconv2 = self._make_mbconv_block(96, 144, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(144, 192, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(192, 288, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(288, 384, 1, 6)\n        \n        # Final layers\n        self.conv_final = nn.Conv2d(384, 1408, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_final = nn.BatchNorm2d(1408)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(1408, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Helper function to create a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride for the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the MBConv block.\n        :return: A sequential container of layers forming the MBConv block.\n        \"\"\"\n        layers = []\n        expanded_channels = in_channels * expand_ratio\n        \n        # Expansion phase\n        if expand_ratio != 1:\n            layers.append(nn.Conv2d(in_channels, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))\n            layers.append(nn.BatchNorm2d(expanded_channels))\n            layers.append(nn.ReLU(inplace=True))\n        \n        # Depthwise convolution\n        layers.append(nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3, stride=stride, padding=1, groups=expanded_channels, bias=False))\n        layers.append(nn.BatchNorm2d(expanded_channels))\n        layers.append(nn.ReLU(inplace=True))\n        \n        # Squeeze and Excitation\n        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n        layers.append(nn.Conv2d(expanded_channels, expanded_channels // 4, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(expanded_channels // 4, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.Sigmoid())\n        \n        # Output phase\n        layers.append(nn.Conv2d(expanded_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.relu(self.bn_final(self.conv_final(x)))\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "24_EfficientNetB2", "problem_id": 24}, "task_id": "3-24"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        \"\"\"\n        ShuffleNet unit implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param groups: Number of groups for group convolution.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Ensure the output channels are divisible by groups\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        # First 1x1 group convolution\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        # Depthwise 3x3 convolution\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        # Second 1x1 group convolution\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        # Shuffle operation\n        self.shuffle = ChannelShuffle(groups)\n        \n        # Shortcut connection if input and output channels are the same\n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet unit.\n\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        \n        out += self.shortcut(x)\n        return out\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        \"\"\"\n        Channel shuffle operation.\n\n        :param groups: Number of groups for shuffling.\n        \"\"\"\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for channel shuffle.\n\n        :param x: Input tensor, shape (batch_size, channels, height, width)\n        :return: Output tensor, shape (batch_size, channels, height, width)\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        # Reshape\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        \n        # Transpose\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten\n        x = x.view(batch_size, -1, height, width)\n        \n        return x\n    \nbatch_size = 10\ninput_channels = 240\nout_channels = 480\ngroups = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [input_channels, out_channels, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        \"\"\"\n        ShuffleNet unit implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param groups: Number of groups for group convolution.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Ensure the output channels are divisible by groups\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        # First 1x1 group convolution\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        # Depthwise 3x3 convolution\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        # Second 1x1 group convolution\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        # Shuffle operation\n        self.shuffle = ChannelShuffle(groups)\n        \n        # Shortcut connection if input and output channels are the same\n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet unit.\n\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        \n        out += self.shortcut(x)\n        return out\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        \"\"\"\n        Channel shuffle operation.\n\n        :param groups: Number of groups for shuffling.\n        \"\"\"\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for channel shuffle.\n\n        :param x: Input tensor, shape (batch_size, channels, height, width)\n        :return: Output tensor, shape (batch_size, channels, height, width)\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        # Reshape\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        \n        # Transpose\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten\n        x = x.view(batch_size, -1, height, width)\n        \n        return x\n    \nbatch_size = 10\ninput_channels = 240\nout_channels = 480\ngroups = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [input_channels, out_channels, groups]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        \"\"\"\n        ShuffleNet unit implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param groups: Number of groups for group convolution.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Ensure the output channels are divisible by groups\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        # First 1x1 group convolution\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        # Depthwise 3x3 convolution\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        # Second 1x1 group convolution\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        # Shuffle operation\n        self.shuffle = ChannelShuffle(groups)\n        \n        # Shortcut connection if input and output channels are the same\n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet unit.\n\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        \n        out += self.shortcut(x)\n        return out\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        \"\"\"\n        Channel shuffle operation.\n\n        :param groups: Number of groups for shuffling.\n        \"\"\"\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for channel shuffle.\n\n        :param x: Input tensor, shape (batch_size, channels, height, width)\n        :return: Output tensor, shape (batch_size, channels, height, width)\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        # Reshape\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        \n        # Transpose\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten\n        x = x.view(batch_size, -1, height, width)\n        \n        return x\n    \nbatch_size = 10\ninput_channels = 240\nout_channels = 480\ngroups = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [input_channels, out_channels, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "25_ShuffleNetUnit", "problem_id": 25}, "task_id": "3-25"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        \"\"\"\n        ShuffleNet unit implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param groups: Number of groups for group convolution.\n        \"\"\"\n        super(ShuffleNetUnit, self).__init__()\n        \n        # Ensure the output channels are divisible by groups\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        # First 1x1 group convolution\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        # Depthwise 3x3 convolution\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        # Second 1x1 group convolution\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        # Shuffle operation\n        self.shuffle = ChannelShuffle(groups)\n        \n        # Shortcut connection if input and output channels are the same\n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet unit.\n\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        \n        out += self.shortcut(x)\n        return out\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        \"\"\"\n        Channel shuffle operation.\n\n        :param groups: Number of groups for shuffling.\n        \"\"\"\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for channel shuffle.\n\n        :param x: Input tensor, shape (batch_size, channels, height, width)\n        :return: Output tensor, shape (batch_size, channels, height, width)\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        # Reshape\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        \n        # Transpose\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten\n        x = x.view(batch_size, -1, height, width)\n        \n        return x\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        \"\"\"\n        ShuffleNet architecture.\n\n        :param num_classes: Number of output classes.\n        :param groups: Number of groups for group convolution.\n        :param stages_repeats: List of ints specifying the number of repeats for each stage.\n        :param stages_out_channels: List of ints specifying the output channels for each stage.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        \n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        \"\"\"\n        Helper function to create a stage of ShuffleNet units.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param repeats: Number of ShuffleNet units in the stage.\n        :param groups: Number of groups for group convolution.\n        :return: nn.Sequential containing the stage.\n        \"\"\"\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet.\n\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        \"\"\"\n        ShuffleNet unit implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param groups: Number of groups for group convolution.\n        \"\"\"\n        super(ShuffleNetUnit, self).__init__()\n        \n        # Ensure the output channels are divisible by groups\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        # First 1x1 group convolution\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        # Depthwise 3x3 convolution\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        # Second 1x1 group convolution\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        # Shuffle operation\n        self.shuffle = ChannelShuffle(groups)\n        \n        # Shortcut connection if input and output channels are the same\n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet unit.\n\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        \n        out += self.shortcut(x)\n        return out\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        \"\"\"\n        Channel shuffle operation.\n\n        :param groups: Number of groups for shuffling.\n        \"\"\"\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for channel shuffle.\n\n        :param x: Input tensor, shape (batch_size, channels, height, width)\n        :return: Output tensor, shape (batch_size, channels, height, width)\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        # Reshape\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        \n        # Transpose\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten\n        x = x.view(batch_size, -1, height, width)\n        \n        return x\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        \"\"\"\n        ShuffleNet architecture.\n\n        :param num_classes: Number of output classes.\n        :param groups: Number of groups for group convolution.\n        :param stages_repeats: List of ints specifying the number of repeats for each stage.\n        :param stages_out_channels: List of ints specifying the output channels for each stage.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        \n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        \"\"\"\n        Helper function to create a stage of ShuffleNet units.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param repeats: Number of ShuffleNet units in the stage.\n        :param groups: Number of groups for group convolution.\n        :return: nn.Sequential containing the stage.\n        \"\"\"\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet.\n\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        \"\"\"\n        ShuffleNet unit implementation.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param groups: Number of groups for group convolution.\n        \"\"\"\n        super(ShuffleNetUnit, self).__init__()\n        \n        # Ensure the output channels are divisible by groups\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        # First 1x1 group convolution\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        # Depthwise 3x3 convolution\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        # Second 1x1 group convolution\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        # Shuffle operation\n        self.shuffle = ChannelShuffle(groups)\n        \n        # Shortcut connection if input and output channels are the same\n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet unit.\n\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        \n        out += self.shortcut(x)\n        return out\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        \"\"\"\n        Channel shuffle operation.\n\n        :param groups: Number of groups for shuffling.\n        \"\"\"\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for channel shuffle.\n\n        :param x: Input tensor, shape (batch_size, channels, height, width)\n        :return: Output tensor, shape (batch_size, channels, height, width)\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        # Reshape\n        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n        \n        # Transpose\n        x = x.transpose(1, 2).contiguous()\n        \n        # Flatten\n        x = x.view(batch_size, -1, height, width)\n        \n        return x\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        \"\"\"\n        ShuffleNet architecture.\n\n        :param num_classes: Number of output classes.\n        :param groups: Number of groups for group convolution.\n        :param stages_repeats: List of ints specifying the number of repeats for each stage.\n        :param stages_out_channels: List of ints specifying the output channels for each stage.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        \n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        \"\"\"\n        Helper function to create a stage of ShuffleNet units.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param repeats: Number of ShuffleNet units in the stage.\n        :param groups: Number of groups for group convolution.\n        :return: nn.Sequential containing the stage.\n        \"\"\"\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass for ShuffleNet.\n\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "26_ShuffleNet", "problem_id": 26}, "task_id": "3-26"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_channels, stages, block_widths, output_classes):\n        \"\"\"\n        :param input_channels: int, Number of input channels for the first layer\n        :param stages: int, Number of stages in the RegNet architecture\n        :param block_widths: List[int], Width (number of channels) for each block in the stages\n        :param output_classes: int, Number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        self.stages = stages\n        self.block_widths = block_widths\n        \n        layers = []\n        current_channels = input_channels\n        \n        # Construct the stages with their respective blocks\n        for i in range(stages):\n            layers.append(self._make_stage(current_channels, block_widths[i]))\n            current_channels = block_widths[i]\n        \n        self.feature_extractor = nn.Sequential(*layers)\n        \n        # Final fully connected layer for classification\n        self.fc = nn.Linear(block_widths[-1], output_classes)\n    \n    def _make_stage(self, in_channels, out_channels):\n        \"\"\"\n        Creates a simple block for each stage.\n        :param in_channels: int, number of input channels\n        :param out_channels: int, number of output channels\n        :return: nn.Sequential block with convolutional layers\n        \"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RegNet model.\n        :param x: torch.Tensor of shape (batch_size, input_channels, height, width)\n        :return: torch.Tensor of shape (batch_size, output_classes)\n        \"\"\"\n        x = self.feature_extractor(x)\n        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling\n        x = self.fc(x)\n        return x\n\n# Test code for the RegNet model\nbatch_size = 8\ninput_channels = 3\nimage_height, image_width = 224, 224\nstages = 3\nblock_widths = [64, 128, 256]\noutput_classes = 10\n\ndef get_inputs():\n    \"\"\" Generates random input tensor of shape (batch_size, input_channels, height, width) \"\"\"\n    return [torch.rand(batch_size, input_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    \"\"\" Initializes model parameters \"\"\"\n    return [input_channels, stages, block_widths, output_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_channels, stages, block_widths, output_classes):\n        \"\"\"\n        :param input_channels: int, Number of input channels for the first layer\n        :param stages: int, Number of stages in the RegNet architecture\n        :param block_widths: List[int], Width (number of channels) for each block in the stages\n        :param output_classes: int, Number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        self.stages = stages\n        self.block_widths = block_widths\n        \n        layers = []\n        current_channels = input_channels\n        \n        # Construct the stages with their respective blocks\n        for i in range(stages):\n            layers.append(self._make_stage(current_channels, block_widths[i]))\n            current_channels = block_widths[i]\n        \n        self.feature_extractor = nn.Sequential(*layers)\n        \n        # Final fully connected layer for classification\n        self.fc = nn.Linear(block_widths[-1], output_classes)\n    \n    def _make_stage(self, in_channels, out_channels):\n        \"\"\"\n        Creates a simple block for each stage.\n        :param in_channels: int, number of input channels\n        :param out_channels: int, number of output channels\n        :return: nn.Sequential block with convolutional layers\n        \"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RegNet model.\n        :param x: torch.Tensor of shape (batch_size, input_channels, height, width)\n        :return: torch.Tensor of shape (batch_size, output_classes)\n        \"\"\"\n        x = self.feature_extractor(x)\n        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling\n        x = self.fc(x)\n        return x\n\n# Test code for the RegNet model\nbatch_size = 8\ninput_channels = 3\nimage_height, image_width = 224, 224\nstages = 3\nblock_widths = [64, 128, 256]\noutput_classes = 10\n\ndef get_inputs():\n    \"\"\" Generates random input tensor of shape (batch_size, input_channels, height, width) \"\"\"\n    return [torch.rand(batch_size, input_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    \"\"\" Initializes model parameters \"\"\"\n    return [input_channels, stages, block_widths, output_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_channels, stages, block_widths, output_classes):\n        \"\"\"\n        :param input_channels: int, Number of input channels for the first layer\n        :param stages: int, Number of stages in the RegNet architecture\n        :param block_widths: List[int], Width (number of channels) for each block in the stages\n        :param output_classes: int, Number of output classes for classification\n        \"\"\"\n        super(Model, self).__init__()\n\n        self.stages = stages\n        self.block_widths = block_widths\n        \n        layers = []\n        current_channels = input_channels\n        \n        # Construct the stages with their respective blocks\n        for i in range(stages):\n            layers.append(self._make_stage(current_channels, block_widths[i]))\n            current_channels = block_widths[i]\n        \n        self.feature_extractor = nn.Sequential(*layers)\n        \n        # Final fully connected layer for classification\n        self.fc = nn.Linear(block_widths[-1], output_classes)\n    \n    def _make_stage(self, in_channels, out_channels):\n        \"\"\"\n        Creates a simple block for each stage.\n        :param in_channels: int, number of input channels\n        :param out_channels: int, number of output channels\n        :return: nn.Sequential block with convolutional layers\n        \"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RegNet model.\n        :param x: torch.Tensor of shape (batch_size, input_channels, height, width)\n        :return: torch.Tensor of shape (batch_size, output_classes)\n        \"\"\"\n        x = self.feature_extractor(x)\n        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling\n        x = self.fc(x)\n        return x\n\n# Test code for the RegNet model\nbatch_size = 8\ninput_channels = 3\nimage_height, image_width = 224, 224\nstages = 3\nblock_widths = [64, 128, 256]\noutput_classes = 10\n\ndef get_inputs():\n    \"\"\" Generates random input tensor of shape (batch_size, input_channels, height, width) \"\"\"\n    return [torch.rand(batch_size, input_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    \"\"\" Initializes model parameters \"\"\"\n    return [input_channels, stages, block_widths, output_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "27_RegNet", "problem_id": 27}, "task_id": "3-27"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0.1, emb_dropout=0.1):\n        \"\"\"\n        Vision Transformer (ViT) model.\n\n        :param image_size: The size of the input image (assumed to be square).\n        :param patch_size: The size of each patch (assumed to be square).\n        :param num_classes: The number of output classes.\n        :param dim: The dimensionality of the embedding space.\n        :param depth: The number of transformer layers.\n        :param heads: The number of attention heads.\n        :param mlp_dim: The dimensionality of the MLP (Multi-Layer Perceptron) in the transformer.\n        :param channels: The number of channels in the input image (default is 3 for RGB).\n        :param dropout: Dropout rate applied in the MLP.\n        :param emb_dropout: Dropout rate applied to the embedded patches.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n        \n        self.patch_size = patch_size\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n        \n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n            num_layers=depth\n        )\n        \n        self.to_cls_token = nn.Identity()\n        self.mlp_head = nn.Sequential(\n            nn.Linear(dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, num_classes)\n        )\n    \n    def forward(self, img):\n        \"\"\"\n        Forward pass of the Vision Transformer.\n\n        :param img: The input image tensor, shape (batch_size, channels, image_size, image_size).\n        :return: The output tensor, shape (batch_size, num_classes).\n        \"\"\"\n        p = self.patch_size\n        \n        x = img.unfold(2, p, p).unfold(3, p, p).reshape(img.shape[0], -1, p*p*img.shape[1])\n        x = self.patch_to_embedding(x)\n        \n        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding\n        x = self.dropout(x)\n        \n        x = self.transformer(x)\n        \n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)\n\n# Test code\nimage_size = 224\npatch_size = 16\nnum_classes = 10\ndim = 512\ndepth = 6\nheads = 8\nmlp_dim = 2048\nchannels = 3\ndropout = 0.0\nemb_dropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(2, channels, image_size, image_size)]\n\ndef get_init_inputs():\n    return [image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels, dropout, emb_dropout]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0.1, emb_dropout=0.1):\n        \"\"\"\n        Vision Transformer (ViT) model.\n\n        :param image_size: The size of the input image (assumed to be square).\n        :param patch_size: The size of each patch (assumed to be square).\n        :param num_classes: The number of output classes.\n        :param dim: The dimensionality of the embedding space.\n        :param depth: The number of transformer layers.\n        :param heads: The number of attention heads.\n        :param mlp_dim: The dimensionality of the MLP (Multi-Layer Perceptron) in the transformer.\n        :param channels: The number of channels in the input image (default is 3 for RGB).\n        :param dropout: Dropout rate applied in the MLP.\n        :param emb_dropout: Dropout rate applied to the embedded patches.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n        \n        self.patch_size = patch_size\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n        \n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n            num_layers=depth\n        )\n        \n        self.to_cls_token = nn.Identity()\n        self.mlp_head = nn.Sequential(\n            nn.Linear(dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, num_classes)\n        )\n    \n    def forward(self, img):\n        \"\"\"\n        Forward pass of the Vision Transformer.\n\n        :param img: The input image tensor, shape (batch_size, channels, image_size, image_size).\n        :return: The output tensor, shape (batch_size, num_classes).\n        \"\"\"\n        p = self.patch_size\n        \n        x = img.unfold(2, p, p).unfold(3, p, p).reshape(img.shape[0], -1, p*p*img.shape[1])\n        x = self.patch_to_embedding(x)\n        \n        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding\n        x = self.dropout(x)\n        \n        x = self.transformer(x)\n        \n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)\n\n# Test code\nimage_size = 224\npatch_size = 16\nnum_classes = 10\ndim = 512\ndepth = 6\nheads = 8\nmlp_dim = 2048\nchannels = 3\ndropout = 0.0\nemb_dropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(2, channels, image_size, image_size)]\n\ndef get_init_inputs():\n    return [image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels, dropout, emb_dropout]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0.1, emb_dropout=0.1):\n        \"\"\"\n        Vision Transformer (ViT) model.\n\n        :param image_size: The size of the input image (assumed to be square).\n        :param patch_size: The size of each patch (assumed to be square).\n        :param num_classes: The number of output classes.\n        :param dim: The dimensionality of the embedding space.\n        :param depth: The number of transformer layers.\n        :param heads: The number of attention heads.\n        :param mlp_dim: The dimensionality of the MLP (Multi-Layer Perceptron) in the transformer.\n        :param channels: The number of channels in the input image (default is 3 for RGB).\n        :param dropout: Dropout rate applied in the MLP.\n        :param emb_dropout: Dropout rate applied to the embedded patches.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n        \n        self.patch_size = patch_size\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n        \n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n            num_layers=depth\n        )\n        \n        self.to_cls_token = nn.Identity()\n        self.mlp_head = nn.Sequential(\n            nn.Linear(dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, num_classes)\n        )\n    \n    def forward(self, img):\n        \"\"\"\n        Forward pass of the Vision Transformer.\n\n        :param img: The input image tensor, shape (batch_size, channels, image_size, image_size).\n        :return: The output tensor, shape (batch_size, num_classes).\n        \"\"\"\n        p = self.patch_size\n        \n        x = img.unfold(2, p, p).unfold(3, p, p).reshape(img.shape[0], -1, p*p*img.shape[1])\n        x = self.patch_to_embedding(x)\n        \n        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding\n        x = self.dropout(x)\n        \n        x = self.transformer(x)\n        \n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)\n\n# Test code\nimage_size = 224\npatch_size = 16\nnum_classes = 10\ndim = 512\ndepth = 6\nheads = 8\nmlp_dim = 2048\nchannels = 3\ndropout = 0.0\nemb_dropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(2, channels, image_size, image_size)]\n\ndef get_init_inputs():\n    return [image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels, dropout, emb_dropout]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "28_VisionTransformer", "problem_id": 28}, "task_id": "3-28"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom itertools import repeat\nimport collections.abc\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass SwinMLPBlock(nn.Module):\n    r\"\"\" Swin MLP Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.padding = [self.window_size - self.shift_size, self.shift_size,\n                        self.window_size - self.shift_size, self.shift_size]  # P_l,P_r,P_t,P_b\n\n        self.norm1 = norm_layer(dim)\n        # use group convolution to implement multi-head MLP\n        self.spatial_mlp = nn.Conv1d(self.num_heads * self.window_size ** 2,\n                                     self.num_heads * self.window_size ** 2,\n                                     kernel_size=1,\n                                     groups=self.num_heads)\n\n        self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # shift\n        if self.shift_size > 0:\n            P_l, P_r, P_t, P_b = self.padding\n            shifted_x = F.pad(x, [0, 0, P_l, P_r, P_t, P_b], \"constant\", 0)\n        else:\n            shifted_x = x\n        _, _H, _W, _ = shifted_x.shape\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # Window/Shifted-Window Spatial MLP\n        x_windows_heads = x_windows.view(-1, self.window_size * self.window_size, self.num_heads, C // self.num_heads)\n        x_windows_heads = x_windows_heads.transpose(1, 2)  # nW*B, nH, window_size*window_size, C//nH\n        x_windows_heads = x_windows_heads.reshape(-1, self.num_heads * self.window_size * self.window_size,\n                                                  C // self.num_heads)\n        spatial_mlp_windows = self.spatial_mlp(x_windows_heads)  # nW*B, nH*window_size*window_size, C//nH\n        spatial_mlp_windows = spatial_mlp_windows.view(-1, self.num_heads, self.window_size * self.window_size,\n                                                       C // self.num_heads).transpose(1, 2)\n        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size * self.window_size, C)\n\n        # merge windows\n        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(spatial_mlp_windows, self.window_size, _H, _W)  # B H' W' C\n\n        # reverse shift\n        if self.shift_size > 0:\n            P_l, P_r, P_t, P_b = self.padding\n            x = shifted_x[:, P_t:-P_b, P_l:-P_r, :].contiguous()\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin MLP layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinMLPBlock(dim=dim, input_resolution=input_resolution,\n                         num_heads=num_heads, window_size=window_size,\n                         shift_size=0 if (i % 2 == 0) else window_size // 2,\n                         mlp_ratio=mlp_ratio,\n                         drop=drop,\n                         drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                         norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass Model(nn.Module):\n    r\"\"\" Swin MLP\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin MLP layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        drop_rate (float): Dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, patch_norm=True,\n                 use_checkpoint=False, **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               drop=drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n    \nbatch_size = 10\nimage_size = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, image_size, image_size)]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom itertools import repeat\nimport collections.abc\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass SwinMLPBlock(nn.Module):\n    r\"\"\" Swin MLP Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.padding = [self.window_size - self.shift_size, self.shift_size,\n                        self.window_size - self.shift_size, self.shift_size]  # P_l,P_r,P_t,P_b\n\n        self.norm1 = norm_layer(dim)\n        # use group convolution to implement multi-head MLP\n        self.spatial_mlp = nn.Conv1d(self.num_heads * self.window_size ** 2,\n                                     self.num_heads * self.window_size ** 2,\n                                     kernel_size=1,\n                                     groups=self.num_heads)\n\n        self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # shift\n        if self.shift_size > 0:\n            P_l, P_r, P_t, P_b = self.padding\n            shifted_x = F.pad(x, [0, 0, P_l, P_r, P_t, P_b], \"constant\", 0)\n        else:\n            shifted_x = x\n        _, _H, _W, _ = shifted_x.shape\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # Window/Shifted-Window Spatial MLP\n        x_windows_heads = x_windows.view(-1, self.window_size * self.window_size, self.num_heads, C // self.num_heads)\n        x_windows_heads = x_windows_heads.transpose(1, 2)  # nW*B, nH, window_size*window_size, C//nH\n        x_windows_heads = x_windows_heads.reshape(-1, self.num_heads * self.window_size * self.window_size,\n                                                  C // self.num_heads)\n        spatial_mlp_windows = self.spatial_mlp(x_windows_heads)  # nW*B, nH*window_size*window_size, C//nH\n        spatial_mlp_windows = spatial_mlp_windows.view(-1, self.num_heads, self.window_size * self.window_size,\n                                                       C // self.num_heads).transpose(1, 2)\n        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size * self.window_size, C)\n\n        # merge windows\n        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(spatial_mlp_windows, self.window_size, _H, _W)  # B H' W' C\n\n        # reverse shift\n        if self.shift_size > 0:\n            P_l, P_r, P_t, P_b = self.padding\n            x = shifted_x[:, P_t:-P_b, P_l:-P_r, :].contiguous()\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin MLP layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinMLPBlock(dim=dim, input_resolution=input_resolution,\n                         num_heads=num_heads, window_size=window_size,\n                         shift_size=0 if (i % 2 == 0) else window_size // 2,\n                         mlp_ratio=mlp_ratio,\n                         drop=drop,\n                         drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                         norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass Model(nn.Module):\n    r\"\"\" Swin MLP\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin MLP layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        drop_rate (float): Dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, patch_norm=True,\n                 use_checkpoint=False, **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               drop=drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n    \nbatch_size = 10\nimage_size = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, image_size, image_size)]\n\ndef get_init_inputs():\n    return []", "level": "3", "type": "", "ground_truth": {"pytorch_module": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom itertools import repeat\nimport collections.abc\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass SwinMLPBlock(nn.Module):\n    r\"\"\" Swin MLP Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.padding = [self.window_size - self.shift_size, self.shift_size,\n                        self.window_size - self.shift_size, self.shift_size]  # P_l,P_r,P_t,P_b\n\n        self.norm1 = norm_layer(dim)\n        # use group convolution to implement multi-head MLP\n        self.spatial_mlp = nn.Conv1d(self.num_heads * self.window_size ** 2,\n                                     self.num_heads * self.window_size ** 2,\n                                     kernel_size=1,\n                                     groups=self.num_heads)\n\n        self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # shift\n        if self.shift_size > 0:\n            P_l, P_r, P_t, P_b = self.padding\n            shifted_x = F.pad(x, [0, 0, P_l, P_r, P_t, P_b], \"constant\", 0)\n        else:\n            shifted_x = x\n        _, _H, _W, _ = shifted_x.shape\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # Window/Shifted-Window Spatial MLP\n        x_windows_heads = x_windows.view(-1, self.window_size * self.window_size, self.num_heads, C // self.num_heads)\n        x_windows_heads = x_windows_heads.transpose(1, 2)  # nW*B, nH, window_size*window_size, C//nH\n        x_windows_heads = x_windows_heads.reshape(-1, self.num_heads * self.window_size * self.window_size,\n                                                  C // self.num_heads)\n        spatial_mlp_windows = self.spatial_mlp(x_windows_heads)  # nW*B, nH*window_size*window_size, C//nH\n        spatial_mlp_windows = spatial_mlp_windows.view(-1, self.num_heads, self.window_size * self.window_size,\n                                                       C // self.num_heads).transpose(1, 2)\n        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size * self.window_size, C)\n\n        # merge windows\n        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(spatial_mlp_windows, self.window_size, _H, _W)  # B H' W' C\n\n        # reverse shift\n        if self.shift_size > 0:\n            P_l, P_r, P_t, P_b = self.padding\n            x = shifted_x[:, P_t:-P_b, P_l:-P_r, :].contiguous()\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin MLP layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinMLPBlock(dim=dim, input_resolution=input_resolution,\n                         num_heads=num_heads, window_size=window_size,\n                         shift_size=0 if (i % 2 == 0) else window_size // 2,\n                         mlp_ratio=mlp_ratio,\n                         drop=drop,\n                         drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                         norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass Model(nn.Module):\n    r\"\"\" Swin MLP\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin MLP layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        drop_rate (float): Dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, patch_norm=True,\n                 use_checkpoint=False, **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               drop=drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n    \nbatch_size = 10\nimage_size = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, image_size, image_size)]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "29_SwinMLP", "problem_id": 29}, "task_id": "3-29"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 128\ninput_size = 16384\nhidden_layer_sizes = [32768, 32768]\noutput_size = 16384\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 128\ninput_size = 16384\nhidden_layer_sizes = [32768, 32768]\noutput_size = 16384\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 128\ninput_size = 16384\nhidden_layer_sizes = [32768, 32768]\noutput_size = 16384\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "2_ShallowWideMLP", "problem_id": 2}, "task_id": "3-2"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python# --------------------------------------------------------\n# Swin Transformer V2\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\nfrom itertools import repeat\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,\n                 pretrained_window_size=[0, 0]):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.pretrained_window_size = pretrained_window_size\n        self.num_heads = num_heads\n\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n\n        # mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                                     nn.ReLU(inplace=True),\n                                     nn.Linear(512, num_heads, bias=False))\n\n        # get relative_coords_table\n        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n        relative_coords_table = torch.stack(\n            torch.meshgrid([relative_coords_h,\n                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n        if pretrained_window_size[0] > 0:\n            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)\n        else:\n            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n        relative_coords_table *= 8  # normalize to -8, 8\n        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n            torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n\n        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(dim))\n            self.v_bias = nn.Parameter(torch.zeros(dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        # cosine attention\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale.to(x.device), max=torch.log(torch.tensor(1. / 0.01, device=x.device))).exp()\n        attn = attn * logit_scale\n\n        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        pretrained_window_size (int): Window size in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,\n            pretrained_window_size=to_2tuple(pretrained_window_size))\n\n        self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path(self.norm1(x))\n\n        # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(2 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.reduction(x)\n        x = self.norm(x)\n\n        return x\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 pretrained_window_size=0):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 pretrained_window_size=pretrained_window_size)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n    \n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n\nclass Model(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., qkv_bias=True,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, patch_norm=True,\n                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint,\n                               pretrained_window_size=pretrained_window_sizes[i_layer])\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\nbatch_size = 10\nimage_size = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, image_size, image_size)]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "# --------------------------------------------------------\n# Swin Transformer V2\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\nfrom itertools import repeat\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,\n                 pretrained_window_size=[0, 0]):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.pretrained_window_size = pretrained_window_size\n        self.num_heads = num_heads\n\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n\n        # mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                                     nn.ReLU(inplace=True),\n                                     nn.Linear(512, num_heads, bias=False))\n\n        # get relative_coords_table\n        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n        relative_coords_table = torch.stack(\n            torch.meshgrid([relative_coords_h,\n                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n        if pretrained_window_size[0] > 0:\n            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)\n        else:\n            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n        relative_coords_table *= 8  # normalize to -8, 8\n        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n            torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n\n        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(dim))\n            self.v_bias = nn.Parameter(torch.zeros(dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        # cosine attention\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale.to(x.device), max=torch.log(torch.tensor(1. / 0.01, device=x.device))).exp()\n        attn = attn * logit_scale\n\n        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        pretrained_window_size (int): Window size in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,\n            pretrained_window_size=to_2tuple(pretrained_window_size))\n\n        self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path(self.norm1(x))\n\n        # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(2 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.reduction(x)\n        x = self.norm(x)\n\n        return x\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 pretrained_window_size=0):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 pretrained_window_size=pretrained_window_size)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n    \n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n\nclass Model(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., qkv_bias=True,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, patch_norm=True,\n                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint,\n                               pretrained_window_size=pretrained_window_sizes[i_layer])\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\nbatch_size = 10\nimage_size = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, image_size, image_size)]\n\ndef get_init_inputs():\n    return []", "level": "3", "type": "", "ground_truth": {"pytorch_module": "# --------------------------------------------------------\n# Swin Transformer V2\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\nfrom itertools import repeat\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,\n                 pretrained_window_size=[0, 0]):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.pretrained_window_size = pretrained_window_size\n        self.num_heads = num_heads\n\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n\n        # mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                                     nn.ReLU(inplace=True),\n                                     nn.Linear(512, num_heads, bias=False))\n\n        # get relative_coords_table\n        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n        relative_coords_table = torch.stack(\n            torch.meshgrid([relative_coords_h,\n                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n        if pretrained_window_size[0] > 0:\n            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)\n        else:\n            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n        relative_coords_table *= 8  # normalize to -8, 8\n        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n            torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n\n        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(dim))\n            self.v_bias = nn.Parameter(torch.zeros(dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        # cosine attention\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale.to(x.device), max=torch.log(torch.tensor(1. / 0.01, device=x.device))).exp()\n        attn = attn * logit_scale\n\n        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        pretrained_window_size (int): Window size in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,\n            pretrained_window_size=to_2tuple(pretrained_window_size))\n\n        self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path(self.norm1(x))\n\n        # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(2 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.reduction(x)\n        x = self.norm(x)\n\n        return x\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 pretrained_window_size=0):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 pretrained_window_size=pretrained_window_size)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n    \n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n\nclass Model(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., qkv_bias=True,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, patch_norm=True,\n                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint,\n                               pretrained_window_size=pretrained_window_sizes[i_layer])\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\nbatch_size = 10\nimage_size = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, image_size, image_size)]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "30_SwinTransformerV2", "problem_id": 30}, "task_id": "3-30"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        \"\"\"\n        Attention Block using Multihead Self-Attention.\n        :param embed_dim: Embedding dimension (the number of channels)\n        :param num_heads: Number of attention heads\n        \"\"\"\n        super(Model, self).__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the AttentionBlock.\n        :param x: Input tensor of shape (B, C, H, W)\n        :return: Output tensor of the same shape (B, C, H, W)\n        \"\"\"\n        B, C, H, W = x.shape\n        x = x.view(B, C, H * W).permute(2, 0, 1)  # (seq_len, batch_size, embed_dim)\n        attn_output, _ = self.attn(x, x, x)\n        x = self.norm(attn_output + x)  # (seq_len, batch_size, embed_dim)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\nembed_dim = 128\nnum_heads = 4\nbatch_size = 2\nnum_channels = embed_dim\nimage_height = 128\nimage_width = 128\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    return [embed_dim, num_heads]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        \"\"\"\n        Attention Block using Multihead Self-Attention.\n        :param embed_dim: Embedding dimension (the number of channels)\n        :param num_heads: Number of attention heads\n        \"\"\"\n        super(Model, self).__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the AttentionBlock.\n        :param x: Input tensor of shape (B, C, H, W)\n        :return: Output tensor of the same shape (B, C, H, W)\n        \"\"\"\n        B, C, H, W = x.shape\n        x = x.view(B, C, H * W).permute(2, 0, 1)  # (seq_len, batch_size, embed_dim)\n        attn_output, _ = self.attn(x, x, x)\n        x = self.norm(attn_output + x)  # (seq_len, batch_size, embed_dim)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\nembed_dim = 128\nnum_heads = 4\nbatch_size = 2\nnum_channels = embed_dim\nimage_height = 128\nimage_width = 128\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    return [embed_dim, num_heads]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        \"\"\"\n        Attention Block using Multihead Self-Attention.\n        :param embed_dim: Embedding dimension (the number of channels)\n        :param num_heads: Number of attention heads\n        \"\"\"\n        super(Model, self).__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the AttentionBlock.\n        :param x: Input tensor of shape (B, C, H, W)\n        :return: Output tensor of the same shape (B, C, H, W)\n        \"\"\"\n        B, C, H, W = x.shape\n        x = x.view(B, C, H * W).permute(2, 0, 1)  # (seq_len, batch_size, embed_dim)\n        attn_output, _ = self.attn(x, x, x)\n        x = self.norm(attn_output + x)  # (seq_len, batch_size, embed_dim)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\nembed_dim = 128\nnum_heads = 4\nbatch_size = 2\nnum_channels = embed_dim\nimage_height = 128\nimage_width = 128\n\ndef get_inputs():\n    return [torch.rand(batch_size, num_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    return [embed_dim, num_heads]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "31_VisionAttention", "problem_id": 31}, "task_id": "3-31"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, num_classes, embed_dim=512, num_heads=8, num_layers=6, \n                 mlp_ratio=4.0, patch_size=4, in_channels=3, image_size=32):\n        \"\"\"\n        Convolutional Vision Transformer (CViT) implementation.\n        :param num_classes: Number of output classes for classification.\n        :param embed_dim: Dimensionality of the embedding space.\n        :param num_heads: Number of attention heads.\n        :param num_layers: Number of transformer layers.\n        :param mlp_ratio: Ratio of the MLP hidden dimension to the embedding dimension.\n        :param patch_size: Size of the convolutional patches.\n        :param in_channels: Number of input channels (e.g., 3 for RGB images).\n        :param image_size: Height/width of the square input image.\n        \"\"\"\n        super(Model, self).__init__()\n\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.embed_dim = embed_dim\n\n        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        num_patches = (image_size // patch_size) ** 2  # Total number of patches after conv\n        self.linear_proj = nn.Linear(embed_dim * num_patches, embed_dim)\n\n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=embed_dim,\n                nhead=num_heads,\n                dim_feedforward=int(embed_dim * mlp_ratio),\n                dropout=0.0,\n                batch_first=True\n            ) for _ in range(num_layers)\n        ])\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.fc_out = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the CViT model.\n        :param x: Input tensor of shape (B, C, H, W)\n        :return: Output tensor of shape (B, num_classes)\n        \"\"\"\n        B = x.size(0)\n        x = self.conv1(x)                  # (B, embed_dim, H/patch_size, W/patch_size)\n        x = x.flatten(start_dim=1)         # (B, embed_dim * num_patches)\n        x = self.linear_proj(x)            # (B, embed_dim)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)  # (B, 2, embed_dim)\n\n        for layer in self.transformer_layers:\n            x = layer(x)\n\n        return self.fc_out(x[:, 0])        # Use [CLS] token for classification\n\n# === Test config ===\nbatch_size = 10\nimage_size = 32\nembed_dim = 128\nin_channels = 3\nnum_heads = 4\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, image_size, image_size)]\n\ndef get_init_inputs():\n    return [num_classes, embed_dim, num_heads]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, num_classes, embed_dim=512, num_heads=8, num_layers=6, \n                 mlp_ratio=4.0, patch_size=4, in_channels=3, image_size=32):\n        \"\"\"\n        Convolutional Vision Transformer (CViT) implementation.\n        :param num_classes: Number of output classes for classification.\n        :param embed_dim: Dimensionality of the embedding space.\n        :param num_heads: Number of attention heads.\n        :param num_layers: Number of transformer layers.\n        :param mlp_ratio: Ratio of the MLP hidden dimension to the embedding dimension.\n        :param patch_size: Size of the convolutional patches.\n        :param in_channels: Number of input channels (e.g., 3 for RGB images).\n        :param image_size: Height/width of the square input image.\n        \"\"\"\n        super(Model, self).__init__()\n\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.embed_dim = embed_dim\n\n        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        num_patches = (image_size // patch_size) ** 2  # Total number of patches after conv\n        self.linear_proj = nn.Linear(embed_dim * num_patches, embed_dim)\n\n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=embed_dim,\n                nhead=num_heads,\n                dim_feedforward=int(embed_dim * mlp_ratio),\n                dropout=0.0,\n                batch_first=True\n            ) for _ in range(num_layers)\n        ])\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.fc_out = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the CViT model.\n        :param x: Input tensor of shape (B, C, H, W)\n        :return: Output tensor of shape (B, num_classes)\n        \"\"\"\n        B = x.size(0)\n        x = self.conv1(x)                  # (B, embed_dim, H/patch_size, W/patch_size)\n        x = x.flatten(start_dim=1)         # (B, embed_dim * num_patches)\n        x = self.linear_proj(x)            # (B, embed_dim)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)  # (B, 2, embed_dim)\n\n        for layer in self.transformer_layers:\n            x = layer(x)\n\n        return self.fc_out(x[:, 0])        # Use [CLS] token for classification\n\n# === Test config ===\nbatch_size = 10\nimage_size = 32\nembed_dim = 128\nin_channels = 3\nnum_heads = 4\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, image_size, image_size)]\n\ndef get_init_inputs():\n    return [num_classes, embed_dim, num_heads]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, num_classes, embed_dim=512, num_heads=8, num_layers=6, \n                 mlp_ratio=4.0, patch_size=4, in_channels=3, image_size=32):\n        \"\"\"\n        Convolutional Vision Transformer (CViT) implementation.\n        :param num_classes: Number of output classes for classification.\n        :param embed_dim: Dimensionality of the embedding space.\n        :param num_heads: Number of attention heads.\n        :param num_layers: Number of transformer layers.\n        :param mlp_ratio: Ratio of the MLP hidden dimension to the embedding dimension.\n        :param patch_size: Size of the convolutional patches.\n        :param in_channels: Number of input channels (e.g., 3 for RGB images).\n        :param image_size: Height/width of the square input image.\n        \"\"\"\n        super(Model, self).__init__()\n\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.embed_dim = embed_dim\n\n        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        num_patches = (image_size // patch_size) ** 2  # Total number of patches after conv\n        self.linear_proj = nn.Linear(embed_dim * num_patches, embed_dim)\n\n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=embed_dim,\n                nhead=num_heads,\n                dim_feedforward=int(embed_dim * mlp_ratio),\n                dropout=0.0,\n                batch_first=True\n            ) for _ in range(num_layers)\n        ])\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.fc_out = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the CViT model.\n        :param x: Input tensor of shape (B, C, H, W)\n        :return: Output tensor of shape (B, num_classes)\n        \"\"\"\n        B = x.size(0)\n        x = self.conv1(x)                  # (B, embed_dim, H/patch_size, W/patch_size)\n        x = x.flatten(start_dim=1)         # (B, embed_dim * num_patches)\n        x = self.linear_proj(x)            # (B, embed_dim)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)  # (B, 2, embed_dim)\n\n        for layer in self.transformer_layers:\n            x = layer(x)\n\n        return self.fc_out(x[:, 0])        # Use [CLS] token for classification\n\n# === Test config ===\nbatch_size = 10\nimage_size = 32\nembed_dim = 128\nin_channels = 3\nnum_heads = 4\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, image_size, image_size)]\n\ndef get_init_inputs():\n    return [num_classes, embed_dim, num_heads]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "32_ConvolutionalVisionTransformer", "problem_id": 32}, "task_id": "3-32"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        \"\"\"\n        Initialize the Vanilla RNN model.\n        \n        :param input_size: The number of input features (int).\n        :param hidden_size: The size of the hidden state (int).\n        :param output_size: The number of output features (int).\n        \"\"\"\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.hidden = torch.randn((batch_size, hidden_size))\n        \n        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden\n        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.tanh = nn.Tanh()  # Activation function for hidden state\n    \n    def forward(self, x: torch.Tensor, initial_hidden=None) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Vanilla RNN.\n        \n        :param x: Input tensor of shape (batch_size, input_size).\n        :param hidden: Hidden state tensor of shape (batch_size, hidden_size).\n        :return: Output tensor of shape (batch_size, output_size), and the new hidden state.\n        \"\"\"\n        if initial_hidden is not None:\n            self.hidden.copy_(initial_hidden)\n        self.hidden = self.hidden.to(x.device)\n        combined = torch.cat((x, self.hidden), dim=1)  # Concatenate input and hidden state\n        self.hidden = self.tanh(self.i2h(combined))  # Update hidden state\n        output = self.h2o(self.hidden)  # Compute output\n        return output\n\nbatch_size = 256\ninput_size = 16384\nhidden_size = 16384\noutput_size = 8192\nsequence_length = 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size),torch.rand(batch_size, hidden_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        \"\"\"\n        Initialize the Vanilla RNN model.\n        \n        :param input_size: The number of input features (int).\n        :param hidden_size: The size of the hidden state (int).\n        :param output_size: The number of output features (int).\n        \"\"\"\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.hidden = torch.randn((batch_size, hidden_size))\n        \n        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden\n        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.tanh = nn.Tanh()  # Activation function for hidden state\n    \n    def forward(self, x: torch.Tensor, initial_hidden=None) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Vanilla RNN.\n        \n        :param x: Input tensor of shape (batch_size, input_size).\n        :param hidden: Hidden state tensor of shape (batch_size, hidden_size).\n        :return: Output tensor of shape (batch_size, output_size), and the new hidden state.\n        \"\"\"\n        if initial_hidden is not None:\n            self.hidden.copy_(initial_hidden)\n        self.hidden = self.hidden.to(x.device)\n        combined = torch.cat((x, self.hidden), dim=1)  # Concatenate input and hidden state\n        self.hidden = self.tanh(self.i2h(combined))  # Update hidden state\n        output = self.h2o(self.hidden)  # Compute output\n        return output\n\nbatch_size = 256\ninput_size = 16384\nhidden_size = 16384\noutput_size = 8192\nsequence_length = 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size),torch.rand(batch_size, hidden_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        \"\"\"\n        Initialize the Vanilla RNN model.\n        \n        :param input_size: The number of input features (int).\n        :param hidden_size: The size of the hidden state (int).\n        :param output_size: The number of output features (int).\n        \"\"\"\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.hidden = torch.randn((batch_size, hidden_size))\n        \n        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden\n        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.tanh = nn.Tanh()  # Activation function for hidden state\n    \n    def forward(self, x: torch.Tensor, initial_hidden=None) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Vanilla RNN.\n        \n        :param x: Input tensor of shape (batch_size, input_size).\n        :param hidden: Hidden state tensor of shape (batch_size, hidden_size).\n        :return: Output tensor of shape (batch_size, output_size), and the new hidden state.\n        \"\"\"\n        if initial_hidden is not None:\n            self.hidden.copy_(initial_hidden)\n        self.hidden = self.hidden.to(x.device)\n        combined = torch.cat((x, self.hidden), dim=1)  # Concatenate input and hidden state\n        self.hidden = self.tanh(self.i2h(combined))  # Update hidden state\n        output = self.h2o(self.hidden)  # Compute output\n        return output\n\nbatch_size = 256\ninput_size = 16384\nhidden_size = 16384\noutput_size = 8192\nsequence_length = 256\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size),torch.rand(batch_size, hidden_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "33_VanillaRNN", "problem_id": 33}, "task_id": "3-33"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        \"\"\"\n        Initialize the Vanilla RNN model.\n\n        :param input_size: The number of input features (int).\n        :param hidden_size: The size of the hidden state (int).\n        :param output_size: The number of output features (int).\n        \"\"\"\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden\n        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.tanh = nn.Tanh()  # Activation function for hidden state\n\n    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Vanilla RNN.\n\n        :param x: Input tensor of shape (seq_len, batch_size, input_size)\n        :param h0: Initial hidden state tensor of shape (batch_size, hidden_size)\n        :return: Output tensor of shape (seq_len, batch_size, output_size)\n        \"\"\"\n        seq_len, batch_size, _ = x.size()\n        hidden = h0.to(x.device)\n        outputs = []\n\n        for t in range(seq_len):\n            combined = torch.cat((x[t], hidden), dim=1)  # Concatenate input and hidden state\n            hidden = self.tanh(self.i2h(combined))  # Update hidden state\n            output = self.h2o(hidden)  # Compute output\n            outputs.append(output)\n\n        return torch.stack(outputs, dim=0)  # (seq_len, batch_size, output_size)\n\n# === Test configuration ===\nbatch_size = 8\ninput_size = 1024\nhidden_size = 256\noutput_size = 128\nsequence_length = 256\n\ndef get_inputs():\n    return [\n        torch.rand(sequence_length, batch_size, input_size),\n        torch.rand(batch_size, hidden_size)\n    ]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        \"\"\"\n        Initialize the Vanilla RNN model.\n\n        :param input_size: The number of input features (int).\n        :param hidden_size: The size of the hidden state (int).\n        :param output_size: The number of output features (int).\n        \"\"\"\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden\n        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.tanh = nn.Tanh()  # Activation function for hidden state\n\n    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Vanilla RNN.\n\n        :param x: Input tensor of shape (seq_len, batch_size, input_size)\n        :param h0: Initial hidden state tensor of shape (batch_size, hidden_size)\n        :return: Output tensor of shape (seq_len, batch_size, output_size)\n        \"\"\"\n        seq_len, batch_size, _ = x.size()\n        hidden = h0.to(x.device)\n        outputs = []\n\n        for t in range(seq_len):\n            combined = torch.cat((x[t], hidden), dim=1)  # Concatenate input and hidden state\n            hidden = self.tanh(self.i2h(combined))  # Update hidden state\n            output = self.h2o(hidden)  # Compute output\n            outputs.append(output)\n\n        return torch.stack(outputs, dim=0)  # (seq_len, batch_size, output_size)\n\n# === Test configuration ===\nbatch_size = 8\ninput_size = 1024\nhidden_size = 256\noutput_size = 128\nsequence_length = 256\n\ndef get_inputs():\n    return [\n        torch.rand(sequence_length, batch_size, input_size),\n        torch.rand(batch_size, hidden_size)\n    ]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        \"\"\"\n        Initialize the Vanilla RNN model.\n\n        :param input_size: The number of input features (int).\n        :param hidden_size: The size of the hidden state (int).\n        :param output_size: The number of output features (int).\n        \"\"\"\n        super(Model, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden\n        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.tanh = nn.Tanh()  # Activation function for hidden state\n\n    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the Vanilla RNN.\n\n        :param x: Input tensor of shape (seq_len, batch_size, input_size)\n        :param h0: Initial hidden state tensor of shape (batch_size, hidden_size)\n        :return: Output tensor of shape (seq_len, batch_size, output_size)\n        \"\"\"\n        seq_len, batch_size, _ = x.size()\n        hidden = h0.to(x.device)\n        outputs = []\n\n        for t in range(seq_len):\n            combined = torch.cat((x[t], hidden), dim=1)  # Concatenate input and hidden state\n            hidden = self.tanh(self.i2h(combined))  # Update hidden state\n            output = self.h2o(hidden)  # Compute output\n            outputs.append(output)\n\n        return torch.stack(outputs, dim=0)  # (seq_len, batch_size, output_size)\n\n# === Test configuration ===\nbatch_size = 8\ninput_size = 1024\nhidden_size = 256\noutput_size = 128\nsequence_length = 256\n\ndef get_inputs():\n    return [\n        torch.rand(sequence_length, batch_size, input_size),\n        torch.rand(batch_size, hidden_size)\n    ]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "34_VanillaRNNHidden", "problem_id": 34}, "task_id": "3-34"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer\n        \"\"\"\n        super(Model, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n                            batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None, c0=None):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :param h0: Optional initial hidden state (num_layers, batch_size, hidden_size)\n        :param c0: Optional initial cell state (num_layers, batch_size, hidden_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        batch_size = x.size(0)\n\n        if h0 is None:\n            h0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)\n        if c0 is None:\n            c0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)\n\n        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)\n        out = self.fc(out[:, -1, :])     # out: (batch_size, output_size)\n\n        return out\n\n# === Test configuration ===\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer\n        \"\"\"\n        super(Model, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n                            batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None, c0=None):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :param h0: Optional initial hidden state (num_layers, batch_size, hidden_size)\n        :param c0: Optional initial cell state (num_layers, batch_size, hidden_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        batch_size = x.size(0)\n\n        if h0 is None:\n            h0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)\n        if c0 is None:\n            c0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)\n\n        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)\n        out = self.fc(out[:, -1, :])     # out: (batch_size, output_size)\n\n        return out\n\n# === Test configuration ===\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer\n        \"\"\"\n        super(Model, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n                            batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None, c0=None):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :param h0: Optional initial hidden state (num_layers, batch_size, hidden_size)\n        :param c0: Optional initial cell state (num_layers, batch_size, hidden_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        batch_size = x.size(0)\n\n        if h0 is None:\n            h0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)\n        if c0 is None:\n            c0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)\n\n        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)\n        out = self.fc(out[:, -1, :])     # out: (batch_size, output_size)\n\n        return out\n\n# === Test configuration ===\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "35_LSTM", "problem_id": 35}, "task_id": "3-35"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x,h0,c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        \n        # Forward propagate LSTM\n        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return state[0]\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x,h0,c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        \n        # Forward propagate LSTM\n        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return state[0]\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x,h0,c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        \n        # Forward propagate LSTM\n        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return state[0]\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "36_LSTMHn", "problem_id": 36}, "task_id": "3-36"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, h0, c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        \n        # Forward propagate LSTM\n        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return state[1]\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, h0, c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        \n        # Forward propagate LSTM\n        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return state[1]\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, h0, c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        \n        # Forward propagate LSTM\n        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return state[1]\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "37_LSTMCn", "problem_id": 37}, "task_id": "3-37"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n    \n    def forward(self, x,h0,c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        # Forward propagate LSTM\n        out, hn = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return out\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers*2, batch_size, hidden_size)),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n    \n    def forward(self, x,h0,c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        # Forward propagate LSTM\n        out, hn = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return out\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers*2, batch_size, hidden_size)),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize hidden state with random values\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n    \n    def forward(self, x,h0,c0):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, sequence_length, output_size)\n        \"\"\"\n        # Forward propagate LSTM\n        out, hn = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)\n        \n        return out\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers*2, batch_size, hidden_size)),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "38_LSTMBidirectional", "problem_id": 38}, "task_id": "3-38"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "39_GRU", "problem_id": 39}, "task_id": "3-39"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 1024\ninput_size = 8192\nhidden_layer_sizes = [1024] * 16  # deep network with wider layers\noutput_size = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 1024\ninput_size = 8192\nhidden_layer_sizes = [1024] * 16  # deep network with wider layers\noutput_size = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(Model, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 1024\ninput_size = 8192\nhidden_layer_sizes = [1024] * 16  # deep network with wider layers\noutput_size = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "3_DeepNarrowMLP", "problem_id": 3}, "task_id": "3-3"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return h_n\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return h_n\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return h_n\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "40_GRUHidden", "problem_id": 40}, "task_id": "3-40"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)\n        self.h0 = torch.randn((num_layers * 2, batch_size, hidden_size))\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)\n        self.h0 = torch.randn((num_layers * 2, batch_size, hidden_size))\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)\n        self.h0 = torch.randn((num_layers * 2, batch_size, hidden_size))\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "41_GRUBidirectional", "problem_id": 41}, "task_id": "3-41"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return h_n\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return h_n\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        \"\"\"\n        :param input_size: The number of expected features in the input x\n        :param hidden_size: The number of features in the hidden state h\n        :param num_layers: Number of recurrent layers (default: 1)\n        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)\n        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)\n    \n    def forward(self, x,h0):\n        \"\"\"\n        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)\n        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)\n        :return: output, h_n\n            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)\n            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)\n        \"\"\"\n        output, h_n = self.gru(x, h0)\n        return h_n\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "42_GRUBidirectionalHidden", "problem_id": 42}, "task_id": "3-42"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "43_MinGPTCausalAttention", "problem_id": 43}, "task_id": "3-43"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass Model(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass Model(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass Model(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "44_MiniGPTBlock", "problem_id": 44}, "task_id": "3-44"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\n# U-Net Implementation\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, features):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param features: Number of base features (will be doubled in each layer)\n        \"\"\"\n        super(Model, self).__init__()\n        self.encoder1 = DoubleConv(in_channels, features)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = DoubleConv(features, features * 2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = DoubleConv(features * 2, features * 4)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = DoubleConv(features * 4, features * 8)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = DoubleConv(features * 8, features * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(features * 16, features * 8)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(features * 8, features * 4)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(features * 4, features * 2)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(features * 2, features)\n\n        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        return self.final_conv(dec1)\n    \nbatch_size = 8\nin_channels = 8\nout_channels = 4\nheight = 64\nwidth = 512\nfeatures = 64\n# Test code for UNet\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, features]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\n# U-Net Implementation\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, features):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param features: Number of base features (will be doubled in each layer)\n        \"\"\"\n        super(Model, self).__init__()\n        self.encoder1 = DoubleConv(in_channels, features)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = DoubleConv(features, features * 2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = DoubleConv(features * 2, features * 4)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = DoubleConv(features * 4, features * 8)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = DoubleConv(features * 8, features * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(features * 16, features * 8)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(features * 8, features * 4)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(features * 4, features * 2)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(features * 2, features)\n\n        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        return self.final_conv(dec1)\n    \nbatch_size = 8\nin_channels = 8\nout_channels = 4\nheight = 64\nwidth = 512\nfeatures = 64\n# Test code for UNet\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, features]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\n# U-Net Implementation\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_channels, features):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param features: Number of base features (will be doubled in each layer)\n        \"\"\"\n        super(Model, self).__init__()\n        self.encoder1 = DoubleConv(in_channels, features)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = DoubleConv(features, features * 2)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = DoubleConv(features * 2, features * 4)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = DoubleConv(features * 4, features * 8)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = DoubleConv(features * 8, features * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(features * 16, features * 8)\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(features * 8, features * 4)\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(features * 4, features * 2)\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(features * 2, features)\n\n        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        return self.final_conv(dec1)\n    \nbatch_size = 8\nin_channels = 8\nout_channels = 4\nheight = 64\nwidth = 512\nfeatures = 64\n# Test code for UNet\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, features]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "45_UNetSoftmax", "problem_id": 45}, "task_id": "3-45"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "46_NetVladWithGhostClusters", "problem_id": 46}, "task_id": "3-46"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 0\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 0\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\n\n\nclass Model(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(Model, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"\n        max_sample = x.size()[1]\n        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x.device != self.clusters.device:\n            msg = f\"x.device {x.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)\n        # remove ghost assigments\n        assignment = assignment[:, :self.cluster_size]\n        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K\n        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N\n\n        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad)\n        return vlad  # B x DK\n\nbatch_size = 2048\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 0\n\ndef get_inputs():\n  return [torch.rand(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "47_NetVladNoGhostClusters", "problem_id": 47}, "task_id": "3-47"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        states = new_states[:, :-1]\n        \n        # 4. Compute state-to-output conversion\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', \n                           C_blocks, states, state_decay_out)\n        \n        # Combine diagonal and off-diagonal terms\n        Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n        \n        \n        return Y\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        states = new_states[:, :-1]\n        \n        # 4. Compute state-to-output conversion\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', \n                           C_blocks, states, state_decay_out)\n        \n        # Combine diagonal and off-diagonal terms\n        Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n        \n        \n        return Y\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        states = new_states[:, :-1]\n        \n        # 4. Compute state-to-output conversion\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', \n                           C_blocks, states, state_decay_out)\n        \n        # Combine diagonal and off-diagonal terms\n        Y = rearrange(Y_diag + Y_off, \"b c l h p -> b (c l) h p\")\n        \n        \n        return Y\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "48_Mamba2ReturnY", "problem_id": 48}, "task_id": "3-48"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        return new_states[:, -1]\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        return new_states[:, -1]\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nclass Model(nn.Module):\n    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):\n        \"\"\"\n        Mamba Structured State Space model implementation for benchmarking.\n        \n        :param batch_size: Size of the batch\n        :param seq_length: Length of the input sequence\n        :param n_heads: Number of attention heads\n        :param d_head: Dimension of each head\n        :param d_state: Dimension of the state space\n        :param block_len: Length of each block for chunked computation\n        \"\"\"\n        super(Model, self).__init__()\n        \n        assert seq_length % block_len == 0, \"Sequence length must be divisible by block length\"\n        \n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.n_heads = n_heads\n        self.d_head = d_head\n        self.d_state = d_state\n        self.block_len = block_len\n        \n        # Initialize parameters\n        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))\n        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))\n        \n    def segsum(self, x):\n        \"\"\"Naive segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x_cumsum = torch.cumsum(x, dim=-1)\n        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n    \n    def forward(self, X, initial_states=None):\n        \"\"\"\n        Forward pass implementing the SSD operation.\n        \n        :param X: Input tensor of shape (batch, length, n_heads, d_head)\n        :param initial_states: Optional initial states\n        :return: Output tensor Y and final state\n        \"\"\"\n        # Rearrange into blocks/chunks\n        X_blocks, A_blocks, B_blocks, C_blocks = [\n            rearrange(x, \"b (c l) ... -> b c l ...\", l=self.block_len)\n            for x in (X, self.A, self.B, self.C)\n        ]\n        \n        A_blocks = rearrange(A_blocks, \"b c l h -> b h c l\")\n        A_cumsum = torch.cumsum(A_blocks, dim=-1)\n        \n        # 1. Compute diagonal block outputs\n        L = torch.exp(self.segsum(A_blocks))\n        Y_diag = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", \n                             C_blocks, B_blocks, L, X_blocks)\n        \n        # 2. Compute intra-chunk states\n        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n        states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", \n                            B_blocks, decay_states, X_blocks)\n        \n        # 3. Compute inter-chunk recurrence\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        \n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n        new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n        return new_states[:, -1]\n\n# Test parameters\nbatch_size = 2048\nseq_length = 128\nn_heads = 8\nd_head = 64\nd_state = 16\nblock_len = 64\n\ndef get_inputs():\n    return [torch.rand(batch_size, seq_length, n_heads, d_head)]\n\ndef get_init_inputs():\n    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "49_Mamba2ReturnFinalState", "problem_id": 49}, "task_id": "3-49"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model (larger batch & image)\nbatch_size = 4096\nnum_classes = 20\n\ndef get_inputs():\n    return [torch.rand(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model (larger batch & image)\nbatch_size = 4096\nnum_classes = 20\n\ndef get_inputs():\n    return [torch.rand(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model (larger batch & image)\nbatch_size = 4096\nnum_classes = 20\n\ndef get_inputs():\n    return [torch.rand(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "4_LeNet5", "problem_id": 4}, "task_id": "3-4"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass Model(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.relu(att)\n\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        return y\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.rand(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass Model(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.relu(att)\n\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        return y\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.rand(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass Model(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.relu(att)\n\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        return y\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.rand(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "50_ReLUSelfAttention", "problem_id": 50}, "task_id": "3-50"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # First convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Second convolutional layer\n        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Third convolutional layer\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU(inplace=True)\n        \n        # Fourth convolutional layer\n        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU(inplace=True)\n        \n        # Fifth convolutional layer\n        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU(inplace=True)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.dropout1 = nn.Dropout(p=0.0)\n        \n        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.dropout2 = nn.Dropout(p=0.0)\n        \n        self.fc3 = nn.Linear(in_features=4096, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n        \n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.maxpool2(x)\n        \n        x = self.conv3(x)\n        x = self.relu3(x)\n        \n        x = self.conv4(x)\n        x = self.relu4(x)\n        \n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = self.maxpool3(x)\n        \n        x = torch.flatten(x, 1)\n        \n        x = self.fc1(x)\n        x = self.relu6(x)\n        x = self.dropout1(x)\n        \n        x = self.fc2(x)\n        x = self.relu7(x)\n        x = self.dropout2(x)\n        \n        x = self.fc3(x)\n        \n        return x\n\n# Test code\nbatch_size = 1024\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # First convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Second convolutional layer\n        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Third convolutional layer\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU(inplace=True)\n        \n        # Fourth convolutional layer\n        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU(inplace=True)\n        \n        # Fifth convolutional layer\n        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU(inplace=True)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.dropout1 = nn.Dropout(p=0.0)\n        \n        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.dropout2 = nn.Dropout(p=0.0)\n        \n        self.fc3 = nn.Linear(in_features=4096, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n        \n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.maxpool2(x)\n        \n        x = self.conv3(x)\n        x = self.relu3(x)\n        \n        x = self.conv4(x)\n        x = self.relu4(x)\n        \n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = self.maxpool3(x)\n        \n        x = torch.flatten(x, 1)\n        \n        x = self.fc1(x)\n        x = self.relu6(x)\n        x = self.dropout1(x)\n        \n        x = self.fc2(x)\n        x = self.relu7(x)\n        x = self.dropout2(x)\n        \n        x = self.fc3(x)\n        \n        return x\n\n# Test code\nbatch_size = 1024\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: The number of output classes (default is 1000 for ImageNet)\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # First convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Second convolutional layer\n        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Third convolutional layer\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU(inplace=True)\n        \n        # Fourth convolutional layer\n        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU(inplace=True)\n        \n        # Fifth convolutional layer\n        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU(inplace=True)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.dropout1 = nn.Dropout(p=0.0)\n        \n        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.dropout2 = nn.Dropout(p=0.0)\n        \n        self.fc3 = nn.Linear(in_features=4096, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n        \n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.maxpool2(x)\n        \n        x = self.conv3(x)\n        x = self.relu3(x)\n        \n        x = self.conv4(x)\n        x = self.relu4(x)\n        \n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = self.maxpool3(x)\n        \n        x = torch.flatten(x, 1)\n        \n        x = self.fc1(x)\n        x = self.relu6(x)\n        x = self.dropout1(x)\n        \n        x = self.fc2(x)\n        x = self.relu7(x)\n        x = self.dropout2(x)\n        \n        x = self.fc3(x)\n        \n        return x\n\n# Test code\nbatch_size = 1024\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "5_AlexNet", "problem_id": 5}, "task_id": "3-5"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_1x1: Number of output channels for the 1x1 convolution\n        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution\n        :param out_3x3: Number of output channels for the 3x3 convolution\n        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution\n        :param out_5x5: Number of output channels for the 5x5 convolution\n        :param pool_proj: Number of output channels for the pooling projection\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # 1x1 convolution branch\n        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n        \n        # 3x3 convolution branch\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)\n        )\n        \n        # 5x5 convolution branch\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)\n        )\n        \n        # Max pooling branch\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        \n        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n        return torch.cat(outputs, 1)\n\n# Test code\nin_channels = 480\nout_1x1 = 192\nreduce_3x3 = 96\nout_3x3 = 208\nreduce_5x5 = 16\nout_5x5 = 48\npool_proj = 64\nbatch_size = 10\nheight = 224\nwidth = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_1x1: Number of output channels for the 1x1 convolution\n        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution\n        :param out_3x3: Number of output channels for the 3x3 convolution\n        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution\n        :param out_5x5: Number of output channels for the 5x5 convolution\n        :param pool_proj: Number of output channels for the pooling projection\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # 1x1 convolution branch\n        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n        \n        # 3x3 convolution branch\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)\n        )\n        \n        # 5x5 convolution branch\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)\n        )\n        \n        # Max pooling branch\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        \n        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n        return torch.cat(outputs, 1)\n\n# Test code\nin_channels = 480\nout_1x1 = 192\nreduce_3x3 = 96\nout_3x3 = 208\nreduce_5x5 = 16\nout_5x5 = 48\npool_proj = 64\nbatch_size = 10\nheight = 224\nwidth = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_1x1: Number of output channels for the 1x1 convolution\n        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution\n        :param out_3x3: Number of output channels for the 3x3 convolution\n        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution\n        :param out_5x5: Number of output channels for the 5x5 convolution\n        :param pool_proj: Number of output channels for the pooling projection\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # 1x1 convolution branch\n        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n        \n        # 3x3 convolution branch\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)\n        )\n        \n        # 5x5 convolution branch\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)\n        )\n        \n        # Max pooling branch\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        \n        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n        return torch.cat(outputs, 1)\n\n# Test code\nin_channels = 480\nout_1x1 = 192\nreduce_3x3 = 96\nout_3x3 = 208\nreduce_5x5 = 16\nout_5x5 = 48\npool_proj = 64\nbatch_size = 10\nheight = 224\nwidth = 224\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "6_GoogleNetInceptionModule", "problem_id": 6}, "task_id": "3-6"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_1x1: Number of output channels for the 1x1 convolution\n        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution\n        :param out_3x3: Number of output channels for the 3x3 convolution\n        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution\n        :param out_5x5: Number of output channels for the 5x5 convolution\n        :param pool_proj: Number of output channels for the pooling projection\n        \"\"\"\n        super(InceptionModule, self).__init__()\n        \n        # 1x1 convolution branch\n        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n        \n        # 3x3 convolution branch\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)\n        )\n        \n        # 5x5 convolution branch\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)\n        )\n        \n        # Max pooling branch\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        \n        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.0)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.maxpool1(F.relu(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = self.maxpool2(F.relu(self.conv3(x)))\n        \n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.maxpool3(x)\n        \n        x = self.inception4a(x)\n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n        x = self.inception4e(x)\n        x = self.maxpool4(x)\n        \n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_1x1: Number of output channels for the 1x1 convolution\n        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution\n        :param out_3x3: Number of output channels for the 3x3 convolution\n        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution\n        :param out_5x5: Number of output channels for the 5x5 convolution\n        :param pool_proj: Number of output channels for the pooling projection\n        \"\"\"\n        super(InceptionModule, self).__init__()\n        \n        # 1x1 convolution branch\n        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n        \n        # 3x3 convolution branch\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)\n        )\n        \n        # 5x5 convolution branch\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)\n        )\n        \n        # Max pooling branch\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        \n        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.0)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.maxpool1(F.relu(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = self.maxpool2(F.relu(self.conv3(x)))\n        \n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.maxpool3(x)\n        \n        x = self.inception4a(x)\n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n        x = self.inception4e(x)\n        x = self.maxpool4(x)\n        \n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_1x1: Number of output channels for the 1x1 convolution\n        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution\n        :param out_3x3: Number of output channels for the 3x3 convolution\n        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution\n        :param out_5x5: Number of output channels for the 5x5 convolution\n        :param pool_proj: Number of output channels for the pooling projection\n        \"\"\"\n        super(InceptionModule, self).__init__()\n        \n        # 1x1 convolution branch\n        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n        \n        # 3x3 convolution branch\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)\n        )\n        \n        # 5x5 convolution branch\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)\n        )\n        \n        # Max pooling branch\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        \n        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)\n        \n        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.0)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.maxpool1(F.relu(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = self.maxpool2(F.relu(self.conv3(x)))\n        \n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.maxpool3(x)\n        \n        x = self.inception4a(x)\n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n        x = self.inception4e(x)\n        x = self.maxpool4(x)\n        \n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "7_GoogleNetInceptionV1", "problem_id": 7}, "task_id": "3-7"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(out_channels * self.expansion),\n        )\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n    \n# Test code\nin_channels = 3\nout_channels = 64\nstride = 1\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, stride]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(out_channels * self.expansion),\n        )\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n    \n# Test code\nin_channels = 3\nout_channels = 64\nstride = 1\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, stride]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(out_channels * self.expansion),\n        )\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n    \n# Test code\nin_channels = 3\nout_channels = 64\nstride = 1\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, stride]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "8_ResNetBasicBlock", "problem_id": 8}, "task_id": "3-8"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.rand(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_3", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.rand(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "3", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(Model, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.rand(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "9_ResNet18", "problem_id": 9}, "task_id": "3-9"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "10_google-bigbird-roberta-base_bs1024_seq32", "problem_id": 10}, "task_id": "4-10"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 511\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 511\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 511\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "11_google-electra-small-discriminator_bs1_seq511", "problem_id": 11}, "task_id": "4-11"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "12_google-electra-small-discriminator_bs1024_seq32", "problem_id": 12}, "task_id": "4-12"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "13_google-reformer-enwik8_bs32_seq256", "problem_id": 13}, "task_id": "4-13"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/electra-small-discriminator\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "14_google-electra-small-discriminator_bs32_seq256", "problem_id": 14}, "task_id": "4-14"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/reformer-enwik8\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "15_google-reformer-enwik8_bs1024_seq32", "problem_id": 15}, "task_id": "4-15"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "16_gpt2_bs1_seq1023", "problem_id": 16}, "task_id": "4-16"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "17_facebook-bart-large_bs1024_seq32", "problem_id": 17}, "task_id": "4-17"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "18_EleutherAI-gpt-neo-2p7B_bs512_seq32", "problem_id": 18}, "task_id": "4-18"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 1024\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "19_gpt2_bs1024_seq32", "problem_id": 19}, "task_id": "4-19"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "1_EleutherAI-gpt-neo-2p7B_bs32_seq256", "problem_id": 1}, "task_id": "4-1"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "20_facebook-bart-large_bs32_seq256", "problem_id": 20}, "task_id": "4-20"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "2_facebook-opt-1p3b_bs1_seq2047", "problem_id": 2}, "task_id": "4-2"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 2047\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "3_EleutherAI-gpt-neo-2p7B_bs1_seq2047", "problem_id": 3}, "task_id": "4-3"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "4_facebook-opt-1p3b_bs32_seq256", "problem_id": 4}, "task_id": "4-4"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 4095\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 4095\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 4095\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "5_google-bigbird-roberta-base_bs1_seq4095", "problem_id": 5}, "task_id": "4-5"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/bart-large\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 1023\nbatch_size = 1\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "6_facebook-bart-large_bs1_seq1023", "problem_id": 6}, "task_id": "4-6"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"gpt2\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "7_gpt2_bs32_seq256", "problem_id": 7}, "task_id": "4-7"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"facebook/opt-1.3b\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 32\nbatch_size = 512\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "8_facebook-opt-1p3b_bs512_seq32", "problem_id": 8}, "task_id": "4-8"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_4", "ability": "code", "answer": "", "raw_problem": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "level": "4", "type": "", "ground_truth": {"pytorch_module": "\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoConfig\n\nclass Model(torch.nn.Module):\n    def __init__(self, model_name, config):\n        super().__init__()\n        self.model_name = model_name\n        self.config = config\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)\n\n    def forward(self, x):\n        return self.model(x).logits\n\nmodel_name = \"google/bigbird-roberta-base\"\nconfig = AutoConfig.from_pretrained(model_name)\nvocab_size = config.vocab_size\nsequence_length = 256\nbatch_size = 32\n\ndef get_inputs():\n    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))\n    return [inputs]\n\ndef get_init_inputs():\n    return [model_name, config]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "9_google-bigbird-roberta-base_bs32_seq256", "problem_id": 9}, "task_id": "4-9"}
