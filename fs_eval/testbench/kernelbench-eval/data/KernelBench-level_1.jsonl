{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Hinge Loss for binary classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Hinge Loss for binary classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Hinge Loss for binary classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "100_HingeLoss", "problem_id": 100}, "task_id": "1-100"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.rand(N, M, K)\n    B = torch.rand(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.rand(N, M, K)\n    B = torch.rand(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.rand(N, M, K)\n    B = torch.rand(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "10_3D_tensor_matrix_multiplication", "problem_id": 10}, "task_id": "1-10"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.rand(b, i, j, l)\n    B = torch.rand(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.rand(b, i, j, l)\n    B = torch.rand(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.rand(b, i, j, l)\n    B = torch.rand(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "11_4D_tensor_matrix_multiplication", "problem_id": 11}, "task_id": "1-11"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).\n            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).\n\n        Returns:\n            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).\n        \"\"\"\n        return torch.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.rand(N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).\n            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).\n\n        Returns:\n            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).\n        \"\"\"\n        return torch.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.rand(N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).\n            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).\n\n        Returns:\n            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).\n        \"\"\"\n        return torch.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.rand(N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "12_Matmul_with_diagonal_matrices_", "problem_id": 12}, "task_id": "1-12"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of two symmetric matrices.\n\n        Args:\n            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.\n            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.\n\n        Returns:\n            torch.Tensor: Output matrix C, shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    A = torch.rand(N, N)\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = torch.rand(N, N)\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of two symmetric matrices.\n\n        Args:\n            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.\n            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.\n\n        Returns:\n            torch.Tensor: Output matrix C, shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    A = torch.rand(N, N)\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = torch.rand(N, N)\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of two symmetric matrices.\n\n        Args:\n            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.\n            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.\n\n        Returns:\n            torch.Tensor: Output matrix C, shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    A = torch.rand(N, N)\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = torch.rand(N, N)\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "13_Matmul_for_symmetric_matrices", "problem_id": 13}, "task_id": "1-13"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication for upper triangular matrices.\n\n        Args:\n            A (torch.Tensor): Upper triangular matrix of shape (N, N).\n            B (torch.Tensor): Upper triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).\n        \"\"\"\n        return torch.triu(torch.matmul(A, B))\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    A = torch.triu(torch.rand(N, N))\n    B = torch.triu(torch.rand(N, N))\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication for upper triangular matrices.\n\n        Args:\n            A (torch.Tensor): Upper triangular matrix of shape (N, N).\n            B (torch.Tensor): Upper triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).\n        \"\"\"\n        return torch.triu(torch.matmul(A, B))\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    A = torch.triu(torch.rand(N, N))\n    B = torch.triu(torch.rand(N, N))\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication for upper triangular matrices.\n\n        Args:\n            A (torch.Tensor): Upper triangular matrix of shape (N, N).\n            B (torch.Tensor): Upper triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).\n        \"\"\"\n        return torch.triu(torch.matmul(A, B))\n\nN = 4096\n\ndef get_inputs():\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    A = torch.triu(torch.rand(N, N))\n    B = torch.triu(torch.rand(N, N))\n    return [A, B]\n\ndef get_init_inputs():\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "14_Matmul_for_upper_triangular_matrices", "problem_id": 14}, "task_id": "1-14"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of lower triangular matrices A and B.\n\n        Args:\n            A (torch.Tensor): Lower triangular matrix of shape (N, N).\n            B (torch.Tensor): Lower triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The result of matrix multiplication C of shape (N, N).\n        \"\"\"\n        return torch.tril(torch.matmul(A, B))\n\nM = 4096\n\ndef get_inputs():\n    A = torch.rand(M, M)\n    B = torch.rand(M, M)\n    A = torch.tril(A)\n    B = torch.tril(B)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of lower triangular matrices A and B.\n\n        Args:\n            A (torch.Tensor): Lower triangular matrix of shape (N, N).\n            B (torch.Tensor): Lower triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The result of matrix multiplication C of shape (N, N).\n        \"\"\"\n        return torch.tril(torch.matmul(A, B))\n\nM = 4096\n\ndef get_inputs():\n    A = torch.rand(M, M)\n    B = torch.rand(M, M)\n    A = torch.tril(A)\n    B = torch.tril(B)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs matrix multiplication of lower triangular matrices A and B.\n\n        Args:\n            A (torch.Tensor): Lower triangular matrix of shape (N, N).\n            B (torch.Tensor): Lower triangular matrix of shape (N, N).\n\n        Returns:\n            torch.Tensor: The result of matrix multiplication C of shape (N, N).\n        \"\"\"\n        return torch.tril(torch.matmul(A, B))\n\nM = 4096\n\ndef get_inputs():\n    A = torch.rand(M, M)\n    B = torch.rand(M, M)\n    A = torch.tril(A)\n    B = torch.tril(B)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "15_Matmul_for_lower_triangular_matrices", "problem_id": 15}, "task_id": "1-15"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "16_Matmul_with_transposed_A", "problem_id": 16}, "task_id": "1-16"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "17_Matmul_with_transposed_B", "problem_id": 17}, "task_id": "1-17"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(K, M)\n    B = torch.rand(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "18_Matmul_with_transposed_both", "problem_id": 18}, "task_id": "1-18"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n        \"\"\"\n        return torch.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n        \"\"\"\n        return torch.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ReLU applied, same shape as input.\n        \"\"\"\n        return torch.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "19_ReLU", "problem_id": 19}, "task_id": "1-19"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(N, N)\n    B = torch.rand(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(N, N)\n    B = torch.rand(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(N, N)\n    B = torch.rand(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "1_Square_matrix_multiplication_", "problem_id": 1}, "task_id": "1-1"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    def __init__(self, negative_slope: float = 0.01):\n        \"\"\"\n        Initializes the LeakyReLU module.\n\n        Args:\n            negative_slope (float, optional): The negative slope of the activation function. Defaults to 0.01.\n        \"\"\"\n        super(Model, self).__init__()\n        self.negative_slope = negative_slope\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LeakyReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with LeakyReLU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.leaky_relu(x, negative_slope=self.negative_slope)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    def __init__(self, negative_slope: float = 0.01):\n        \"\"\"\n        Initializes the LeakyReLU module.\n\n        Args:\n            negative_slope (float, optional): The negative slope of the activation function. Defaults to 0.01.\n        \"\"\"\n        super(Model, self).__init__()\n        self.negative_slope = negative_slope\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LeakyReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with LeakyReLU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.leaky_relu(x, negative_slope=self.negative_slope)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    def __init__(self, negative_slope: float = 0.01):\n        \"\"\"\n        Initializes the LeakyReLU module.\n\n        Args:\n            negative_slope (float, optional): The negative slope of the activation function. Defaults to 0.01.\n        \"\"\"\n        super(Model, self).__init__()\n        self.negative_slope = negative_slope\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LeakyReLU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with LeakyReLU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.leaky_relu(x, negative_slope=self.negative_slope)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "20_LeakyReLU", "problem_id": 20}, "task_id": "1-20"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Sigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.\n        \"\"\"\n        return torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Sigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.\n        \"\"\"\n        return torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Sigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.\n        \"\"\"\n        return torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "21_Sigmoid", "problem_id": 21}, "task_id": "1-21"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Tanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Tanh applied, same shape as input.\n        \"\"\"\n        return torch.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Tanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Tanh applied, same shape as input.\n        \"\"\"\n        return torch.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Tanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Tanh applied, same shape as input.\n        \"\"\"\n        return torch.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "22_Tanh", "problem_id": 22}, "task_id": "1-22"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Output tensor with Softmax applied, same shape as input.\n        \"\"\"\n        return torch.softmax(x, dim=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Output tensor with Softmax applied, same shape as input.\n        \"\"\"\n        return torch.softmax(x, dim=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n\n        Returns:\n            torch.Tensor: Output tensor with Softmax applied, same shape as input.\n        \"\"\"\n        return torch.softmax(x, dim=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "23_Softmax", "problem_id": 23}, "task_id": "1-23"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(Model, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LogSoftmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, dim).\n\n        Returns:\n            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.\n        \"\"\"\n        return torch.log_softmax(x, dim=self.dim)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(Model, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LogSoftmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, dim).\n\n        Returns:\n            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.\n        \"\"\"\n        return torch.log_softmax(x, dim=self.dim)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    def __init__(self, dim: int = 1):\n        super(Model, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies LogSoftmax activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, dim).\n\n        Returns:\n            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.\n        \"\"\"\n        return torch.log_softmax(x, dim=self.dim)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "24_LogSoftmax", "problem_id": 24}, "task_id": "1-24"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Swish activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Swish applied, same shape as input.\n        \"\"\"\n        return x * torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Swish activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Swish applied, same shape as input.\n        \"\"\"\n        return x * torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Swish activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Swish applied, same shape as input.\n        \"\"\"\n        return x * torch.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "25_Swish", "problem_id": 25}, "task_id": "1-25"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies GELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with GELU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies GELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with GELU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies GELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with GELU applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "26_GELU_", "problem_id": 26}, "task_id": "1-26"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n    \nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n    \nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n    \nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "27_SELU_", "problem_id": 27}, "task_id": "1-27"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardSigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardSigmoid applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.hardsigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardSigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardSigmoid applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.hardsigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardSigmoid activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardSigmoid applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.hardsigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "28_HardSigmoid", "problem_id": 28}, "task_id": "1-28"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softplus activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softplus applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softplus activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softplus applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softplus activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softplus applied, same shape as input.\n        \"\"\"\n        return torch.nn.functional.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "29_Softplus", "problem_id": 29}, "task_id": "1-29"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "2_Standard_matrix_multiplication_", "problem_id": 2}, "task_id": "1-2"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softsign activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softsign applied, same shape as input.\n        \"\"\"\n        return x / (1 + torch.abs(x))\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softsign activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softsign applied, same shape as input.\n        \"\"\"\n        return x / (1 + torch.abs(x))\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Softsign activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Softsign applied, same shape as input.\n        \"\"\"\n        return x / (1 + torch.abs(x))\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "30_Softsign", "problem_id": 30}, "task_id": "1-30"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    def __init__(self, alpha: float = 1.0):\n        \"\"\"\n        Initializes the ELU model.\n\n        Args:\n            alpha (float, optional): The alpha parameter for the ELU function. Defaults to 1.0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ELU applied, same shape as input.\n        \"\"\"\n        return F.elu(x, alpha=self.alpha)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return [1.0]  # Provide alpha value for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    def __init__(self, alpha: float = 1.0):\n        \"\"\"\n        Initializes the ELU model.\n\n        Args:\n            alpha (float, optional): The alpha parameter for the ELU function. Defaults to 1.0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ELU applied, same shape as input.\n        \"\"\"\n        return F.elu(x, alpha=self.alpha)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return [1.0]  # Provide alpha value for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    def __init__(self, alpha: float = 1.0):\n        \"\"\"\n        Initializes the ELU model.\n\n        Args:\n            alpha (float, optional): The alpha parameter for the ELU function. Defaults to 1.0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies ELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with ELU applied, same shape as input.\n        \"\"\"\n        return F.elu(x, alpha=self.alpha)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return [1.0]  # Provide alpha value for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "31_ELU", "problem_id": 31}, "task_id": "1-31"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardTanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardTanh applied, same shape as input.\n        \"\"\"\n        return F.hardtanh(x, min_val=-1., max_val=1.)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardTanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardTanh applied, same shape as input.\n        \"\"\"\n        return F.hardtanh(x, min_val=-1., max_val=1.)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies HardTanh activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with HardTanh applied, same shape as input.\n        \"\"\"\n        return F.hardtanh(x, min_val=-1., max_val=1.)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "32_HardTanh", "problem_id": 32}, "task_id": "1-32"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the BatchNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return self.bn(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the BatchNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return self.bn(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the BatchNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return self.bn(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "33_BatchNorm", "problem_id": 33}, "task_id": "1-33"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return self.inorm(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return self.inorm(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    def __init__(self, num_features: int):\n        \"\"\"\n        Initializes the InstanceNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n        \"\"\"\n        super(Model, self).__init__()\n        self.inorm = nn.InstanceNorm2d(num_features=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return self.inorm(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "34_InstanceNorm", "problem_id": 34}, "task_id": "1-34"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, num_groups: int):\n        \"\"\"\n        Initializes the GroupNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            num_groups (int): Number of groups to divide the channels into.\n        \"\"\"\n        super(Model, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return self.gn(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\nnum_groups = 8\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features, num_groups] # num_features```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, num_groups: int):\n        \"\"\"\n        Initializes the GroupNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            num_groups (int): Number of groups to divide the channels into.\n        \"\"\"\n        super(Model, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return self.gn(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\nnum_groups = 8\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features, num_groups] # num_features", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, num_groups: int):\n        \"\"\"\n        Initializes the GroupNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            num_groups (int): Number of groups to divide the channels into.\n        \"\"\"\n        super(Model, self).__init__()\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return self.gn(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\nnum_groups = 8\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features, num_groups] # num_features", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "35_GroupNorm_", "problem_id": 35}, "task_id": "1-35"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, eps: float = 1e-5):\n        \"\"\"\n        Initializes the RMSNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        \"\"\"\n        super(Model, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # Calculate the RMS along the feature dimension\n        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n\n        # Normalize the input by dividing by the RMS\n        return x / rms\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, eps: float = 1e-5):\n        \"\"\"\n        Initializes the RMSNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        \"\"\"\n        super(Model, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # Calculate the RMS along the feature dimension\n        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n\n        # Normalize the input by dividing by the RMS\n        return x / rms\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    def __init__(self, num_features: int, eps: float = 1e-5):\n        \"\"\"\n        Initializes the RMSNorm layer.\n\n        Args:\n            num_features (int): Number of features in the input tensor.\n            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        \"\"\"\n        super(Model, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).\n\n        Returns:\n            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # Calculate the RMS along the feature dimension\n        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n\n        # Normalize the input by dividing by the RMS\n        return x / rms\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [features]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "36_RMSNorm_", "problem_id": 36}, "task_id": "1-36"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Frobenius norm normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Frobenius norm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.\n        \"\"\"\n        norm = torch.norm(x, p='fro')\n        return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Frobenius norm normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Frobenius norm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.\n        \"\"\"\n        norm = torch.norm(x, p='fro')\n        return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Frobenius norm normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Frobenius norm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.\n        \"\"\"\n        norm = torch.norm(x, p='fro')\n        return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "37_FrobeniusNorm_", "problem_id": 37}, "task_id": "1-37"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "38_L1Norm_", "problem_id": 38}, "task_id": "1-38"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(Model, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "39_L2Norm_", "problem_id": 39}, "task_id": "1-39"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k)\n    B = torch.rand(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k)\n    B = torch.rand(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k)\n    B = torch.rand(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "3_Batched_matrix_multiplication", "problem_id": 3}, "task_id": "1-3"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    def __init__(self, normalized_shape: tuple):\n        \"\"\"\n        Initializes the LayerNorm layer.\n\n        Args:\n            normalized_shape (tuple): Shape of the input tensor to be normalized.\n        \"\"\"\n        super(Model, self).__init__()\n        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, normalized_shape).\n\n        Returns:\n            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        return self.ln(x)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [(features, dim1, dim2)]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    def __init__(self, normalized_shape: tuple):\n        \"\"\"\n        Initializes the LayerNorm layer.\n\n        Args:\n            normalized_shape (tuple): Shape of the input tensor to be normalized.\n        \"\"\"\n        super(Model, self).__init__()\n        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, normalized_shape).\n\n        Returns:\n            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        return self.ln(x)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [(features, dim1, dim2)]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    def __init__(self, normalized_shape: tuple):\n        \"\"\"\n        Initializes the LayerNorm layer.\n\n        Args:\n            normalized_shape (tuple): Shape of the input tensor to be normalized.\n        \"\"\"\n        super(Model, self).__init__()\n        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, normalized_shape).\n\n        Returns:\n            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        return self.ln(x)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [(features, dim1, dim2)]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "40_LayerNorm", "problem_id": 40}, "task_id": "1-40"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 1D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 1D layer.\n\n        Args:\n            kernel_size (int): Size of the window to take a max over.\n            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).\n            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 1D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3            \n\nreturn_indices = False\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation, return_indices]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 1D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 1D layer.\n\n        Args:\n            kernel_size (int): Size of the window to take a max over.\n            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).\n            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 1D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3            \n\nreturn_indices = False\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation, return_indices]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 1D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 1D layer.\n\n        Args:\n            kernel_size (int): Size of the window to take a max over.\n            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).\n            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 1D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3            \n\nreturn_indices = False\n\ndef get_inputs():\n    x = torch.rand(batch_size, features, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation, return_indices]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "41_Max_Pooling_1D", "problem_id": 41}, "task_id": "1-41"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 2D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):\n        \"\"\"\n        Initializes the Max Pooling 2D layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int): Stride of the pooling window.\n            padding (int): Padding to be applied before pooling.\n            dilation (int): Spacing between kernel elements.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 2D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 2D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):\n        \"\"\"\n        Initializes the Max Pooling 2D layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int): Stride of the pooling window.\n            padding (int): Padding to be applied before pooling.\n            dilation (int): Spacing between kernel elements.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 2D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 2D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):\n        \"\"\"\n        Initializes the Max Pooling 2D layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int): Stride of the pooling window.\n            padding (int): Padding to be applied before pooling.\n            dilation (int): Spacing between kernel elements.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 2D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "42_Max_Pooling_2D", "problem_id": 42}, "task_id": "1-42"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(Model, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "43_Max_Pooling_3D", "problem_id": 43}, "task_id": "1-43"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 1D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n        \"\"\"\n        Initializes the 1D Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to 1.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 1D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).\n\n        Returns:\n            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\nkernel_size = 8\nstride = 1\npadding = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, input_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 1D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n        \"\"\"\n        Initializes the 1D Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to 1.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 1D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).\n\n        Returns:\n            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\nkernel_size = 8\nstride = 1\npadding = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, input_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 1D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n        \"\"\"\n        Initializes the 1D Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to 1.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 1D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).\n\n        Returns:\n            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\nkernel_size = 8\nstride = 1\npadding = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, input_length)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "44_Average_Pooling_1D", "problem_id": 44}, "task_id": "1-44"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\nkernel_size = 11\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\nkernel_size = 11\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\nkernel_size = 11\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "45_Average_Pooling_2D", "problem_id": 45}, "task_id": "1-45"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 3D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the kernel to apply pooling.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which uses the kernel size.\n            padding (int, optional): Padding to apply before pooling. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied, shape depends on kernel_size, stride and padding.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 3D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the kernel to apply pooling.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which uses the kernel size.\n            padding (int, optional): Padding to apply before pooling. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied, shape depends on kernel_size, stride and padding.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs 3D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the kernel to apply pooling.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which uses the kernel size.\n            padding (int, optional): Padding to apply before pooling. Defaults to 0.\n        \"\"\"\n        super(Model, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied, shape depends on kernel_size, stride and padding.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "46_Average_Pooling_3D", "problem_id": 46}, "task_id": "1-46"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): Dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies sum reduction over the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).\n        \"\"\"\n        return torch.sum(x, dim=self.dim, keepdim=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): Dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies sum reduction over the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).\n        \"\"\"\n        return torch.sum(x, dim=self.dim, keepdim=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): Dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies sum reduction over the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).\n        \"\"\"\n        return torch.sum(x, dim=self.dim, keepdim=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "47_Sum_reduction_over_a_dimension", "problem_id": 47}, "task_id": "1-47"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reduces the input tensor along the specified dimension by taking the mean.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.\n        \"\"\"\n        return torch.mean(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reduces the input tensor along the specified dimension by taking the mean.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.\n        \"\"\"\n        return torch.mean(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reduces the input tensor along the specified dimension by taking the mean.\n\n        Args:\n            x (torch.Tensor): Input tensor of arbitrary shape.\n\n        Returns:\n            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.\n        \"\"\"\n        return torch.mean(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "48_Mean_reduction_over_a_dimension", "problem_id": 48}, "task_id": "1-48"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after Max reduction over the specified dimension.\n        \"\"\"\n        return torch.max(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after Max reduction over the specified dimension.\n        \"\"\"\n        return torch.max(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after Max reduction over the specified dimension.\n        \"\"\"\n        return torch.max(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "49_Max_reduction_over_a_dimension", "problem_id": 49}, "task_id": "1-49"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "4_Matrix_vector_multiplication_", "problem_id": 4}, "task_id": "1-4"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n# Test code\nbatch_size = 256\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n# Test code\nbatch_size = 256\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n# Test code\nbatch_size = 256\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.rand(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "50_conv_standard_2D__square_input__square_kernel", "problem_id": 50}, "task_id": "1-50"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmax.\n\n        Args:\n            dim (int): The dimension to perform argmax over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies argmax over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.\n        \"\"\"\n        return torch.argmax(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmax.\n\n        Args:\n            dim (int): The dimension to perform argmax over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies argmax over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.\n        \"\"\"\n        return torch.argmax(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmax.\n\n        Args:\n            dim (int): The dimension to perform argmax over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies argmax over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.\n        \"\"\"\n        return torch.argmax(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "51_Argmax_over_a_dimension", "problem_id": 51}, "task_id": "1-51"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmin on.\n\n        Args:\n            dim (int): Dimension along which to find the minimum value.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Finds the index of the minimum value along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.\n        \"\"\"\n        return torch.argmin(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmin on.\n\n        Args:\n            dim (int): Dimension along which to find the minimum value.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Finds the index of the minimum value along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.\n        \"\"\"\n        return torch.argmin(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [dim]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to perform argmin on.\n\n        Args:\n            dim (int): Dimension along which to find the minimum value.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Finds the index of the minimum value along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.\n        \"\"\"\n        return torch.argmin(x, dim=self.dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "52_Argmin_over_a_dimension", "problem_id": 52}, "task_id": "1-52"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies min reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after min reduction over the specified dimension.\n        \"\"\"\n        return torch.min(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies min reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after min reduction over the specified dimension.\n        \"\"\"\n        return torch.min(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    def __init__(self, dim: int):\n        \"\"\"\n        Initializes the model with the dimension to reduce over.\n\n        Args:\n            dim (int): The dimension to reduce over.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies min reduction over the specified dimension to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after min reduction over the specified dimension.\n        \"\"\"\n        return torch.min(x, dim=self.dim)[0]\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs():\n    x = torch.rand(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1] # Example, change to desired dimension", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "53_Min_reduction_over_a_dimension", "problem_id": 53}, "task_id": "1-53"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "54_conv_standard_3D__square_input__square_kernel", "problem_id": 54}, "task_id": "1-54"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\nin_channels = 64  # increased channels\nout_channels = 128\nkernel_size = 3\n# asymmetric input: make width considerably larger than height\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\nin_channels = 64  # increased channels\nout_channels = 128\nkernel_size = 3\n# asymmetric input: make width considerably larger than height\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\nin_channels = 64  # increased channels\nout_channels = 128\nkernel_size = 3\n# asymmetric input: make width considerably larger than height\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "55_conv_standard_2D__asymmetric_input__square_kernel", "problem_id": 55}, "task_id": "1-55"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of two integers representing the height and width of the convolution kernel.\n        stride (tuple, optional): Tuple of two integers representing the stride in the height and width dimensions. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of two integers representing the padding in the height and width dimensions. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of two integers representing the dilation in the height and width dimensions. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nheight = 512\nwidth = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of two integers representing the height and width of the convolution kernel.\n        stride (tuple, optional): Tuple of two integers representing the stride in the height and width dimensions. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of two integers representing the padding in the height and width dimensions. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of two integers representing the dilation in the height and width dimensions. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nheight = 512\nwidth = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of two integers representing the height and width of the convolution kernel.\n        stride (tuple, optional): Tuple of two integers representing the stride in the height and width dimensions. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of two integers representing the padding in the height and width dimensions. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of two integers representing the dilation in the height and width dimensions. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nheight = 512\nwidth = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "56_conv_standard_2D__asymmetric_input__asymmetric_kernel", "problem_id": 56}, "task_id": "1-56"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64  # double channels for heavier compute\nout_channels = 64\nkernel_size = 3\n# larger square input\nheight = 1024\nwidth = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64  # double channels for heavier compute\nout_channels = 64\nkernel_size = 3\n# larger square input\nheight = 1024\nwidth = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64  # double channels for heavier compute\nout_channels = 64\nkernel_size = 3\n# larger square input\nheight = 1024\nwidth = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "57_conv_transposed_2D__square_input__square_kernel", "problem_id": 57}, "task_id": "1-57"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of 3 integers representing the kernel size in the form (depth, height, width).\n        stride (tuple, optional): Tuple of 3 integers representing the stride in the form (depth, height, width). Defaults to (1, 1, 1).\n        padding (tuple, optional): Tuple of 3 integers representing the padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Tuple of 3 integers representing the output padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth_in, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth_in, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of 3 integers representing the kernel size in the form (depth, height, width).\n        stride (tuple, optional): Tuple of 3 integers representing the stride in the form (depth, height, width). Defaults to (1, 1, 1).\n        padding (tuple, optional): Tuple of 3 integers representing the padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Tuple of 3 integers representing the output padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth_in, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth_in, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of 3 integers representing the kernel size in the form (depth, height, width).\n        stride (tuple, optional): Tuple of 3 integers representing the stride in the form (depth, height, width). Defaults to (1, 1, 1).\n        padding (tuple, optional): Tuple of 3 integers representing the padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Tuple of 3 integers representing the output padding in the form (depth, height, width). Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth_in, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth_in, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "58_conv_transposed_3D__asymmetric_input__asymmetric_kernel", "problem_id": 58}, "task_id": "1-58"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel (kernel_size x kernel_size).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel (kernel_size x kernel_size).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with an asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel (kernel_size x kernel_size).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, (kernel_size, kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "59_conv_standard_3D__asymmetric_input__square_kernel", "problem_id": 59}, "task_id": "1-59"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "5_Matrix_scalar_multiplication", "problem_id": 5}, "task_id": "1-5"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_width, kernel_height, kernel_depth).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, width, height, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, width_out, height_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, width, height, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_width, kernel_height, kernel_depth).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, width, height, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, width_out, height_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, width, height, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_width, kernel_height, kernel_depth).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, width, height, depth).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, width_out, height_out, depth_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, width, height, depth)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "60_conv_standard_3D__square_input__asymmetric_kernel", "problem_id": 60}, "task_id": "1-60"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 48\nkernel_size = 3\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 48\nkernel_size = 3\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 48\nkernel_size = 3\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "61_conv_transposed_3D__square_input__square_kernel", "problem_id": 61}, "task_id": "1-61"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "62_conv_standard_2D__square_input__asymmetric_kernel", "problem_id": 62}, "task_id": "1-62"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with a square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "63_conv_standard_2D__square_input__square_kernel", "problem_id": 63}, "task_id": "1-63"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n# much larger signal length for heavier workload\nlength = 65536\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n# much larger signal length for heavier workload\nlength = 65536\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n# much larger signal length for heavier workload\nlength = 65536\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "64_conv_transposed_1D", "problem_id": 64}, "task_id": "1-64"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)  # larger asymmetric kernel\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)  # larger asymmetric kernel\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)  # larger asymmetric kernel\nwidth = 512\nheight = 512\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "65_conv_transposed_2D__square_input__asymmetric_kernel", "problem_id": 65}, "task_id": "1-65"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).\n        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).\n        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).\n        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).\n        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).\n        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).\n        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "66_conv_standard_3D__asymmetric_input__asymmetric_kernel", "problem_id": 66}, "task_id": "1-66"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nlength = 131072\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nlength = 131072\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nlength = 131072\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "67_conv_standard_1D", "problem_id": 67}, "task_id": "1-67"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), \n                             where kernel_width == kernel_height.\n        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), \n                             where kernel_width == kernel_height.\n        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), \n                             where kernel_width == kernel_height.\n        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).\n        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).\n        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, width, height)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "68_conv_transposed_3D__square_input__asymmetric_kernel", "problem_id": 68}, "task_id": "1-68"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution operation with asymmetric input and kernel size.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of integers representing the kernel size (height, width).\n        stride (tuple, optional): Tuple of integers representing the stride of the convolution. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of integers representing the padding applied to the input. Defaults to (0, 0).\n        output_padding (tuple, optional): Tuple of integers representing the additional size added to one side of the output shape. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of integers representing the spacing between kernel elements. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), output_padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution operation with asymmetric input and kernel size.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of integers representing the kernel size (height, width).\n        stride (tuple, optional): Tuple of integers representing the stride of the convolution. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of integers representing the padding applied to the input. Defaults to (0, 0).\n        output_padding (tuple, optional): Tuple of integers representing the additional size added to one side of the output shape. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of integers representing the spacing between kernel elements. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), output_padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution operation with asymmetric input and kernel size.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Tuple of integers representing the kernel size (height, width).\n        stride (tuple, optional): Tuple of integers representing the stride of the convolution. Defaults to (1, 1).\n        padding (tuple, optional): Tuple of integers representing the padding applied to the input. Defaults to (0, 0).\n        output_padding (tuple, optional): Tuple of integers representing the additional size added to one side of the output shape. Defaults to (0, 0).\n        dilation (tuple, optional): Tuple of integers representing the spacing between kernel elements. Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), output_padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "69_conv_transposed_2D__asymmetric_input__asymmetric_kernel", "problem_id": 69}, "task_id": "1-69"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "6_Matmul_with_large_K_dimension_", "problem_id": 6}, "task_id": "1-6"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. \n                                                  Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, \n                 dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), \n                                                stride=stride, padding=padding, output_padding=output_padding, \n                                                dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 24\nkernel_size = 3\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. \n                                                  Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, \n                 dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), \n                                                stride=stride, padding=padding, output_padding=output_padding, \n                                                dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 24\nkernel_size = 3\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 3D convolution operation with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Defaults to 1.\n        padding (int or tuple, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the output shape. \n                                                  Defaults to 0.\n        dilation (int or tuple, optional): Spacing between kernel elements. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, \n                 dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, (kernel_size, kernel_size, kernel_size), \n                                                stride=stride, padding=padding, output_padding=output_padding, \n                                                dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 3D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 48\nout_channels = 24\nkernel_size = 3\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "70_conv_transposed_3D__asymmetric_input__square_kernel", "problem_id": 70}, "task_id": "1-70"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n# large asymmetric input\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n# large asymmetric input\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 2D convolution with asymmetric input and a square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n# large asymmetric input\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "71_conv_transposed_2D__asymmetric_input__square_kernel", "problem_id": 71}, "task_id": "1-71"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).\n        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).\n        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).\n        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\ndepth = 12\nheight = 24\nwidth = 48\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).\n        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).\n        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).\n        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\ndepth = 12\nheight = 24\nwidth = 48\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).\n        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).\n        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).\n        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\ndepth = 12\nheight = 24\nwidth = 48\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_", "problem_id": 72}, "task_id": "1-72"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and square kernel.\n    The input is padded before the convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 4\nin_channels = 32\nout_channels = 32\nkernel_size = 3\ndepth = 32\nheight = 64\nwidth = 128\nstride = 2\npadding = 1\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and square kernel.\n    The input is padded before the convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 4\nin_channels = 32\nout_channels = 32\nkernel_size = 3\ndepth = 32\nheight = 64\nwidth = 128\nstride = 2\npadding = 1\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with asymmetric input and square kernel.\n    The input is padded before the convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, groups=groups, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 4\nin_channels = 32\nout_channels = 32\nkernel_size = 3\ndepth = 32\nheight = 64\nwidth = 128\nstride = 2\npadding = 1\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "73_conv_transposed_3D_asymmetric_input_square_kernel__strided_padded__grouped", "problem_id": 73}, "task_id": "1-73"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nlength = 131072\nstride = 1\npadding = 0\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nlength = 131072\nstride = 1\npadding = 0\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 32\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nlength = 131072\nstride = 1\npadding = 0\ndilation = 3\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "74_conv_transposed_1D_dilated", "problem_id": 74}, "task_id": "1-74"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input, asymmetric kernel, \n    grouped, padded, and dilated.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nheight = 128\nwidth = 256\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input, asymmetric kernel, \n    grouped, padded, and dilated.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nheight = 128\nwidth = 256\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input, asymmetric kernel, \n    grouped, padded, and dilated.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), dilation: tuple = (1, 1), groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nheight = 128\nwidth = 256\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "75_conv_transposed_2D_asymmetric_input_asymmetric_kernel_strided__grouped____padded____dilated__", "problem_id": 75}, "task_id": "1-75"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation with asymmetric input and a square kernel, potentially dilated and strided.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n# longer signal\nlength = 524280\nstride = 3\ndilation = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation with asymmetric input and a square kernel, potentially dilated and strided.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n# longer signal\nlength = 524280\nstride = 3\ndilation = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 1D convolution operation with asymmetric input and a square kernel, potentially dilated and strided.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n# longer signal\nlength = 524280\nstride = 3\ndilation = 4\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "76_conv_standard_1D_dilated_strided__", "problem_id": 76}, "task_id": "1-76"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with square input and square kernel,\n    and supports padding, dilation, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square kernel, so only one value needed).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\ndepth = 16\nheight = 32\nwidth = 32\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with square input and square kernel,\n    and supports padding, dilation, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square kernel, so only one value needed).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\ndepth = 16\nheight = 32\nwidth = 32\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 3D transposed convolution operation with square input and square kernel,\n    and supports padding, dilation, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square kernel, so only one value needed).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 3D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose3d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\ndepth = 16\nheight = 32\nwidth = 32\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "77_conv_transposed_3D_square_input_square_kernel___padded____dilated____strided__", "problem_id": 77}, "task_id": "1-77"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and kernel, with optional padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nheight = 512\nwidth = 1024\nstride = (1, 1)\npadding = (1, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and kernel, with optional padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nheight = 512\nwidth = 1024\nstride = (1, 1)\npadding = (1, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and kernel, with optional padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width).\n        stride (tuple, optional): Stride of the convolution (height, width). Defaults to (1, 1).\n        padding (tuple, optional): Padding applied to the input (height, width). Defaults to (0, 0).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nheight = 512\nwidth = 1024\nstride = (1, 1)\npadding = (1, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "78_conv_transposed_2D_asymmetric_input_asymmetric_kernel___padded__", "problem_id": 78}, "task_id": "1-78"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with asymmetric input and square kernel.\n    Supports padding, striding, and dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\n# long sequence\nlength = 131072\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with asymmetric input and square kernel.\n    Supports padding, striding, and dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\n# long sequence\nlength = 131072\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a transposed 1D convolution operation with asymmetric input and square kernel.\n    Supports padding, striding, and dilation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the transposed 1D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).\n        \"\"\"\n        return self.conv1d_transpose(x)\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\n# long sequence\nlength = 131072\nstride = 2\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, length)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "79_conv_transposed_1D_asymmetric_input_square_kernel___padded____strided____dilated__", "problem_id": 79}, "task_id": "1-79"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "7_Matmul_with_small_K_dimension_", "problem_id": 7}, "task_id": "1-7"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with square input and asymmetric kernel, with dilation and padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width). \n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (tuple, optional): Padding applied to the input (top/bottom, left/right). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: tuple = (0, 0), dilation: tuple = (1, 1), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\nstride = 1\npadding = (2, 4)\ndilation = (2, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with square input and asymmetric kernel, with dilation and padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width). \n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (tuple, optional): Padding applied to the input (top/bottom, left/right). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: tuple = (0, 0), dilation: tuple = (1, 1), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\nstride = 1\npadding = (2, 4)\ndilation = (2, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a standard 2D convolution operation with square input and asymmetric kernel, with dilation and padding.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (tuple): Size of the convolution kernel (height, width). \n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (tuple, optional): Padding applied to the input (top/bottom, left/right). Defaults to (0, 0).\n        dilation (tuple, optional): Spacing between kernel elements (height, width). Defaults to (1, 1).\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: int = 1, padding: tuple = (0, 0), dilation: tuple = (1, 1), bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 8\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nwidth = 512\nheight = 512\nstride = 1\npadding = (2, 4)\ndilation = (2, 3)\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "80_conv_standard_2D_square_input_asymmetric_kernel___dilated____padded__", "problem_id": 80}, "task_id": "1-80"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and square kernel, supporting dilation, padding, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square, e.g., 3 for a 3x3 kernel).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in). \n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nheight_in = 64\nwidth_in = 128\nstride = 5\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and square kernel, supporting dilation, padding, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square, e.g., 3 for a 3x3 kernel).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in). \n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nheight_in = 64\nwidth_in = 128\nstride = 5\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a 2D transposed convolution operation with asymmetric input and square kernel, supporting dilation, padding, and stride.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel (square, e.g., 3 for a 3x3 kernel).\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the 2D transposed convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in). \n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv_transpose2d(x)\n\n\n# Test code\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nheight_in = 64\nwidth_in = 128\nstride = 5\npadding = 1\ndilation = 2\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "81_conv_transposed_2D_asymmetric_input_square_kernel___dilated____padded____strided__", "problem_id": 81}, "task_id": "1-81"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution operation with square input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "82_conv_depthwise_2D_square_input_square_kernel", "problem_id": 82}, "task_id": "1-82"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 8\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 8\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with a square input and an asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, in_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 8\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "83_conv_depthwise_2D_square_input_asymmetric_kernel", "problem_id": 83}, "task_id": "1-83"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nwidth_in = 512\nheight_in = 256\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nwidth_in = 512\nheight_in = 256\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and square kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the square convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 64\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nwidth_in = 512\nheight_in = 256\nstride = 1\npadding = 0\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height_in, width_in)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "84_conv_depthwise_2D_asymmetric_input_square_kernel", "problem_id": 84}, "task_id": "1-84"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size_h (int): Height of the convolution kernel.\n        kernel_size_w (int): Width of the convolution kernel.\n        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.\n        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.\n        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.\n        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.\n        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.\n        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 128\nout_channels = 128\nkernel_size_h = 3\nkernel_size_w = 7\nwidth = 256\nheight = 128\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\ndilation_h = 1\ndilation_w = 1\ngroups = in_channels\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size_h (int): Height of the convolution kernel.\n        kernel_size_w (int): Width of the convolution kernel.\n        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.\n        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.\n        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.\n        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.\n        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.\n        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 128\nout_channels = 128\nkernel_size_h = 3\nkernel_size_w = 7\nwidth = 256\nheight = 128\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\ndilation_h = 1\ndilation_w = 1\ngroups = in_channels\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size_h (int): Height of the convolution kernel.\n        kernel_size_w (int): Width of the convolution kernel.\n        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.\n        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.\n        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.\n        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.\n        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.\n        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        return self.conv2d(x)\n\n# Test code\nbatch_size = 32\nin_channels = 128\nout_channels = 128\nkernel_size_h = 3\nkernel_size_w = 7\nwidth = 256\nheight = 128\nstride_h = 1\nstride_w = 1\npadding_h = 0\npadding_w = 0\ndilation_h = 1\ndilation_w = 1\ngroups = in_channels\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "85_conv_depthwise_2D_asymmetric_input_asymmetric_kernel", "problem_id": 85}, "task_id": "1-85"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise-separable 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise-separable 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int): Size of the convolution kernel.\n        stride (int, optional): Stride of the convolution. Defaults to 1.\n        padding (int, optional): Padding applied to the input. Defaults to 0.\n        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(Model, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the depthwise-separable 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).\n        \"\"\"\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nwidth = 512\nheight = 512\nstride = 1\npadding = 1\ndilation = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "86_conv_depthwise_separable_2D", "problem_id": 86}, "task_id": "1-86"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a pointwise 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the pointwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a pointwise 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the pointwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs a pointwise 2D convolution operation.\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels produced by the convolution.\n        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):\n        super(Model, self).__init__()\n        self.conv1d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the pointwise 2D convolution.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n        \"\"\"\n        return self.conv1d(x)\n\n# Test code\nbatch_size = 16\nin_channels = 64\nout_channels = 128\nwidth = 1024\nheight = 1024\n\ndef get_inputs():\n    x = torch.rand(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "87_conv_pointwise_2D", "problem_id": 87}, "task_id": "1-87"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nbatch_size = 8192\ndim = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, dim)]\n\ndef get_init_inputs():\n    return []```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nbatch_size = 8192\ndim = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, dim)]\n\ndef get_init_inputs():\n    return []", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass Model(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nbatch_size = 8192\ndim = 8192\n\ndef get_inputs():\n    return [torch.rand(batch_size, dim)]\n\ndef get_init_inputs():\n    return []", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "88_MinGPTNewGelu", "problem_id": 88}, "task_id": "1-88"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the scan operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the Scan model.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative sum.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` \n                              can vary depending on the use case.\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.\n        \"\"\"\n        return torch.cumsum(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    \"\"\"\n    Generates random inputs for testing the Scan model.\n\n    Returns:\n        list: A list containing a single randomly generated tensor with shape \n              (batch_size, *input_shape).\n    \"\"\"\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    \"\"\"\n    Returns the initialization parameters for the Scan model.\n\n    Returns:\n        list: A list containing the `dim` parameter for model initialization.\n    \"\"\"\n    return [dim]```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the scan operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the Scan model.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative sum.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` \n                              can vary depending on the use case.\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.\n        \"\"\"\n        return torch.cumsum(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    \"\"\"\n    Generates random inputs for testing the Scan model.\n\n    Returns:\n        list: A list containing a single randomly generated tensor with shape \n              (batch_size, *input_shape).\n    \"\"\"\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    \"\"\"\n    Returns the initialization parameters for the Scan model.\n\n    Returns:\n        list: A list containing the `dim` parameter for model initialization.\n    \"\"\"\n    return [dim]", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the scan operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the Scan model.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative sum.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` \n                              can vary depending on the use case.\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.\n        \"\"\"\n        return torch.cumsum(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    \"\"\"\n    Generates random inputs for testing the Scan model.\n\n    Returns:\n        list: A list containing a single randomly generated tensor with shape \n              (batch_size, *input_shape).\n    \"\"\"\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    \"\"\"\n    Returns the initialization parameters for the Scan model.\n\n    Returns:\n        list: A list containing the `dim` parameter for model initialization.\n    \"\"\"\n    return [dim]", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "89_cumsum", "problem_id": 89}, "task_id": "1-89"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "8_Matmul_with_irregular_shapes_", "problem_id": 8}, "task_id": "1-8"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a cumulative product operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the cumulative product operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the CumulativeProductModel.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative product.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass, computing the cumulative product along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.\n        \"\"\"\n        return torch.cumprod(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a cumulative product operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the cumulative product operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the CumulativeProductModel.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative product.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass, computing the cumulative product along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.\n        \"\"\"\n        return torch.cumprod(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a cumulative product operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the cumulative product operation.\n    \"\"\"\n\n    def __init__(self, dim):\n        \"\"\"\n        Initialize the CumulativeProductModel.\n\n        Args:\n            dim (int): The dimension along which to perform the cumulative product.\n        \"\"\"\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass, computing the cumulative product along the specified dimension.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n\n        Returns:\n            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.\n        \"\"\"\n        return torch.cumprod(x, dim=self.dim)\n\n# Define input dimensions and parameters\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "90_cumprod", "problem_id": 90}, "task_id": "1-90"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a reverse cumulative sum operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the reverse cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a reverse cumulative sum operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the reverse cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a reverse cumulative sum operation along a specified dimension.\n\n    Parameters:\n        dim (int): The dimension along which to perform the reverse cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "91_cumsum_reverse", "problem_id": 91}, "task_id": "1-91"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs an exclusive cumulative sum (does not include the current element).\n\n    Parameters:\n        dim (int): The dimension along which to perform the exclusive cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]\n        return torch.cumsum(exclusive_cumsum, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs an exclusive cumulative sum (does not include the current element).\n\n    Parameters:\n        dim (int): The dimension along which to perform the exclusive cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]\n        return torch.cumsum(exclusive_cumsum, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs an exclusive cumulative sum (does not include the current element).\n\n    Parameters:\n        dim (int): The dimension along which to perform the exclusive cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]\n        return torch.cumsum(exclusive_cumsum, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "92_cumsum_exclusive", "problem_id": 92}, "task_id": "1-92"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(Model, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    x = torch.rand(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "93_masked_cumsum", "problem_id": 93}, "task_id": "1-93"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes the Mean Squared Error loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean((predictions - targets) ** 2)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes the Mean Squared Error loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean((predictions - targets) ** 2)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes the Mean Squared Error loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.mean((predictions - targets) ** 2)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "94_MSELoss", "problem_id": 94}, "task_id": "1-94"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 32768\nnum_classes = 4096\ninput_shape = (num_classes,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 32768\nnum_classes = 4096\ninput_shape = (num_classes,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 32768\nnum_classes = 4096\ninput_shape = (num_classes,)\ndim = 1\n\ndef get_inputs():\n    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "95_CrossEntropyLoss", "problem_id": 95}, "task_id": "1-95"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Smooth L1 (Huber) Loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.smooth_l1_loss(predictions, targets)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Smooth L1 (Huber) Loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.smooth_l1_loss(predictions, targets)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Smooth L1 (Huber) Loss for regression tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.smooth_l1_loss(predictions, targets)\n\nbatch_size = 32768\ninput_shape = (32768,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "96_HuberLoss", "problem_id": 96}, "task_id": "1-96"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        return out\n\nbatch_size = 32\nnum_heads = 32\nsequence_length = 512\nembedding_dimension = 1024\n\ndef get_inputs():\n    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    return [Q, K, V]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        return out\n\nbatch_size = 32\nnum_heads = 32\nsequence_length = 512\nembedding_dimension = 1024\n\ndef get_inputs():\n    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    return [Q, K, V]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n        return out\n\nbatch_size = 32\nnum_heads = 32\nsequence_length = 512\nembedding_dimension = 1024\n\ndef get_inputs():\n    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)\n    return [Q, K, V]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "97_ScaledDotProductAttention", "problem_id": 97}, "task_id": "1-97"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')\n\nbatch_size = 8192 * 2\ninput_shape = (8192 * 2,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]\n\ndef get_init_inputs():\n    return []\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')\n\nbatch_size = 8192 * 2\ninput_shape = (8192 * 2,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]\n\ndef get_init_inputs():\n    return []\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Kullback-Leibler Divergence for comparing two distributions.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')\n\nbatch_size = 8192 * 2\ninput_shape = (8192 * 2,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]\n\ndef get_init_inputs():\n    return []\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "98_KLDivLoss", "problem_id": 98}, "task_id": "1-98"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Triplet Margin Loss for metric learning tasks.\n\n    Parameters:\n        margin (float): The margin between the positive and negative samples.\n    \"\"\"\n    def __init__(self, margin=1.0):\n        super(Model, self).__init__()\n        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)\n\nbatch_size = 32768\ninput_shape = (8192,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]\n    \ndef get_init_inputs():\n    return [1.0]  # Default margin\n```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Triplet Margin Loss for metric learning tasks.\n\n    Parameters:\n        margin (float): The margin between the positive and negative samples.\n    \"\"\"\n    def __init__(self, margin=1.0):\n        super(Model, self).__init__()\n        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)\n\nbatch_size = 32768\ninput_shape = (8192,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]\n    \ndef get_init_inputs():\n    return [1.0]  # Default margin\n", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    A model that computes Triplet Margin Loss for metric learning tasks.\n\n    Parameters:\n        margin (float): The margin between the positive and negative samples.\n    \"\"\"\n    def __init__(self, margin=1.0):\n        super(Model, self).__init__()\n        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)\n\nbatch_size = 32768\ninput_shape = (8192,)\ndim = 1\n\ndef get_inputs():\n    scale = torch.rand(())\n    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]\n    \ndef get_init_inputs():\n    return [1.0]  # Default margin\n", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "99_TripletMarginLoss", "problem_id": 99}, "task_id": "1-99"}
{"question": [{"role": "system", "content": "You are an expert in PyTorch and CUDA programming. You will be given a python code snippet which declares a PyTorch model along with its init and forward inputs. The model is an instance of class `Model`. It will be created with arguments from `get_init_inputs()`. Then its `forward` function will be called with data from `get_inputs()`. Your task is to write a custom CUDA extension to accelerate the model `forward` function. Note that:\n- Provide only a single python code block in your final answer.\n- Name your optimized model as `ModelNew`. Keep its `__init__` and `forward` function signature the same as `Model`. Keep the names of all submodules unchanged. Ensure the keys of model state dict are unchanged. Do not create any extra tensor parameter during model initialization.\n- Inline the CUDA code within quotes and assign it to the `source` variable. Inline the C++ function definition into the `cpp_src` variable. Compile and load the extension using `torch.utils.cpp_extension.load_inline`.\n- Carefully decide the kernel function signature and pass the correct arguments into your kernel.\n- Do not perform extra initialization on parameters of any submodule. Keep them initialized by default.\n- Implement all CUDA operators by yourself. Do not call any function from `torch` namespace except for allocating or initializing tensors. Do not call any function from `torch.nn.functional` namespace. Do not call the forward function of any submodule. You can only use the parameters and attributes of the submodule. For example, you should pass `self.linear.weight` and `self.linear.bias` as arguments to your CUDA kernel instead of directly running `self.linear(x)`.\n- You can implement more than one kernel in the CUDA extension. If there are multiple operators within `forward` function, you must implement all of them no matter how many CUDA kernels are needed.\nFor example:\n## problem:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return self.alpha * a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return [2.0]\n```\n## answer:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nsource = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, float alpha, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = alpha * a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), alpha, size);\n\n    return out;\n}\n\"\"\"\n\ncpp_src = \"\"\"\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b, float alpha);\n\"\"\"\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, alpha: float) -> None:\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, a, b):\n        return elementwise_add.elementwise_add_cuda(a, b, self.alpha)\n```\nNow, please write the PyTorch code with custom CUDA extension based on the problem given by the user.\n"}, {"role": "user", "content": "## problem: ```pythonimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed```\n"}], "data_source": "ScalingIntelligence/KernelBench/level_1", "ability": "code", "answer": "", "raw_problem": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "level": "1", "type": "", "ground_truth": {"pytorch_module": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs():\n    A = torch.rand(M, N)\n    B = torch.rand(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed", "pytorch_functional": "", "cuda_code": ""}, "style": "cuda-sandbox-v2", "extra_info": {"op_name": "9_Tall_skinny_matrix_multiplication_", "problem_id": 9}, "task_id": "1-9"}
